<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Codegen on MikesBlog</title>
    <link>https://oshea00.github.io/tags/codegen/</link>
    <description>Recent content in Codegen on MikesBlog</description>
    <generator>Hugo -- 0.146.7</generator>
    <language>en</language>
    <lastBuildDate>Thu, 01 May 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://oshea00.github.io/tags/codegen/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Vibecoding an Agentic Coder - Part 2</title>
      <link>https://oshea00.github.io/posts/agentic-codegen-part2/</link>
      <pubDate>Thu, 01 May 2025 00:00:00 +0000</pubDate>
      <guid>https://oshea00.github.io/posts/agentic-codegen-part2/</guid>
      <description>&lt;p&gt;&lt;figure class=&#34;align-center &#34;&gt;
    &lt;img loading=&#34;lazy&#34; src=&#34;images/agentflock.png#center&#34;/&gt; 
&lt;/figure&gt;

In this segment, I&amp;rsquo;ll generate many candidate applications using my experimental framework, &lt;a href=&#34;https://github.com/oshea00&#34;&gt;CodeAgents&lt;/a&gt;, choosing from a set of models: GPT-4.1, Claude 3.7, and GPT-4o. Then, I&amp;rsquo;ll compare and contrast the solutions. Along the way, I&amp;rsquo;ll present some ideas and tips on improving AI-generated code in ways that generally translate to other tools and frameworks.&lt;/p&gt;
&lt;p&gt;It isn&amp;rsquo;t easy to score how good an AI-coded solution is. Of the possible metrics, code complexity might not be as meaningful as long as the AI understands the code, as would &amp;ldquo;maintainability,&amp;rdquo; as that&amp;rsquo;s based on human limitations; the AI can refactor on the fly. Test coverage is a good metric as it measures how well the AI-generated test suite covers the code.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Vibecoding an Agentic Coder - Part 1</title>
      <link>https://oshea00.github.io/posts/agentic-codegen-part1/</link>
      <pubDate>Sun, 27 Apr 2025 00:00:00 +0000</pubDate>
      <guid>https://oshea00.github.io/posts/agentic-codegen-part1/</guid>
      <description>&lt;p&gt;&lt;figure class=&#34;align-center &#34;&gt;
    &lt;img loading=&#34;lazy&#34; src=&#34;images/agentengineer_3_1_aspect_ratio.png#center&#34;/&gt; 
&lt;/figure&gt;

I&amp;rsquo;ve tried Cursor, Replit, Lovable, and Bolt with varying degrees of success and found recurring themes in the use of these tools that require &amp;ldquo;vibing&amp;rdquo; until you arrive at a finished, hopefully working, result. Whether the result is good can sometimes be in the eye of the beholder.&lt;/p&gt;
&lt;p&gt;I’ve also become fascinated by how these tools will change the way programmers think about code and its organization — how many rules will be thrown completely out the window and how, oddly, the new rules will harken back to the early days of programming before Google and the Internet.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
