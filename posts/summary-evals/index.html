<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Building a Hybrid Summary Evaluation Framework | MikesBlog</title>
<meta name="keywords" content="llm, openai, prompt engineering, summarization">
<meta name="description" content="
     


Combining deterministic NLP with LLM-as-Judge for robust evaluation
Summary Evaluation Challenges
Summary evaluation metrics sometimes fall short in capturing the qualities most relevant to assessing summary quality. Traditional machine learning for natural language processing (NLP)
has covered a lot of ground in this area. Widely used measures such as ROUGE focus on surface-level token overlap and n-gram matches. While effective for evaluating lexical similarity, these approaches offer limited insight into aspects such as factual accuracy or semantic completeness [1].">
<meta name="author" content="Michael OShea">
<link rel="canonical" href="https://oshea00.github.io/posts/summary-evals/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.f49d66caae9ea0fd43f21f29e71a8d3e284517ed770f2aa86fa012953ad3c9ef.css" integrity="sha256-9J1myq6eoP1D8h8p5xqNPihFF&#43;13Dyqob6ASlTrTye8=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://oshea00.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://oshea00.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://oshea00.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://oshea00.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://oshea00.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://oshea00.github.io/posts/summary-evals/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css" integrity="sha384-MlJdn/WNKDGXveldHDdyRP1R4CTHr3FeuDNfhsLPYrq2t0UBkUdK2jyTnXPEK1NQ" crossorigin="anonymous">

<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.js" integrity="sha384-VQ8d8WVFw0yHhCk5E8I86oOhv48xLpnDZx5T9GogA/Y84DcCKWXDmSDfn13bzFZY" crossorigin="anonymous"></script>

<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/contrib/auto-render.min.js" integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR" crossorigin="anonymous"
    onload="renderMathInElement(document.body);"></script>


<script>
document.addEventListener("DOMContentLoaded", function() {
    renderMathInElement(document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "$", right: "$", display: false}
        ]
    });
});
</script>

      <script async src="https://www.googletagmanager.com/gtag/js?id=G-37B7H2GBCX"></script>
      <script>
        var doNotTrack = false;
        if ( false ) {
          var dnt = (navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack);
          var doNotTrack = (dnt == "1" || dnt == "yes");
        }
        if (!doNotTrack) {
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());
          gtag('config', 'G-37B7H2GBCX');
        }
      </script><meta property="og:url" content="https://oshea00.github.io/posts/summary-evals/">
  <meta property="og:site_name" content="MikesBlog">
  <meta property="og:title" content="Building a Hybrid Summary Evaluation Framework">
  <meta property="og:description" content=" Combining deterministic NLP with LLM-as-Judge for robust evaluation
Summary Evaluation Challenges Summary evaluation metrics sometimes fall short in capturing the qualities most relevant to assessing summary quality. Traditional machine learning for natural language processing (NLP) has covered a lot of ground in this area. Widely used measures such as ROUGE focus on surface-level token overlap and n-gram matches. While effective for evaluating lexical similarity, these approaches offer limited insight into aspects such as factual accuracy or semantic completeness [1].">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2025-09-14T00:00:00+00:00">
    <meta property="article:modified_time" content="2025-09-14T00:00:00+00:00">
    <meta property="article:tag" content="Llm">
    <meta property="article:tag" content="Openai">
    <meta property="article:tag" content="Prompt Engineering">
    <meta property="article:tag" content="Summarization">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Building a Hybrid Summary Evaluation Framework">
<meta name="twitter:description" content="
     


Combining deterministic NLP with LLM-as-Judge for robust evaluation
Summary Evaluation Challenges
Summary evaluation metrics sometimes fall short in capturing the qualities most relevant to assessing summary quality. Traditional machine learning for natural language processing (NLP)
has covered a lot of ground in this area. Widely used measures such as ROUGE focus on surface-level token overlap and n-gram matches. While effective for evaluating lexical similarity, these approaches offer limited insight into aspects such as factual accuracy or semantic completeness [1].">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://oshea00.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Building a Hybrid Summary Evaluation Framework",
      "item": "https://oshea00.github.io/posts/summary-evals/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Building a Hybrid Summary Evaluation Framework",
  "name": "Building a Hybrid Summary Evaluation Framework",
  "description": " Combining deterministic NLP with LLM-as-Judge for robust evaluation\nSummary Evaluation Challenges Summary evaluation metrics sometimes fall short in capturing the qualities most relevant to assessing summary quality. Traditional machine learning for natural language processing (NLP) has covered a lot of ground in this area. Widely used measures such as ROUGE focus on surface-level token overlap and n-gram matches. While effective for evaluating lexical similarity, these approaches offer limited insight into aspects such as factual accuracy or semantic completeness [1].\n",
  "keywords": [
    "llm", "openai", "prompt engineering", "summarization"
  ],
  "articleBody": " Combining deterministic NLP with LLM-as-Judge for robust evaluation\nSummary Evaluation Challenges Summary evaluation metrics sometimes fall short in capturing the qualities most relevant to assessing summary quality. Traditional machine learning for natural language processing (NLP) has covered a lot of ground in this area. Widely used measures such as ROUGE focus on surface-level token overlap and n-gram matches. While effective for evaluating lexical similarity, these approaches offer limited insight into aspects such as factual accuracy or semantic completeness [1].\nFor example, consider two summaries that state ‚ÄúThe study found 73% improvement‚Äù and ‚ÄúNearly three-quarters showed positive outcomes.‚Äù Both express the same underlying fact, yet traditional metrics treat them as distinct. This illustrates how lexical approaches can struggle to account for semantic equivalence and factual consistency.\nI did some experimentation with the Natural Language Toolkit (NLTK) and managed to get a good first cut working with the traditional ML approach, but my accuracy scoring was falling short on many edge cases such as the one above, and others described in [1]. So, a new experiment was needed.\nA Hybrid Approach Initial explorations with established NLP metrics demonstrated their strengths and limitations, but extending these approaches to semantic accuracy was not robust. It seemed LLM-based evaluation might be better for semantic assessment with good prompt design.\nAt the same time, relying solely on LLM-based evaluation for the other more deterministic scores related to coherence and completeness seemed like overkill. When considering the different quality dimensions, I considered a tailored ‚Äúhybrid‚Äù approach:\nAccuracy: Dependent on semantic understanding and factual consistency assessment - LLM Completeness: Amenable to topic extraction and coverage analysis - NLTK Coherence: Supported by established linguistic measures - ROUGE Overall Score: Using weighted average of the above components. This seemed to be worth exploring with some experimentation.\nArchitecture: Three Evaluation Paradigms Accuracy: LLM-as-Judge with Structured Prompting For accuracy, large language models provide the needed semantic reasoning to assess equivalence and factual consistency. Structured prompting ensures consistent, interpretable judgments:\nprompt = f\"Compare the and provided. Using only the information provided in the , characterize the accuracy of the on a scale of 0=Poor, 1=Fair, 2=Good, or 3=Excellent. Provide a short rationale for your score. {source_text} {summary_text} Format as JSON: {{'score': int, 'rationale': str}}\" Key design considerations:\nExplicit source constraint prevents hallucinations or injection of external knowledge Discrete 0‚Äì3 scale improves judgment consistency Required rationale encourages reasoning and enables debugging JSON formatting ensures parseable responses The rationale requirement, in particular, enhances judgment quality by prompting more deliberate decisions.\nCompleteness: Enhanced Topic Extraction Completeness is measured through topic extraction methods that extend beyond TF-IDF. By incorporating document structure knowledge and positional weighting, the evaluation captures salient content more effectively:\n# Weight sentences by position and content characteristics if i == 0 or i == len(sentences) - 1: position_weight = 1.5 elif i \u003c len(sentences) * 0.2 or i \u003e len(sentences) * 0.8: position_weight = 1.2 else: position_weight = 1.0 combined_score = position_weight * tfidf_score * length_weight Semantic similarity (cosine similarity on sentence embeddings) is then used to match extracted topics with summary content. A coverage threshold of 0.4 balances precision and recall. This approach captures topic coverage that lexical matching alone may miss, especially when summaries express concepts using different terminology.\nCoherence: ROUGE Metrics For coherence, established computational linguistics methods are sufficient and efficient:\ncoherence = (0.4 * semantic + 0.25 * discourse + 0.08 * (rouge1+ rouge2+ rougeL+ lexical)) Components:\nROUGE-1/2/L: Official F1 scores using rouge-score library with stemming Semantic coherence: Sentence embedding similarities combined with ROUGE-L Discourse coherence: Penn Discourse Treebank marker analysis Lexical diversity: Type-Token Ratio with windowed normalization Contradiction penalty: 1.0 (no contradictions) to 0.0 (many contradictions) Readability: Flesch-Kincaid grade normalized as max(0, min(1, (20-grade)/20)) Length penalty: 1.0 (normal length) to 0.3 minimum (very short texts) Deterministic methods are appropriate here because coherence has objective, measurable properties and reliable metrics already exist.\nOverall Score Weighting Rationale I chose a 60/25/15 weighting across accuracy, completeness, and coherence to reflect practical priorities: (0.6 * Accuracy + 0.25 * Completeness + 0.15 * Coherence).\nAccuracy receives the greatest emphasis, since factual errors undermine trust. Completeness is weighted significantly, as omission of key information reduces usefulness. Coherence is weighted lower, since awkward phrasing is less harmful than inaccuracy. These weights can be tuned for domain-specific applications.\nPrompt Engineering for Consistent Judgments Reliability in accuracy scoring depends heavily on careful prompt design. Critical elements include:\nConstraining evaluation to provided source material Using a discrete scale rather than continuous ranges Requiring explicit reasoning Enforcing structured output Among these, the source constraint proved particularly important, as unconstrained prompts encouraged models to draw on external knowledge rather than evaluate the given material.\nPerformance Characteristics The hybrid framework yields overall score distributions that align with intuitive quality assessments:\n0.8‚Äì1.0: High-quality summaries with minimal issues 0.6‚Äì0.8: Good summaries with some gaps in completeness or flow 0.4‚Äì0.6: Usable but with noticeable issues 0.0‚Äì0.4: Significant problems in accuracy, completeness, or coherence The system captures error types that purely algorithmic methods tend to miss:\nSemantic paraphrasing Factual contradictions expressed with different wording Important topic omissions despite high lexical overlap Issues with discourse and readability The overall scoring establishes levels that can be used to flag summaries for human evaluators to more closely examine.\nSummary Evaluation Using a Standard Baseline To test this framework I chose to use the CNN/DailyMail dataset which contains 287K training articles and summaries of news articles. This is a standard used in many summary evaluation experiments [1].\nSummary Evaluation Tool for CNN/DailyMail Dataset This module provides functionality to evaluate reference summaries from the CNN/DailyMail dataset using a comprehensive SummaryEvaluator with ROUGE-enhanced coherence scoring [2].\nKey Features:\nLoads and samples from CNN/DailyMail dataset (287k+ articles) Evaluates summaries across three dimensions: Accuracy: LLM-as-judge scoring using OpenAI models (GPT-4, GPT-4o) Completeness: Topic coverage analysis using TF-IDF and Semantic similarity Coherence: ROUGE-enhanced scoring with discourse analysis and readability Flexible output formats: pretty console display or CSV export Configurable sampling with optional seeding for reproducibility Command-line interface with comprehensive argument parsing Usage Examples:\npython evalsummary.py # default 5 samples, GPT-4o, pretty output python evalsummary.py -n 10 -m gpt-4.1 # 10 samples with GPT-4.1``` python evalsummary.py -f csv \u003e results.csv # CSV export for analysis``` python evalsummary.py --seed 42 -n 20 # Reproducible results``` Example output üì• Loading CNN/DailyMail dataset... ‚úÖ Dataset loaded - Train: 287113 üöÄ Evaluating 1 random reference summaries using gpt-4o ================================================================================ Article 1 (index 221292): Bradford midfielder Billy Knott has taken to Twitter to thank John Terry for giving his dad a signed shirt following his side's stunning 4-2 FA Cup win against Chelsea on Saturday. The former Chelsea youngster was still revelling in the win on Sunday morning as he posed with his father, Steve, and the famous No 26 shirt of the Blues captain. Knott tweeted: 'Thanks to jt top man give the shirt to my dad. What a day that was every were we goooo [sic].' Billy Knott shows off his signed shirt from... Reference Summary: Billy Knott received signed shirt from John Terry following Bradford's 4-2 FA Cup win against Chelsea. Knott took to Twitter to thank 'top man' Terry for the gift. Bantams are through to FA Cup fifth round following memorable win. -------------------------------------------------------------------------------- üìä SUMMARY EVALUATION RESULTS: ---------------------------------------- üéØ Accuracy: 1.000 (100.0%) üìã Completeness: 0.532 (53.2%) üîó Coherence: 0.437 (43.7%) ‚≠ê Overall: 0.799 (79.9%) üí≠ Accuracy Rationale: The summary accurately captures the key points from the source: Billy Knott received a signed shirt from John Terry, he thanked Terry on Twitter, and Bradford's win against Chelsea allowed them to progress to the FA Cup fifth round. The summary is concise and includes all the main elements mentioned in the source. Conclusion The hybrid framework combines evaluation paradigms to provide robust, semantically aware summary assessment. It leverages LLMs for accuracy, structured NLP methods for completeness, and classical measures for coherence, producing a more comprehensive evaluation than any single method alone.\nThe result is an approach that balances semantic nuance with consistency and efficiency, offering a flexible foundation for summary evaluation across diverse domains. With overall scoring establishing levels that can be used to flag summaries for human evaluators to review.\nReferences [1] Fabbri, A. R., et al. (2020). SummEval: Re-evaluating Summarization Evaluation. arXiv preprint arXiv:2007.12626. https://arxiv.org/pdf/2007.12626\n[2] O‚ÄôShea, M. (2024). Summary Evaluation Framework. GitHub repository. https://github.com/oshea00/summaryeval\n",
  "wordCount" : "1387",
  "inLanguage": "en",
  "datePublished": "2025-09-14T00:00:00Z",
  "dateModified": "2025-09-14T00:00:00Z",
  "author":[{
    "@type": "Person",
    "name": "Michael OShea"
  }],
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://oshea00.github.io/posts/summary-evals/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "MikesBlog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://oshea00.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://oshea00.github.io/" accesskey="h" title="MikesBlog (Alt + H)">MikesBlog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://oshea00.github.io/archives" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://oshea00.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="https://oshea00.github.io/posts/" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      Building a Hybrid Summary Evaluation Framework
    </h1>
    <div class="post-meta"><span title='2025-09-14 00:00:00 +0000 UTC'>September 14, 2025</span>&nbsp;¬∑&nbsp;7 min&nbsp;¬∑&nbsp;Michael OShea

</div>
  </header> 
  <div class="post-content"><p><figure class="align-center ">
    <img loading="lazy" src="images/cutecomputereditor.png#center"/> 
</figure>

<em>Combining deterministic NLP with LLM-as-Judge for robust evaluation</em></p>
<h2 id="summary-evaluation-challenges">Summary Evaluation Challenges<a hidden class="anchor" aria-hidden="true" href="#summary-evaluation-challenges">#</a></h2>
<p>Summary evaluation metrics sometimes fall short in capturing the qualities most relevant to assessing summary quality. Traditional machine learning for natural language processing (NLP)
has covered a lot of ground in this area. Widely used measures such as ROUGE focus on surface-level token overlap and n-gram matches. While effective for evaluating lexical similarity, these approaches offer limited insight into aspects such as factual accuracy or semantic completeness [1].</p>
<p>For example, consider two summaries that state ‚ÄúThe study found 73% improvement‚Äù and ‚ÄúNearly three-quarters showed positive outcomes.‚Äù Both express the same underlying fact, yet traditional metrics treat them as distinct. This illustrates how lexical approaches can struggle to account for semantic equivalence and factual consistency.</p>
<p>I did some experimentation with the Natural Language Toolkit (NLTK) and managed to get a good first cut working with the traditional ML approach, but my accuracy scoring was falling short on many edge cases such as the one above, and others described in [1]. So, a new experiment was needed.</p>
<hr>
<h2 id="a-hybrid-approach">A Hybrid Approach<a hidden class="anchor" aria-hidden="true" href="#a-hybrid-approach">#</a></h2>
<p>Initial explorations with established NLP metrics demonstrated their strengths and limitations, but extending these approaches to semantic accuracy was not robust. It seemed LLM-based evaluation might be better for semantic assessment with good prompt design.</p>
<p>At the same time, relying solely on LLM-based evaluation for the other more deterministic scores related to coherence and completeness seemed like overkill. When considering the different quality dimensions, I considered a tailored &ldquo;hybrid&rdquo; approach:</p>
<ul>
<li><strong>Accuracy</strong>: Dependent on semantic understanding and factual consistency assessment - LLM</li>
<li><strong>Completeness</strong>: Amenable to topic extraction and coverage analysis - NLTK</li>
<li><strong>Coherence</strong>: Supported by established linguistic measures - ROUGE</li>
<li><strong>Overall Score</strong>: Using weighted average of the above components.</li>
</ul>
<p>This seemed to be worth exploring with some experimentation.</p>
<hr>
<h2 id="architecture-three-evaluation-paradigms">Architecture: Three Evaluation Paradigms<a hidden class="anchor" aria-hidden="true" href="#architecture-three-evaluation-paradigms">#</a></h2>
<h3 id="accuracy-llm-as-judge-with-structured-prompting">Accuracy: LLM-as-Judge with Structured Prompting<a hidden class="anchor" aria-hidden="true" href="#accuracy-llm-as-judge-with-structured-prompting">#</a></h3>
<p>For accuracy, large language models provide the needed semantic reasoning to assess equivalence and factual consistency. Structured prompting ensures consistent, interpretable judgments:</p>
<pre tabindex="0"><code>prompt = f&#34;Compare the &lt;source&gt; and &lt;summary&gt; provided. Using only the
information provided in the &lt;source&gt;, characterize the accuracy of the
&lt;summary&gt; on a scale of 0=Poor, 1=Fair, 2=Good, or 3=Excellent.
Provide a short rationale for your score.
&lt;source&gt;
{source_text}
&lt;/source&gt;
&lt;summary&gt;
{summary_text}
&lt;/summary&gt;
Format as JSON: {{&#39;score&#39;: int, &#39;rationale&#39;: str}}&#34;
</code></pre><p><strong>Key design considerations:</strong></p>
<ul>
<li><strong>Explicit source constraint</strong> prevents hallucinations or injection of external knowledge</li>
<li><strong>Discrete 0‚Äì3 scale</strong> improves judgment consistency</li>
<li><strong>Required rationale</strong> encourages reasoning and enables debugging</li>
<li><strong>JSON formatting</strong> ensures parseable responses</li>
</ul>
<p>The rationale requirement, in particular, enhances judgment quality by prompting more deliberate decisions.</p>
<hr>
<h3 id="completeness-enhanced-topic-extraction">Completeness: Enhanced Topic Extraction<a hidden class="anchor" aria-hidden="true" href="#completeness-enhanced-topic-extraction">#</a></h3>
<p>Completeness is measured through topic extraction methods that extend beyond TF-IDF. By incorporating document structure knowledge and positional weighting, the evaluation captures salient content more effectively:</p>
<pre tabindex="0"><code># Weight sentences by position and content characteristics
if i == 0 or i == len(sentences) - 1:
    position_weight = 1.5
elif i &lt; len(sentences) * 0.2 or i &gt; len(sentences) * 0.8:
    position_weight = 1.2
else:
    position_weight = 1.0

combined_score = position_weight * tfidf_score * length_weight
</code></pre><p>Semantic similarity (cosine similarity on sentence embeddings) is then used to match extracted topics with summary content. A coverage threshold of 0.4 balances precision and recall. This approach captures topic coverage that lexical matching alone may miss, especially when summaries express concepts using different terminology.</p>
<hr>
<h3 id="coherence-rouge-metrics">Coherence: ROUGE Metrics<a hidden class="anchor" aria-hidden="true" href="#coherence-rouge-metrics">#</a></h3>
<p>For coherence, established computational linguistics methods are sufficient and efficient:</p>
<pre tabindex="0"><code>coherence = (0.4 * semantic + 0.25 * discourse + 0.08 * (rouge1+ rouge2+ rougeL+ lexical))
</code></pre><p><strong>Components:</strong></p>
<ul>
<li><strong>ROUGE-1/2/L</strong>: Official F1 scores using rouge-score library with stemming</li>
<li><strong>Semantic coherence</strong>: Sentence embedding similarities combined with ROUGE-L</li>
<li><strong>Discourse coherence</strong>: Penn Discourse Treebank marker analysis</li>
<li><strong>Lexical diversity</strong>: Type-Token Ratio with windowed normalization</li>
<li><strong>Contradiction penalty</strong>: 1.0 (no contradictions) to 0.0 (many contradictions)</li>
<li><strong>Readability</strong>: Flesch-Kincaid grade normalized as max(0, min(1, (20-grade)/20))</li>
<li><strong>Length penalty</strong>: 1.0 (normal length) to 0.3 minimum (very short texts)</li>
</ul>
<p>Deterministic methods are appropriate here because coherence has objective, measurable properties and reliable metrics already exist.</p>
<h3 id="overall-score-weighting-rationale">Overall Score Weighting Rationale<a hidden class="anchor" aria-hidden="true" href="#overall-score-weighting-rationale">#</a></h3>
<p>I chose a 60/25/15 weighting across accuracy, completeness, and coherence to reflect practical priorities:  <em>(0.6 * Accuracy + 0.25 * Completeness + 0.15 * Coherence)</em>.</p>
<ul>
<li><strong>Accuracy</strong> receives the greatest emphasis, since factual errors undermine trust.</li>
<li><strong>Completeness</strong> is weighted significantly, as omission of key information reduces usefulness.</li>
<li><strong>Coherence</strong> is weighted lower, since awkward phrasing is less harmful than inaccuracy.</li>
</ul>
<p><em>These weights can be tuned for domain-specific applications.</em></p>
<hr>
<h2 id="prompt-engineering-for-consistent-judgments">Prompt Engineering for Consistent Judgments<a hidden class="anchor" aria-hidden="true" href="#prompt-engineering-for-consistent-judgments">#</a></h2>
<p>Reliability in accuracy scoring depends heavily on careful prompt design. Critical elements include:</p>
<ul>
<li>Constraining evaluation to provided source material</li>
<li>Using a discrete scale rather than continuous ranges</li>
<li>Requiring explicit reasoning</li>
<li>Enforcing structured output</li>
</ul>
<p>Among these, the source constraint proved particularly important, as unconstrained prompts encouraged models to draw on external knowledge rather than evaluate the given material.</p>
<h2 id="performance-characteristics">Performance Characteristics<a hidden class="anchor" aria-hidden="true" href="#performance-characteristics">#</a></h2>
<p>The hybrid framework yields overall score distributions that align with intuitive quality assessments:</p>
<ul>
<li><strong>0.8‚Äì1.0</strong>: High-quality summaries with minimal issues</li>
<li><strong>0.6‚Äì0.8</strong>: Good summaries with some gaps in completeness or flow</li>
<li><strong>0.4‚Äì0.6</strong>: Usable but with noticeable issues</li>
<li><strong>0.0‚Äì0.4</strong>: Significant problems in accuracy, completeness, or coherence</li>
</ul>
<p>The system captures error types that purely algorithmic methods tend to miss:</p>
<ul>
<li>Semantic paraphrasing</li>
<li>Factual contradictions expressed with different wording</li>
<li>Important topic omissions despite high lexical overlap</li>
<li>Issues with discourse and readability</li>
</ul>
<p>The overall scoring establishes levels that can be used to flag summaries for human evaluators to more closely examine.</p>
<hr>
<h2 id="summary-evaluation-using-a-standard-baseline">Summary Evaluation Using a Standard Baseline<a hidden class="anchor" aria-hidden="true" href="#summary-evaluation-using-a-standard-baseline">#</a></h2>
<p>To test this framework I chose to use the <a href="https://huggingface.co/datasets/abisee/cnn_dailymail">CNN/DailyMail</a> dataset which contains 287K training articles and summaries of news articles. This is a standard used in many summary evaluation
experiments [1].</p>
<h3 id="summary-evaluation-tool-for-cnndailymail-dataset">Summary Evaluation Tool for CNN/DailyMail Dataset<a hidden class="anchor" aria-hidden="true" href="#summary-evaluation-tool-for-cnndailymail-dataset">#</a></h3>
<p>This module provides functionality to evaluate reference summaries from the CNN/DailyMail dataset
using a comprehensive SummaryEvaluator with ROUGE-enhanced coherence scoring [2].</p>
<p><strong>Key Features</strong>:</p>
<ul>
<li>Loads and samples from CNN/DailyMail dataset (287k+ articles)</li>
<li>Evaluates summaries across three dimensions:
<ul>
<li>Accuracy: LLM-as-judge scoring using OpenAI models (GPT-4, GPT-4o)</li>
<li>Completeness: Topic coverage analysis using TF-IDF and Semantic similarity</li>
<li>Coherence: ROUGE-enhanced scoring with discourse analysis and readability</li>
</ul>
</li>
<li>Flexible output formats: pretty console display or CSV export</li>
<li>Configurable sampling with optional seeding for reproducibility</li>
<li>Command-line interface with comprehensive argument parsing</li>
</ul>
<p><strong>Usage Examples</strong>:</p>
<pre tabindex="0"><code>python evalsummary.py                      # default 5 samples, GPT-4o, pretty output
python evalsummary.py -n 10 -m gpt-4.1     # 10 samples with GPT-4.1```
python evalsummary.py -f csv &gt; results.csv # CSV export for analysis```
python evalsummary.py --seed 42 -n 20      # Reproducible results```
</code></pre><h2 id="example-output">Example output<a hidden class="anchor" aria-hidden="true" href="#example-output">#</a></h2>
<pre tabindex="0"><code>üì• Loading CNN/DailyMail dataset...
‚úÖ Dataset loaded - Train: 287113
üöÄ Evaluating 1 random reference summaries using gpt-4o
================================================================================
Article 1 (index 221292):
Bradford midfielder Billy Knott has taken to Twitter to thank John Terry for 
giving his dad a signed shirt following his side&#39;s stunning 4-2 FA Cup win 
against Chelsea on Saturday. The former Chelsea youngster was still revelling 
in the win on Sunday morning as he posed with his father, Steve, and the 
famous No 26 shirt of the Blues captain. Knott tweeted: &#39;Thanks to jt top man
give the shirt to my dad. What a day that was every were we goooo [sic].&#39;
Billy Knott shows off his signed shirt from...

Reference Summary:
Billy Knott received signed shirt from John Terry following Bradford&#39;s 
4-2 FA Cup win against Chelsea. Knott took to Twitter to thank &#39;top man&#39; Terry
for the gift. Bantams are through to FA Cup fifth round following memorable win.
--------------------------------------------------------------------------------

üìä SUMMARY EVALUATION RESULTS:
----------------------------------------
üéØ Accuracy:     1.000 (100.0%)
üìã Completeness: 0.532 (53.2%)
üîó Coherence:    0.437 (43.7%)
‚≠ê Overall:      0.799 (79.9%)

üí≠ Accuracy Rationale: The summary accurately captures the key points
from the source: Billy Knott received a signed shirt from John Terry,
he thanked Terry on Twitter, and Bradford&#39;s win against Chelsea allowed
them to progress to the FA Cup fifth round. The summary is concise and includes
all the main elements mentioned in the source.
</code></pre><h2 id="conclusion">Conclusion<a hidden class="anchor" aria-hidden="true" href="#conclusion">#</a></h2>
<p>The hybrid framework combines evaluation paradigms to provide robust, semantically aware summary assessment. It leverages LLMs for accuracy, structured NLP methods for completeness, and classical measures for coherence, producing a more comprehensive evaluation than any single method alone.</p>
<p>The result is an approach that balances semantic nuance with consistency and efficiency, offering a flexible foundation for summary evaluation across diverse domains. With overall scoring establishing levels that can be used to flag summaries for human evaluators to review.</p>
<hr>
<h2 id="references">References<a hidden class="anchor" aria-hidden="true" href="#references">#</a></h2>
<p>[1] Fabbri, A. R., et al. (2020). SummEval: Re-evaluating Summarization Evaluation. <em>arXiv preprint arXiv:2007.12626</em>. <a href="https://arxiv.org/pdf/2007.12626">https://arxiv.org/pdf/2007.12626</a></p>
<p>[2] O&rsquo;Shea, M. (2024). Summary Evaluation Framework. <em>GitHub repository</em>. <a href="https://github.com/oshea00/summaryeval">https://github.com/oshea00/summaryeval</a></p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://oshea00.github.io/tags/llm/">Llm</a></li>
      <li><a href="https://oshea00.github.io/tags/openai/">Openai</a></li>
      <li><a href="https://oshea00.github.io/tags/prompt-engineering/">Prompt Engineering</a></li>
      <li><a href="https://oshea00.github.io/tags/summarization/">Summarization</a></li>
    </ul>
<nav class="paginav">
  <a class="next" href="https://oshea00.github.io/posts/agentic-codegen-part2/">
    <span class="title">Next ¬ª</span>
    <br>
    <span>Vibecoding an Agentic Coder - Part 2</span>
  </a>
</nav>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Building a Hybrid Summary Evaluation Framework on x"
            href="https://x.com/intent/tweet/?text=Building%20a%20Hybrid%20Summary%20Evaluation%20Framework&amp;url=https%3a%2f%2foshea00.github.io%2fposts%2fsummary-evals%2f&amp;hashtags=llm%2copenai%2cpromptengineering%2csummarization">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Building a Hybrid Summary Evaluation Framework on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2foshea00.github.io%2fposts%2fsummary-evals%2f&amp;title=Building%20a%20Hybrid%20Summary%20Evaluation%20Framework&amp;summary=Building%20a%20Hybrid%20Summary%20Evaluation%20Framework&amp;source=https%3a%2f%2foshea00.github.io%2fposts%2fsummary-evals%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Building a Hybrid Summary Evaluation Framework on reddit"
            href="https://reddit.com/submit?url=https%3a%2f%2foshea00.github.io%2fposts%2fsummary-evals%2f&title=Building%20a%20Hybrid%20Summary%20Evaluation%20Framework">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Building a Hybrid Summary Evaluation Framework on facebook"
            href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2foshea00.github.io%2fposts%2fsummary-evals%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Building a Hybrid Summary Evaluation Framework on whatsapp"
            href="https://api.whatsapp.com/send?text=Building%20a%20Hybrid%20Summary%20Evaluation%20Framework%20-%20https%3a%2f%2foshea00.github.io%2fposts%2fsummary-evals%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Building a Hybrid Summary Evaluation Framework on telegram"
            href="https://telegram.me/share/url?text=Building%20a%20Hybrid%20Summary%20Evaluation%20Framework&amp;url=https%3a%2f%2foshea00.github.io%2fposts%2fsummary-evals%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Building a Hybrid Summary Evaluation Framework on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=Building%20a%20Hybrid%20Summary%20Evaluation%20Framework&u=https%3a%2f%2foshea00.github.io%2fposts%2fsummary-evals%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="https://oshea00.github.io/">MikesBlog</a></span> ¬∑ 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
