<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Fine-tuning Llama3 | MikesBlog</title>
<meta name="keywords" content="llm, tuning, llama3">
<meta name="description" content="Since Llama3 was released, the PyTorch llama3 documentation has a few glitches pointing at configurations in torchtune that are still referencing Llama2. The meta website is a little more up-to-date, but the documentation is a little light on details. So, I wrote this article to bring everything together.
Prerequisites You&rsquo;ll want to use Python 3.11 until Torch compile supports Python 3.12 , and I recommend setting up a virtual environment for this using venv or pipenv.">
<meta name="author" content="Michael OShea">
<link rel="canonical" href="https://oshea00.github.io/posts/finetuning-llama3-8b/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.5ff2630c4d1b3e25bc21f0ecd96681dbcf58219e741fa627857820b5485cb770.css" integrity="sha256-X/JjDE0bPiW8IfDs2WaB289YIZ50H6YnhXggtUhct3A=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://oshea00.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://oshea00.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://oshea00.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://oshea00.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://oshea00.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://oshea00.github.io/posts/finetuning-llama3-8b/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
  

<meta property="og:title" content="Fine-tuning Llama3" />
<meta property="og:description" content="Since Llama3 was released, the PyTorch llama3 documentation has a few glitches pointing at configurations in torchtune that are still referencing Llama2. The meta website is a little more up-to-date, but the documentation is a little light on details. So, I wrote this article to bring everything together.
Prerequisites You&rsquo;ll want to use Python 3.11 until Torch compile supports Python 3.12 , and I recommend setting up a virtual environment for this using venv or pipenv." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://oshea00.github.io/posts/finetuning-llama3-8b/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-05-11T00:00:00+00:00" />
<meta property="article:modified_time" content="2024-05-11T00:00:00+00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Fine-tuning Llama3"/>
<meta name="twitter:description" content="Since Llama3 was released, the PyTorch llama3 documentation has a few glitches pointing at configurations in torchtune that are still referencing Llama2. The meta website is a little more up-to-date, but the documentation is a little light on details. So, I wrote this article to bring everything together.
Prerequisites You&rsquo;ll want to use Python 3.11 until Torch compile supports Python 3.12 , and I recommend setting up a virtual environment for this using venv or pipenv."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://oshea00.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Fine-tuning Llama3",
      "item": "https://oshea00.github.io/posts/finetuning-llama3-8b/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Fine-tuning Llama3",
  "name": "Fine-tuning Llama3",
  "description": "Since Llama3 was released, the PyTorch llama3 documentation has a few glitches pointing at configurations in torchtune that are still referencing Llama2. The meta website is a little more up-to-date, but the documentation is a little light on details. So, I wrote this article to bring everything together.\nPrerequisites You\u0026rsquo;ll want to use Python 3.11 until Torch compile supports Python 3.12 , and I recommend setting up a virtual environment for this using venv or pipenv.",
  "keywords": [
    "llm", "tuning", "llama3"
  ],
  "articleBody": "Since Llama3 was released, the PyTorch llama3 documentation has a few glitches pointing at configurations in torchtune that are still referencing Llama2. The meta website is a little more up-to-date, but the documentation is a little light on details. So, I wrote this article to bring everything together.\nPrerequisites You’ll want to use Python 3.11 until Torch compile supports Python 3.12 , and I recommend setting up a virtual environment for this using venv or pipenv. Install torchtune pip install torchtune Install EleutherAI’s Evaluation Harness pip install lm_eval==0.4.* Download Llama3-8B model You will need to get access to Llama3 via instructions on the official Meta Llama3 page. You will also need your Hugging Face token setup from here.\nNote: some examples here reference “checkpoint-directory”. This will be the directory where your downloaded model weights are stored. In the following examples we’ll use /tmp/Meta-Llama-3-8B for the checkpoint directory.\n1 2 3 tune download meta-llama/Meta-Llama-3-8B \\ --output-dir /tmp/Meta-Llama-3-8B \\ --hf-token $HF_TOKEN Fine-tune the model The out-of-the-box recipe for torchtune single-GPU script tuning uses the Stanford Alpaca dataset, which has 52K instruction-following prompt pairs. It’s worth looking this over if you want to provide your own data, but for now, we’ll use the default recipe.\nGet some coffee. This process will take, depending on your GPU, at least a couple of hours on a single-GPU. I’m running a 24GB NVIDIA RTX 4090, and this process took three hours.\nWith less VRAM and a lighter-weight GPU, this could take up to 16 hours or more. There are instructions on the Meta Llama3, and the Llama3 PyTorch torchtune site that discuss running on multiple-GPU systems and tuning for smaller GPUs.\n1 2 3 4 tune run lora_finetune_single_device --config llama3/8B_lora_single_device \\ checkpointer.checkpoint_dir=/tmp/Meta-Llama-3-8B/original \\ tokenizer.path=/tmp/Meta-Llama-3-8B/original/tokenizer.model \\ checkpointer.output_dir=/tmp/Meta-Llama-3-8B Tuning run results When completed, the above command will place meta_model_0.pt and adapter_0.pt files in the checkpoint directory.\nEvaluating the tuned model To run evaluations, you can use torchtune to make copies of the various elleuther_evaluation config files, then edit them to reflect where to look for models and which merics to run.\nFor example\ntune cp eleuther_evaluation ./custom_eval_config.yaml However, the instructions I found on the PyTorch end-to-end workflow needed fixing, so I have provided already edited copies of these files for our use here.\nBaseline Evaluation of Un-tuned Llama3-8B custom_eval_config_orig.yaml\n# Config for EleutherEvalRecipe in eleuther_eval.py # # To launch, run the following command from root torchtune directory: # tune run eleuther_eval --config eleuther_evaluation tasks=[\"truthfulqa_mc2\",\"hellaswag\"] # Model Arguments model: _component_: torchtune.models.llama3.llama3_8b checkpointer: _component_: torchtune.utils.FullModelMetaCheckpointer checkpoint_dir: /tmp/Meta-Llama-3-8B/original checkpoint_files: [ consolidated.00.pth ] recipe_checkpoint: null output_dir: /tmp/Meta-Llama-3-8B/original model_type: LLAMA3 # Tokenizer tokenizer: _component_: torchtune.models.llama3.llama3_tokenizer path: /tmp/Meta-Llama-3-8B/original/tokenizer.model # Environment device: cuda dtype: bf16 seed: 217 # EleutherAI specific eval args tasks: [\"truthfulqa_mc2\"] limit: null max_seq_length: 4096 # Quantization specific args quantizer: null Run\ntune run eleuther_eval --config ./custom_eval_config_orig.yaml On the 24GB RTX 4090 this takes about four minutes, and the output looks like this. We get about 43.9% accuracy. Fine-Tuned Llama 8B Evaluation custom_eval_config.yaml\n# Config for EleutherEvalRecipe in eleuther_eval.py # # To launch, run the following command from root torchtune directory: # tune run eleuther_eval --config eleuther_evaluation tasks=[\"truthfulqa_mc2\",\"hellaswag\"] # Model Arguments model: _component_: torchtune.models.llama3.llama3_8b checkpointer: _component_: torchtune.utils.FullModelMetaCheckpointer checkpoint_dir: /tmp/Meta-Llama-3-8B checkpoint_files: [ meta_model_0.pt ] recipe_checkpoint: null output_dir: /tmp/Meta-Llama-3-8B model_type: LLAMA3 # Tokenizer tokenizer: _component_: torchtune.models.llama3.llama3_tokenizer path: /tmp/Meta-Llama-3-8B/original/tokenizer.model # Environment device: cuda dtype: bf16 seed: 217 # EleutherAI specific eval args tasks: [\"truthfulqa_mc2\"] limit: null max_seq_length: 4096 # Quantization specific args quantizer: null Note in the above config we are now pointing at the location of the fine-tuned weights meta_model_0.pt\nRun\ntune run eleuther_eval --config ./custom_eval_config.yaml The output looks like this. We get 55.3% accuracy. An increase of about 11.4%! Model Generation Now for the fun part. Seeing how the fine-tuned model handles prompts. We will use a top_k=300 and a temperature=0.6 for these tests. I noticed that temperature=0.8 definitely produces hallucinatory output.\nFine-tuned Llama-8B Generation custom_generation_config.yaml\n# Config for EleutherEvalRecipe in eleuther_eval.py # # To launch, run the following command from root torchtune directory: # tune run eleuther_eval --config eleuther_evaluation tasks=[\"truthfulqa_mc2\",\"hellaswag\"] # Model Arguments model: _component_: torchtune.models.llama3.llama3_8b checkpointer: _component_: torchtune.utils.FullModelMetaCheckpointer checkpoint_dir: /tmp/Meta-Llama-3-8B checkpoint_files: [ meta_model_0.pt ] recipe_checkpoint: null output_dir: /tmp/Meta-Llama-3-8B model_type: LLAMA3 # Tokenizer tokenizer: _component_: torchtune.models.llama3.llama3_tokenizer path: /tmp/Meta-Llama-3-8B/original/tokenizer.model # Environment device: cuda dtype: bf16 seed: 217 # Quantization specific args quantizer: null # Generation arguments; defaults taken from gpt-fast # prompt: \"Hello, my name is\" max_new_tokens: 600 temperature: 0.6 # 0.8 and 0.6 are popular values to try top_k: 300 Run\ntune run generate --config ./custom_generation_config.yaml \\ prompt=\"What are some interesting sites to visit in the Bay Area?\" So, this works, but we still have to load up 16GB worth of weights and run inference, which takes about 60 seconds on my rig. Your mileage may vary. Let’s see if we can “quantize” our weights to speed up load time, and the inference time.\nQuantization Quantizing the model weights involves reducing the weights to smaller integer types. There are many algorithms, but we will use one of the standard torchtune recipes using TORCHAO to produce an ‘INT4’ quantization.\nINT4 config custom_quantization_config.yaml\n# Config for EleutherEvalRecipe in eleuther_eval.py # # To launch, run the following command from root torchtune directory: # tune run eleuther_eval --config eleuther_evaluation tasks=[\"truthfulqa_mc2\",\"hellaswag\"] # Model Arguments model: _component_: torchtune.models.llama3.llama3_8b checkpointer: _component_: torchtune.utils.FullModelMetaCheckpointer checkpoint_dir: /tmp/Meta-Llama-3-8B checkpoint_files: [ meta_model_0.pt ] recipe_checkpoint: null output_dir: /tmp/Meta-Llama-3-8B model_type: LLAMA3 # Tokenizer tokenizer: _component_: torchtune.models.llama3.llama3_tokenizer path: /tmp/Meta-Llama-3-8B/original/tokenizer.model # Environment device: cuda dtype: bf16 seed: 217 quantizer: _component_: torchtune.utils.quantization.Int4WeightOnlyQuantizer groupsize: 256 Run\ntune run quantize --config ./custom_quantization_config.yaml This runs fairly quickly, producing a meta_model_0-4w.pt weights file of only 4.92GB in the checkpoint directory. Generation using the quantized model OK! Let’s see how much faster we can run things, but keep in mind that the INT4 version of the weights will reduce the model’s language performance somewhat.\nQuantized generation configuration custom_generation_4w_config.yaml\n# Config for EleutherEvalRecipe in eleuther_eval.py # # To launch, run the following command from root torchtune directory: # tune run eleuther_eval --config eleuther_evaluation tasks=[\"truthfulqa_mc2\",\"hellaswag\"] # Model Arguments model: _component_: torchtune.models.llama3.llama3_8b checkpointer: _component_: torchtune.utils.FullModelTorchTuneCheckpointer checkpoint_dir: /tmp/Meta-Llama-3-8B checkpoint_files: [ meta_model_0-4w.pt ] recipe_checkpoint: null output_dir: /tmp/Meta-Llama-3-8B model_type: LLAMA3 # Tokenizer tokenizer: _component_: torchtune.models.llama3.llama3_tokenizer path: /tmp/Meta-Llama-3-8B/original/tokenizer.model # Environment device: cuda dtype: bf16 seed: 217 # Quantization specific args quantizer: _component_: torchtune.utils.quantization.Int4WeightOnlyQuantizer groupsize: 256 # Generation arguments; defaults taken from gpt-fast # prompt: \"Hello, my name is\" max_new_tokens: 600 temperature: 0.6 # 0.8 and 0.6 are popular values to try top_k: 300 Note there are some key differences in this generation configuration file with the non-quantized generation file we used earlier on the fine-tuned model. Namely, we are pointing at the meta_model_0-4w.pt weights now. Also, we must provide the quantizer details matching the ones we used in the quantization step.\nRun\ntune run generate --config ./custom_generation_4w_config.yaml \\ prompt=\"What are some interesting sites to visit in the Bay Area?\" Here is what I got It takes about 22 seconds to load up the model, and a mere 7 seconds to run the inference. This is about a 300% improvement over the un-quantized generation run, and the output also looks pretty good.\nHowever, let’s see if we can evaluate this quantized model’s performance on instruction following to see if the quantization has affected the accuracy of the model.\nQuantized evaluation configuration custom_eval_4w_config.yaml\n# Config for EleutherEvalRecipe in eleuther_eval.py # # To launch, run the following command from root torchtune directory: # tune run eleuther_eval --config eleuther_evaluation tasks=[\"truthfulqa_mc2\",\"hellaswag\"] # Model Arguments model: _component_: torchtune.models.llama3.llama3_8b checkpointer: _component_: torchtune.utils.FullModelTorchTuneCheckpointer checkpoint_dir: /tmp/Meta-Llama-3-8B checkpoint_files: [ meta_model_0-4w.pt ] recipe_checkpoint: null output_dir: /tmp/Meta-Llama-3-8B model_type: LLAMA3 # Tokenizer tokenizer: _component_: torchtune.models.llama3.llama3_tokenizer path: /tmp/Meta-Llama-3-8B/original/tokenizer.model # Environment device: cuda dtype: bf16 seed: 217 # EleutherAI specific eval args tasks: [\"truthfulqa_mc2\"] limit: null max_seq_length: 4096 # Quantization specific args quantizer: _component_: torchtune.utils.quantization.Int4WeightOnlyQuantizer groupsize: 256 Note in the above config we have changed the checkpointer to FullModelTorchTuneCheckpointer, as this checkpointer can support the weights_only=true checkpoint file we created with the Int4WeightOnlyQuantizer when we quantized the model. We have also added the quantizer details.\nRun\ntune run eleuther_eval --config ./custom_eval_4w_config.yaml Here are the results showing we got an accuracy of 49% with the quantized, fine-tuned, model. This is a net increase of 5% over the baseline non-finetuned Llama3 model. By quantizing, we “took back” about 6% of the accuracy we gained in the fine-tuning of the model.\nSo, we traded some accuracy for performance, but we still ended up with an overall improvement. This shows the importance of adding evaluation benchmarking when fine-tuning LLM models.\nWe Made It! If you followed along this far, congratulations 🎉!\nI hope you had as much fun as I did. Next, I’ll cover how to encode this smaller model into GGUF format and post it to a Huggingface repository to share with others.\n",
  "wordCount" : "1444",
  "inLanguage": "en",
  "datePublished": "2024-05-11T00:00:00Z",
  "dateModified": "2024-05-11T00:00:00Z",
  "author":[{
    "@type": "Person",
    "name": "Michael OShea"
  }],
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://oshea00.github.io/posts/finetuning-llama3-8b/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "MikesBlog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://oshea00.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://oshea00.github.io/" accesskey="h" title="MikesBlog (Alt + H)">MikesBlog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://oshea00.github.io/archives" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://oshea00.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="https://oshea00.github.io/posts/" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      Fine-tuning Llama3
    </h1>
    <div class="post-meta"><span title='2024-05-11 00:00:00 +0000 UTC'>May 11, 2024</span>&nbsp;·&nbsp;7 min&nbsp;·&nbsp;Michael OShea

</div>
  </header> 
  <div class="post-content"><p>Since Llama3 was released, the <a href="https://pytorch.org/torchtune/stable/tutorials/llama3.html">PyTorch llama3</a> documentation has a few glitches pointing at configurations in torchtune that are still referencing Llama2. <a href="https://llama.meta.com/docs/how-to-guides/fine-tuning/">The meta website</a> is a little more up-to-date, but the documentation is a little light on details. So, I
wrote this article to bring everything together.</p>
<h2 id="prerequisites">Prerequisites<a hidden class="anchor" aria-hidden="true" href="#prerequisites">#</a></h2>
<ul>
<li>You&rsquo;ll want to use Python 3.11 <a href="https://github.com/pytorch/pytorch/issues/120233">until Torch compile supports Python 3.12</a> , and I recommend setting up a virtual environment for this using <code>venv</code> or <code>pipenv</code>.</li>
<li>Install <a href="https://pytorch.org/torchtune/stable/install.html#install-label">torchtune</a></li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">pip install torchtune
</span></span></code></pre></div><ul>
<li>Install <a href="https://github.com/EleutherAI/lm-evaluation-harness">EleutherAI&rsquo;s Evaluation Harness</a></li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">pip install <span class="nv">lm_eval</span><span class="o">==</span>0.4.*
</span></span></code></pre></div><h2 id="download-llama3-8b-model">Download Llama3-8B model<a hidden class="anchor" aria-hidden="true" href="#download-llama3-8b-model">#</a></h2>
<p>You will need to get access to Llama3 via instructions on the <a href="https://github.com/meta-llama/llama3/blob/main/README.md">official Meta Llama3</a> page. You will also need your Hugging Face token setup from <a href="https://huggingface.co/settings/tokens">here</a>.</p>
<p>Note: some examples here reference &ldquo;checkpoint-directory&rdquo;. This will be the directory where your downloaded model weights are stored. In the following examples we&rsquo;ll use <code>/tmp/Meta-Llama-3-8B</code> for the checkpoint directory.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">tune download meta-llama/Meta-Llama-3-8B <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>--output-dir /tmp/Meta-Llama-3-8B <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>--hf-token <span class="nv">$HF_TOKEN</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="fine-tune-the-model">Fine-tune the model<a hidden class="anchor" aria-hidden="true" href="#fine-tune-the-model">#</a></h2>
<p>The out-of-the-box recipe for torchtune single-GPU script tuning uses the <a href="https://github.com/tatsu-lab/stanford_alpaca">Stanford Alpaca dataset</a>, which has 52K instruction-following prompt pairs. It&rsquo;s worth looking this over if you want to
provide your own data, but for now, we&rsquo;ll use the default recipe.</p>
<p>Get some coffee. This process will take, depending on your GPU, at least a couple of hours on a single-GPU. I&rsquo;m running a 24GB NVIDIA RTX 4090, and this process took three hours.</p>
<p>With less VRAM and a lighter-weight GPU, this could take up to 16 hours or more. There are instructions on the <a href="https://llama.meta.com/docs/how-to-guides/fine-tuning">Meta Llama3</a>, and
the <a href="https://pytorch.org/torchtune/stable/tutorials/llama3.html">Llama3 PyTorch torchtune</a> site that discuss running on multiple-GPU systems and tuning for smaller GPUs.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">tune run lora_finetune_single_device --config llama3/8B_lora_single_device <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>    checkpointer.checkpoint_dir<span class="o">=</span>/tmp/Meta-Llama-3-8B/original <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>    tokenizer.path<span class="o">=</span>/tmp/Meta-Llama-3-8B/original/tokenizer.model <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>    checkpointer.output_dir<span class="o">=</span>/tmp/Meta-Llama-3-8B
</span></span></code></pre></td></tr></table>
</div>
</div><h3 id="tuning-run-results">Tuning run results<a hidden class="anchor" aria-hidden="true" href="#tuning-run-results">#</a></h3>
<p>When completed, the above command will place <code>meta_model_0.pt</code> and <code>adapter_0.pt</code> files in the checkpoint directory.</p>
<p><img loading="lazy" src="images/llama3_7b_finetune.png" alt="tuning run"  />
</p>
<h2 id="evaluating-the-tuned-model">Evaluating the tuned model<a hidden class="anchor" aria-hidden="true" href="#evaluating-the-tuned-model">#</a></h2>
<p>To run evaluations, you can use torchtune to make copies of the various elleuther_evaluation config files, then edit them to reflect where to look for models and which merics to run.</p>
<p>For example</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">tune cp eleuther_evaluation ./custom_eval_config.yaml
</span></span></code></pre></div><p>However, the instructions I found on the <a href="https://pytorch.org/torchtune/stable/tutorials/e2e_flow.html">PyTorch end-to-end workflow</a> needed fixing, so I have provided already edited copies of these files for our use here.</p>
<h3 id="baseline-evaluation-of-un-tuned-llama3-8b">Baseline Evaluation of Un-tuned Llama3-8B<a hidden class="anchor" aria-hidden="true" href="#baseline-evaluation-of-un-tuned-llama3-8b">#</a></h3>
<p><code>custom_eval_config_orig.yaml</code></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl"><span class="c1"># Config for EleutherEvalRecipe in eleuther_eval.py</span>
</span></span><span class="line"><span class="cl"><span class="c1">#</span>
</span></span><span class="line"><span class="cl"><span class="c1"># To launch, run the following command from root torchtune directory:</span>
</span></span><span class="line"><span class="cl"><span class="c1">#    tune run eleuther_eval --config eleuther_evaluation tasks=[&#34;truthfulqa_mc2&#34;,&#34;hellaswag&#34;]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Model Arguments</span>
</span></span><span class="line"><span class="cl">model:
</span></span><span class="line"><span class="cl">  _component_: torchtune.models.llama3.llama3_8b
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">checkpointer:
</span></span><span class="line"><span class="cl">  _component_: torchtune.utils.FullModelMetaCheckpointer
</span></span><span class="line"><span class="cl">  checkpoint_dir: /tmp/Meta-Llama-3-8B/original
</span></span><span class="line"><span class="cl">  checkpoint_files: <span class="o">[</span>
</span></span><span class="line"><span class="cl">    consolidated.00.pth
</span></span><span class="line"><span class="cl">  <span class="o">]</span>
</span></span><span class="line"><span class="cl">  recipe_checkpoint: null
</span></span><span class="line"><span class="cl">  output_dir: /tmp/Meta-Llama-3-8B/original
</span></span><span class="line"><span class="cl">  model_type: LLAMA3
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Tokenizer</span>
</span></span><span class="line"><span class="cl">tokenizer:
</span></span><span class="line"><span class="cl">  _component_: torchtune.models.llama3.llama3_tokenizer
</span></span><span class="line"><span class="cl">  path: /tmp/Meta-Llama-3-8B/original/tokenizer.model
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Environment</span>
</span></span><span class="line"><span class="cl">device: cuda
</span></span><span class="line"><span class="cl">dtype: bf16
</span></span><span class="line"><span class="cl">seed: <span class="m">217</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># EleutherAI specific eval args</span>
</span></span><span class="line"><span class="cl">tasks: <span class="o">[</span><span class="s2">&#34;truthfulqa_mc2&#34;</span><span class="o">]</span>
</span></span><span class="line"><span class="cl">limit: null
</span></span><span class="line"><span class="cl">max_seq_length: <span class="m">4096</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Quantization specific args</span>
</span></span><span class="line"><span class="cl">quantizer: null
</span></span></code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">tune run eleuther_eval --config ./custom_eval_config_orig.yaml
</span></span></code></pre></div><p>On the 24GB RTX 4090 this takes about four minutes, and the output looks like this. We get about 43.9% accuracy.
<img loading="lazy" src="images/eleuther_llama3_eval.png" alt="untrained eval"  />
</p>
<h3 id="fine-tuned-llama-8b-evaluation">Fine-Tuned Llama 8B Evaluation<a hidden class="anchor" aria-hidden="true" href="#fine-tuned-llama-8b-evaluation">#</a></h3>
<p><code>custom_eval_config.yaml</code></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl"><span class="c1"># Config for EleutherEvalRecipe in eleuther_eval.py</span>
</span></span><span class="line"><span class="cl"><span class="c1">#</span>
</span></span><span class="line"><span class="cl"><span class="c1"># To launch, run the following command from root torchtune directory:</span>
</span></span><span class="line"><span class="cl"><span class="c1">#    tune run eleuther_eval --config eleuther_evaluation tasks=[&#34;truthfulqa_mc2&#34;,&#34;hellaswag&#34;]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Model Arguments</span>
</span></span><span class="line"><span class="cl">model:
</span></span><span class="line"><span class="cl">  _component_: torchtune.models.llama3.llama3_8b
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">checkpointer:
</span></span><span class="line"><span class="cl">  _component_: torchtune.utils.FullModelMetaCheckpointer
</span></span><span class="line"><span class="cl">  checkpoint_dir: /tmp/Meta-Llama-3-8B
</span></span><span class="line"><span class="cl">  checkpoint_files: <span class="o">[</span>
</span></span><span class="line"><span class="cl">    meta_model_0.pt
</span></span><span class="line"><span class="cl">  <span class="o">]</span>
</span></span><span class="line"><span class="cl">  recipe_checkpoint: null
</span></span><span class="line"><span class="cl">  output_dir: /tmp/Meta-Llama-3-8B
</span></span><span class="line"><span class="cl">  model_type: LLAMA3
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Tokenizer</span>
</span></span><span class="line"><span class="cl">tokenizer:
</span></span><span class="line"><span class="cl">  _component_: torchtune.models.llama3.llama3_tokenizer
</span></span><span class="line"><span class="cl">  path: /tmp/Meta-Llama-3-8B/original/tokenizer.model
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Environment</span>
</span></span><span class="line"><span class="cl">device: cuda
</span></span><span class="line"><span class="cl">dtype: bf16
</span></span><span class="line"><span class="cl">seed: <span class="m">217</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># EleutherAI specific eval args</span>
</span></span><span class="line"><span class="cl">tasks: <span class="o">[</span><span class="s2">&#34;truthfulqa_mc2&#34;</span><span class="o">]</span>
</span></span><span class="line"><span class="cl">limit: null
</span></span><span class="line"><span class="cl">max_seq_length: <span class="m">4096</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Quantization specific args</span>
</span></span><span class="line"><span class="cl">quantizer: null
</span></span></code></pre></div><p>Note in the above config we are now pointing at the location of the fine-tuned weights <code>meta_model_0.pt</code></p>
<p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">tune run eleuther_eval --config ./custom_eval_config.yaml
</span></span></code></pre></div><p>The output looks like this. We get 55.3% accuracy. An increase of about 11.4%!
<img loading="lazy" src="images/eleuther_llama3_finetuned_eval.png" alt="untrained eval"  />
</p>
<h2 id="model-generation">Model Generation<a hidden class="anchor" aria-hidden="true" href="#model-generation">#</a></h2>
<p>Now for the fun part. Seeing how the fine-tuned model handles prompts.
We will use a <code>top_k=300</code> and a <code>temperature=0.6</code> for these tests. I noticed that <code>temperature=0.8</code> definitely produces hallucinatory output.</p>
<h3 id="fine-tuned-llama-8b-generation">Fine-tuned Llama-8B Generation<a hidden class="anchor" aria-hidden="true" href="#fine-tuned-llama-8b-generation">#</a></h3>
<p><code>custom_generation_config.yaml</code></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="c1"># Config for EleutherEvalRecipe in eleuther_eval.py</span>
</span></span><span class="line"><span class="cl"><span class="c1">#</span>
</span></span><span class="line"><span class="cl"><span class="c1"># To launch, run the following command from root torchtune directory:</span>
</span></span><span class="line"><span class="cl"><span class="c1">#    tune run eleuther_eval --config eleuther_evaluation tasks=[&#34;truthfulqa_mc2&#34;,&#34;hellaswag&#34;]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Model Arguments</span>
</span></span><span class="line"><span class="cl">model:
</span></span><span class="line"><span class="cl">  _component_: torchtune.models.llama3.llama3_8b
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">checkpointer:
</span></span><span class="line"><span class="cl">  _component_: torchtune.utils.FullModelMetaCheckpointer
</span></span><span class="line"><span class="cl">  checkpoint_dir: /tmp/Meta-Llama-3-8B
</span></span><span class="line"><span class="cl">  checkpoint_files: <span class="o">[</span>
</span></span><span class="line"><span class="cl">    meta_model_0.pt
</span></span><span class="line"><span class="cl">  <span class="o">]</span>
</span></span><span class="line"><span class="cl">  recipe_checkpoint: null
</span></span><span class="line"><span class="cl">  output_dir: /tmp/Meta-Llama-3-8B
</span></span><span class="line"><span class="cl">  model_type: LLAMA3
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Tokenizer</span>
</span></span><span class="line"><span class="cl">tokenizer:
</span></span><span class="line"><span class="cl">  _component_: torchtune.models.llama3.llama3_tokenizer
</span></span><span class="line"><span class="cl">  path: /tmp/Meta-Llama-3-8B/original/tokenizer.model
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Environment</span>
</span></span><span class="line"><span class="cl">device: cuda
</span></span><span class="line"><span class="cl">dtype: bf16
</span></span><span class="line"><span class="cl">seed: <span class="m">217</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Quantization specific args</span>
</span></span><span class="line"><span class="cl">quantizer: null
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Generation arguments; defaults taken from gpt-fast</span>
</span></span><span class="line"><span class="cl"><span class="c1"># prompt: &#34;Hello, my name is&#34;</span>
</span></span><span class="line"><span class="cl">max_new_tokens: <span class="m">600</span>
</span></span><span class="line"><span class="cl">temperature: 0.6 <span class="c1"># 0.8 and 0.6 are popular values to try</span>
</span></span><span class="line"><span class="cl">top_k: <span class="m">300</span>
</span></span></code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">tune run generate --config ./custom_generation_config.yaml <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span><span class="nv">prompt</span><span class="o">=</span><span class="s2">&#34;What are some interesting sites to visit in the Bay Area?&#34;</span>
</span></span></code></pre></div><p>So, this works, but we still have to load up 16GB worth of weights and run inference, which takes about 60 seconds on my rig. Your mileage may vary. Let&rsquo;s see if we can &ldquo;quantize&rdquo; our weights to speed up load time, and the inference time.</p>
<h2 id="quantization">Quantization<a hidden class="anchor" aria-hidden="true" href="#quantization">#</a></h2>
<p>Quantizing the model weights involves reducing the weights to smaller integer types. There are many algorithms, but we will use one of the standard torchtune recipes using <a href="https://github.com/pytorch/ao">TORCHAO</a> to produce an &lsquo;INT4&rsquo; quantization.</p>
<h3 id="int4-config">INT4 config<a hidden class="anchor" aria-hidden="true" href="#int4-config">#</a></h3>
<p><code>custom_quantization_config.yaml</code></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="c1"># Config for EleutherEvalRecipe in eleuther_eval.py</span>
</span></span><span class="line"><span class="cl"><span class="c1">#</span>
</span></span><span class="line"><span class="cl"><span class="c1"># To launch, run the following command from root torchtune directory:</span>
</span></span><span class="line"><span class="cl"><span class="c1">#    tune run eleuther_eval --config eleuther_evaluation tasks=[&#34;truthfulqa_mc2&#34;,&#34;hellaswag&#34;]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Model Arguments</span>
</span></span><span class="line"><span class="cl">model:
</span></span><span class="line"><span class="cl">  _component_: torchtune.models.llama3.llama3_8b
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">checkpointer:
</span></span><span class="line"><span class="cl">  _component_: torchtune.utils.FullModelMetaCheckpointer
</span></span><span class="line"><span class="cl">  checkpoint_dir: /tmp/Meta-Llama-3-8B
</span></span><span class="line"><span class="cl">  checkpoint_files: <span class="o">[</span>
</span></span><span class="line"><span class="cl">    meta_model_0.pt
</span></span><span class="line"><span class="cl">  <span class="o">]</span>
</span></span><span class="line"><span class="cl">  recipe_checkpoint: null
</span></span><span class="line"><span class="cl">  output_dir: /tmp/Meta-Llama-3-8B
</span></span><span class="line"><span class="cl">  model_type: LLAMA3
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Tokenizer</span>
</span></span><span class="line"><span class="cl">tokenizer:
</span></span><span class="line"><span class="cl">  _component_: torchtune.models.llama3.llama3_tokenizer
</span></span><span class="line"><span class="cl">  path: /tmp/Meta-Llama-3-8B/original/tokenizer.model
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Environment</span>
</span></span><span class="line"><span class="cl">device: cuda
</span></span><span class="line"><span class="cl">dtype: bf16
</span></span><span class="line"><span class="cl">seed: <span class="m">217</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">quantizer:
</span></span><span class="line"><span class="cl">  _component_: torchtune.utils.quantization.Int4WeightOnlyQuantizer
</span></span><span class="line"><span class="cl">  groupsize: <span class="m">256</span>
</span></span></code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">tune run quantize --config ./custom_quantization_config.yaml
</span></span></code></pre></div><p>This runs fairly quickly, producing a <code>meta_model_0-4w.pt</code> weights file of only 4.92GB in the checkpoint directory.
<img loading="lazy" src="images/llama3_quantize_4k.png" alt="quantized run"  />
</p>
<h2 id="generation-using-the-quantized-model">Generation using the quantized model<a hidden class="anchor" aria-hidden="true" href="#generation-using-the-quantized-model">#</a></h2>
<p>OK! Let&rsquo;s see how much faster we can run things, but keep in mind that the INT4 version of the weights will reduce the model&rsquo;s language performance somewhat.</p>
<h3 id="quantized-generation-configuration">Quantized generation configuration<a hidden class="anchor" aria-hidden="true" href="#quantized-generation-configuration">#</a></h3>
<p><code>custom_generation_4w_config.yaml</code></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="c1"># Config for EleutherEvalRecipe in eleuther_eval.py</span>
</span></span><span class="line"><span class="cl"><span class="c1">#</span>
</span></span><span class="line"><span class="cl"><span class="c1"># To launch, run the following command from root torchtune directory:</span>
</span></span><span class="line"><span class="cl"><span class="c1">#    tune run eleuther_eval --config eleuther_evaluation tasks=[&#34;truthfulqa_mc2&#34;,&#34;hellaswag&#34;]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Model Arguments</span>
</span></span><span class="line"><span class="cl">model:
</span></span><span class="line"><span class="cl">  _component_: torchtune.models.llama3.llama3_8b
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">checkpointer:
</span></span><span class="line"><span class="cl">  _component_: torchtune.utils.FullModelTorchTuneCheckpointer
</span></span><span class="line"><span class="cl">  checkpoint_dir: /tmp/Meta-Llama-3-8B
</span></span><span class="line"><span class="cl">  checkpoint_files: <span class="o">[</span>
</span></span><span class="line"><span class="cl">    meta_model_0-4w.pt
</span></span><span class="line"><span class="cl">  <span class="o">]</span>
</span></span><span class="line"><span class="cl">  recipe_checkpoint: null
</span></span><span class="line"><span class="cl">  output_dir: /tmp/Meta-Llama-3-8B
</span></span><span class="line"><span class="cl">  model_type: LLAMA3
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Tokenizer</span>
</span></span><span class="line"><span class="cl">tokenizer:
</span></span><span class="line"><span class="cl">  _component_: torchtune.models.llama3.llama3_tokenizer
</span></span><span class="line"><span class="cl">  path: /tmp/Meta-Llama-3-8B/original/tokenizer.model
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Environment</span>
</span></span><span class="line"><span class="cl">device: cuda
</span></span><span class="line"><span class="cl">dtype: bf16
</span></span><span class="line"><span class="cl">seed: <span class="m">217</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Quantization specific args</span>
</span></span><span class="line"><span class="cl">quantizer:
</span></span><span class="line"><span class="cl">  _component_: torchtune.utils.quantization.Int4WeightOnlyQuantizer
</span></span><span class="line"><span class="cl">  groupsize: <span class="m">256</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Generation arguments; defaults taken from gpt-fast</span>
</span></span><span class="line"><span class="cl"><span class="c1"># prompt: &#34;Hello, my name is&#34;</span>
</span></span><span class="line"><span class="cl">max_new_tokens: <span class="m">600</span>
</span></span><span class="line"><span class="cl">temperature: 0.6 <span class="c1"># 0.8 and 0.6 are popular values to try</span>
</span></span><span class="line"><span class="cl">top_k: <span class="m">300</span>
</span></span></code></pre></div><p>Note there are some key differences in this generation configuration file with the non-quantized generation file we used earlier on the fine-tuned model. Namely, we are pointing at the <code>meta_model_0-4w.pt</code> weights now. Also, we must provide the quantizer details matching the ones we used in the quantization step.</p>
<p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">tune run generate --config ./custom_generation_4w_config.yaml <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span><span class="nv">prompt</span><span class="o">=</span><span class="s2">&#34;What are some interesting sites to visit in the Bay Area?&#34;</span>
</span></span></code></pre></div><p>Here is what I got
<img loading="lazy" src="images/llama3_quantized_gen.png" alt="quantized gen"  />
</p>
<p>It takes about 22 seconds to load up the model, and a mere 7 seconds to run the inference. This is about a 300% improvement over the un-quantized generation run, and the output also looks pretty good.</p>
<p>However, let&rsquo;s see if we can evaluate this quantized model&rsquo;s performance on instruction following to see if the quantization has affected the accuracy of the model.</p>
<h3 id="quantized-evaluation-configuration">Quantized evaluation configuration<a hidden class="anchor" aria-hidden="true" href="#quantized-evaluation-configuration">#</a></h3>
<p><code>custom_eval_4w_config.yaml</code></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="c1"># Config for EleutherEvalRecipe in eleuther_eval.py</span>
</span></span><span class="line"><span class="cl"><span class="c1">#</span>
</span></span><span class="line"><span class="cl"><span class="c1"># To launch, run the following command from root torchtune directory:</span>
</span></span><span class="line"><span class="cl"><span class="c1">#    tune run eleuther_eval --config eleuther_evaluation tasks=[&#34;truthfulqa_mc2&#34;,&#34;hellaswag&#34;]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Model Arguments</span>
</span></span><span class="line"><span class="cl">model:
</span></span><span class="line"><span class="cl">  _component_: torchtune.models.llama3.llama3_8b
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">checkpointer:
</span></span><span class="line"><span class="cl">  _component_: torchtune.utils.FullModelTorchTuneCheckpointer
</span></span><span class="line"><span class="cl">  checkpoint_dir: /tmp/Meta-Llama-3-8B
</span></span><span class="line"><span class="cl">  checkpoint_files: <span class="o">[</span>
</span></span><span class="line"><span class="cl">    meta_model_0-4w.pt
</span></span><span class="line"><span class="cl">  <span class="o">]</span>
</span></span><span class="line"><span class="cl">  recipe_checkpoint: null
</span></span><span class="line"><span class="cl">  output_dir: /tmp/Meta-Llama-3-8B
</span></span><span class="line"><span class="cl">  model_type: LLAMA3
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Tokenizer</span>
</span></span><span class="line"><span class="cl">tokenizer:
</span></span><span class="line"><span class="cl">  _component_: torchtune.models.llama3.llama3_tokenizer
</span></span><span class="line"><span class="cl">  path: /tmp/Meta-Llama-3-8B/original/tokenizer.model
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Environment</span>
</span></span><span class="line"><span class="cl">device: cuda
</span></span><span class="line"><span class="cl">dtype: bf16
</span></span><span class="line"><span class="cl">seed: <span class="m">217</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># EleutherAI specific eval args</span>
</span></span><span class="line"><span class="cl">tasks: <span class="o">[</span><span class="s2">&#34;truthfulqa_mc2&#34;</span><span class="o">]</span>
</span></span><span class="line"><span class="cl">limit: null
</span></span><span class="line"><span class="cl">max_seq_length: <span class="m">4096</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Quantization specific args</span>
</span></span><span class="line"><span class="cl">quantizer:
</span></span><span class="line"><span class="cl">  _component_: torchtune.utils.quantization.Int4WeightOnlyQuantizer
</span></span><span class="line"><span class="cl">  groupsize: <span class="m">256</span>
</span></span></code></pre></div><p>Note in the above config we have changed the checkpointer to <code>FullModelTorchTuneCheckpointer</code>, as this checkpointer can support the <code>weights_only=true</code> checkpoint file we created with the <code>Int4WeightOnlyQuantizer</code> when we quantized the model. We have also added the quantizer details.</p>
<p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">tune run eleuther_eval --config ./custom_eval_4w_config.yaml
</span></span></code></pre></div><p>Here are the results showing we got an accuracy of 49% with the quantized, fine-tuned, model. This is a net increase of 5% over the baseline non-finetuned Llama3 model. By quantizing, we &ldquo;took back&rdquo; about 6% of the accuracy we gained in the fine-tuning of the model.</p>
<p><img loading="lazy" src="images/llama3_quantized_eval.png" alt="quantized eval"  />
</p>
<p>So, we traded some accuracy for performance, but we still ended up with an overall improvement. This shows the importance of adding evaluation benchmarking when fine-tuning LLM models.</p>
<h2 id="we-made-it">We Made It!<a hidden class="anchor" aria-hidden="true" href="#we-made-it">#</a></h2>
<p>If you followed along this far, congratulations 🎉!</p>
<p>I hope you had as much fun as I did. Next, I&rsquo;ll cover how to encode this smaller model into GGUF format and post it to a Huggingface repository to share with others.</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://oshea00.github.io/tags/llm/">Llm</a></li>
      <li><a href="https://oshea00.github.io/tags/tuning/">Tuning</a></li>
      <li><a href="https://oshea00.github.io/tags/llama3/">Llama3</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="https://oshea00.github.io/posts/transformers-positional-encoding/">
    <span class="title">« Prev</span>
    <br>
    <span>Transformers -  Positional Encoding</span>
  </a>
  <a class="next" href="https://oshea00.github.io/posts/openai-compatible-apis/">
    <span class="title">Next »</span>
    <br>
    <span>OpenAI Python API Compatibility</span>
  </a>
</nav>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Fine-tuning Llama3 on x"
            href="https://x.com/intent/tweet/?text=Fine-tuning%20Llama3&amp;url=https%3a%2f%2foshea00.github.io%2fposts%2ffinetuning-llama3-8b%2f&amp;hashtags=llm%2ctuning%2cllama3">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Fine-tuning Llama3 on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2foshea00.github.io%2fposts%2ffinetuning-llama3-8b%2f&amp;title=Fine-tuning%20Llama3&amp;summary=Fine-tuning%20Llama3&amp;source=https%3a%2f%2foshea00.github.io%2fposts%2ffinetuning-llama3-8b%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Fine-tuning Llama3 on reddit"
            href="https://reddit.com/submit?url=https%3a%2f%2foshea00.github.io%2fposts%2ffinetuning-llama3-8b%2f&title=Fine-tuning%20Llama3">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Fine-tuning Llama3 on facebook"
            href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2foshea00.github.io%2fposts%2ffinetuning-llama3-8b%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Fine-tuning Llama3 on whatsapp"
            href="https://api.whatsapp.com/send?text=Fine-tuning%20Llama3%20-%20https%3a%2f%2foshea00.github.io%2fposts%2ffinetuning-llama3-8b%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Fine-tuning Llama3 on telegram"
            href="https://telegram.me/share/url?text=Fine-tuning%20Llama3&amp;url=https%3a%2f%2foshea00.github.io%2fposts%2ffinetuning-llama3-8b%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Fine-tuning Llama3 on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=Fine-tuning%20Llama3&u=https%3a%2f%2foshea00.github.io%2fposts%2ffinetuning-llama3-8b%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2024 <a href="https://oshea00.github.io/">MikesBlog</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
