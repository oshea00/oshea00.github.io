<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Classify With Confidence | MikesBlog</title>
<meta name="keywords" content="llm, openai, prompt engineering, classification">
<meta name="description" content="
     


Large foundation models like GPT can classify text according to a well-crafted prompt instruction, and it&rsquo;s remarkable how well they can do this, considering there has been no explicit training with labeled datasets. This has traditionally been done using machine learning models and logistic regression techniques.
However, with generative model classification, we lose the &lsquo;confidence level&rsquo; or the probability score of the prediction available in logistic regression. Traditional models like logistic regression provide a probability score for each class, indicating the model&rsquo;s confidence level in its predictions. This confidence score is not just valuable; it&rsquo;s essential for decision-making, as it helps users gauge how confident the model is about its classifications. While generative model responses may align well with the intended classification, we don&rsquo;t directly get an explicit probability for each class. This can be a limitation, particularly in high-stakes applications where knowing the model&rsquo;s confidence level is crucial.">
<meta name="author" content="Michael OShea">
<link rel="canonical" href="https://oshea00.github.io/posts/classify-with-confidence/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.f49d66caae9ea0fd43f21f29e71a8d3e284517ed770f2aa86fa012953ad3c9ef.css" integrity="sha256-9J1myq6eoP1D8h8p5xqNPihFF&#43;13Dyqob6ASlTrTye8=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://oshea00.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://oshea00.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://oshea00.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://oshea00.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://oshea00.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://oshea00.github.io/posts/classify-with-confidence/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css" integrity="sha384-MlJdn/WNKDGXveldHDdyRP1R4CTHr3FeuDNfhsLPYrq2t0UBkUdK2jyTnXPEK1NQ" crossorigin="anonymous">

<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.js" integrity="sha384-VQ8d8WVFw0yHhCk5E8I86oOhv48xLpnDZx5T9GogA/Y84DcCKWXDmSDfn13bzFZY" crossorigin="anonymous"></script>

<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/contrib/auto-render.min.js" integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR" crossorigin="anonymous"
    onload="renderMathInElement(document.body);"></script>

>
<script>
document.addEventListener("DOMContentLoaded", function() {
    renderMathInElement(document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "$", right: "$", display: false}
        ]
    });
});
</script>

      <script async src="https://www.googletagmanager.com/gtag/js?id=G-37B7H2GBCX"></script>
      <script>
        var doNotTrack = false;
        if ( false ) {
          var dnt = (navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack);
          var doNotTrack = (dnt == "1" || dnt == "yes");
        }
        if (!doNotTrack) {
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());
          gtag('config', 'G-37B7H2GBCX');
        }
      </script><meta property="og:url" content="https://oshea00.github.io/posts/classify-with-confidence/">
  <meta property="og:site_name" content="MikesBlog">
  <meta property="og:title" content="Classify With Confidence">
  <meta property="og:description" content=" Large foundation models like GPT can classify text according to a well-crafted prompt instruction, and it’s remarkable how well they can do this, considering there has been no explicit training with labeled datasets. This has traditionally been done using machine learning models and logistic regression techniques.
However, with generative model classification, we lose the ‘confidence level’ or the probability score of the prediction available in logistic regression. Traditional models like logistic regression provide a probability score for each class, indicating the model’s confidence level in its predictions. This confidence score is not just valuable; it’s essential for decision-making, as it helps users gauge how confident the model is about its classifications. While generative model responses may align well with the intended classification, we don’t directly get an explicit probability for each class. This can be a limitation, particularly in high-stakes applications where knowing the model’s confidence level is crucial.">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-11-02T00:00:00+00:00">
    <meta property="article:modified_time" content="2024-11-02T00:00:00+00:00">
    <meta property="article:tag" content="Llm">
    <meta property="article:tag" content="Openai">
    <meta property="article:tag" content="Prompt Engineering">
    <meta property="article:tag" content="Classification">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Classify With Confidence">
<meta name="twitter:description" content="
     


Large foundation models like GPT can classify text according to a well-crafted prompt instruction, and it&rsquo;s remarkable how well they can do this, considering there has been no explicit training with labeled datasets. This has traditionally been done using machine learning models and logistic regression techniques.
However, with generative model classification, we lose the &lsquo;confidence level&rsquo; or the probability score of the prediction available in logistic regression. Traditional models like logistic regression provide a probability score for each class, indicating the model&rsquo;s confidence level in its predictions. This confidence score is not just valuable; it&rsquo;s essential for decision-making, as it helps users gauge how confident the model is about its classifications. While generative model responses may align well with the intended classification, we don&rsquo;t directly get an explicit probability for each class. This can be a limitation, particularly in high-stakes applications where knowing the model&rsquo;s confidence level is crucial.">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://oshea00.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Classify With Confidence",
      "item": "https://oshea00.github.io/posts/classify-with-confidence/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Classify With Confidence",
  "name": "Classify With Confidence",
  "description": " Large foundation models like GPT can classify text according to a well-crafted prompt instruction, and it\u0026rsquo;s remarkable how well they can do this, considering there has been no explicit training with labeled datasets. This has traditionally been done using machine learning models and logistic regression techniques.\nHowever, with generative model classification, we lose the \u0026lsquo;confidence level\u0026rsquo; or the probability score of the prediction available in logistic regression. Traditional models like logistic regression provide a probability score for each class, indicating the model\u0026rsquo;s confidence level in its predictions. This confidence score is not just valuable; it\u0026rsquo;s essential for decision-making, as it helps users gauge how confident the model is about its classifications. While generative model responses may align well with the intended classification, we don\u0026rsquo;t directly get an explicit probability for each class. This can be a limitation, particularly in high-stakes applications where knowing the model\u0026rsquo;s confidence level is crucial.\n",
  "keywords": [
    "llm", "openai", "prompt engineering", "classification"
  ],
  "articleBody": " Large foundation models like GPT can classify text according to a well-crafted prompt instruction, and it’s remarkable how well they can do this, considering there has been no explicit training with labeled datasets. This has traditionally been done using machine learning models and logistic regression techniques.\nHowever, with generative model classification, we lose the ‘confidence level’ or the probability score of the prediction available in logistic regression. Traditional models like logistic regression provide a probability score for each class, indicating the model’s confidence level in its predictions. This confidence score is not just valuable; it’s essential for decision-making, as it helps users gauge how confident the model is about its classifications. While generative model responses may align well with the intended classification, we don’t directly get an explicit probability for each class. This can be a limitation, particularly in high-stakes applications where knowing the model’s confidence level is crucial.\nOne way to approach this limitation and gain a better sense of confidence in LLM classification results is to add instructions to the prompt to provide a “confidence level” over some range, such as 1.0 through 5.0, along with the predicted class of a given text example. But is this confidence value to be trusted?\nAn Experiment Is Needed To help answer that question, I’ve done a little experiment to see if the confidence level passed back in the LLM’s response has some statistical significance using an open-source dataset from Hugging Face.\nAG is a collection of more than 1 million news articles. ComeToMyHead, an academic news search engine, has gathered articles from over 2000 news sources in more than one year of activity. The dataset is provided by the academic community for research purposes in data mining (clustering, classification, etc.), information retrieval (ranking, search, etc.), XML, data compression, data streaming, and any other non-commercial activity.\nXiang Zhang (xiang.zhang@nyu.edu) constructed the AG’s news topic classification dataset from the dataset above. It is used as a classification benchmark in Zhang, Xiang, Junbo Zhao, and Yann LeCun. “Character-level Convolutional Networks for Text Classification.” ArXiv:1509.01626.\nThis code will download the dataset and cache it locally:\nfrom datasets import load_dataset import pandas as pd dataset = load_dataset(\"wangrongsheng/ag_news\", cache_dir=\"./data/ag_news\") labels = dataset[\"train\"].features[\"label\"].names df = pd.DataFrame(dataset['train']) print(labels) df.head() Figure 1. Sample labeled data of ‘Business’ topics.\nThe Classification Prompt Here is the example prompt I used with the GPT-4 model to get classification results for 1000 rows of the data, sampling an even distribution of classes:\nclassify_prompt = f\"\"\" Examine the text delimited below by ``` and classify it into one of the following categories: WORLD, SPORTS, BUSINESS, or SCITECH. ```{text}``` output: On a single line write the category that best fits and a confidence score in the range 1 to 5 (1 being the least confident) separated by a comma. On the following line write a brief explanation of why you chose that category. Do not include any other information in your response. \"\"\" The raw response looks like this:\nWORLD, 5.0 The text describes a geopolitical event involving an explosion in Baghdad, Iraq, which is a newsworthy incident related to global affairs typically covered in the WORLD news category. Enhancing the sampled dataset to add these values results in the following table: Figure 2. Prediction results.\nAnalyzing Results I’ll extract the labeled and predicted pairs from the data to create a classification report using the following code:\ndef generate_classification_report(labeled_predicted_pairs, label_names): # Extract labels and predictions from the pairs labels = [pair[0] for pair in labeled_predicted_pairs] predictions = [pair[1] for pair in labeled_predicted_pairs] # Map string labels to numerical values label_to_num = {label: i for i, label in enumerate(label_names)} num_labels = [label_to_num[label] for label in labels] num_predictions = [label_to_num[pred] for pred in predictions] # Generate confusion matrix conf_matrix = confusion_matrix(num_labels, num_predictions) # Generate classification report class_report = classification_report(num_labels, num_predictions, target_names=label_names) return conf_matrix, class_report # Get labeled_predicted_pairs from sampled_df labeled_predicted_pairs = list(zip(sampled_df['labelName'], sampled_df['predicted_class'])) label_values = sampled_df['labelName'].unique() # Generate report conf_matrix, class_report = generate_classification_report(labeled_predicted_pairs, label_values) #print(\"Confusion Matrix:\\n\", conf_matrix) print(\"\\nClassification Report:\\n\", class_report) Which produces this:\nClassification Report: precision recall f1-score support WORLD 0.84 0.88 0.86 250 SPORTS 0.96 1.00 0.98 250 BUSINESS 0.74 0.90 0.82 250 SCITECH 0.92 0.63 0.75 250 accuracy 0.85 1000 macro avg 0.86 0.85 0.85 1000 weighted avg 0.86 0.85 0.85 1000 The overall accuracy is 85%, which is pretty good, and we can dig a little deeper. The ‘support’ column shows how many samples of each class was in the dataset. I made sure to select a balanced set of samples for each class.\nTo understand how these were broken out into true and false positive and negatives, we’ll graph the “confusion matrix”, which shows actual versus predicted counts of each class. This data is used to calculate the precision/recall and f1-scores in the above report.\n# Plot confusion matrix import matplotlib.pyplot as plt import seaborn as sns import numpy as np plt.figure(figsize=(10, 7)) sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=label_values, yticklabels=label_values) plt.xlabel('Predicted') plt.ylabel('Actual') plt.title('Confusion Matrix') plt.show() Figure 3. Multi-class Confusion Matrix.\nHow Confident Was The Model? We can use the confidence level combined with the prediction’s “correctness” score to see how the confidence values were distributed for both true and false predictions as follows:\ntrue_predictions = sampled_df[sampled_df['correct'] == True] false_predictions = sampled_df[sampled_df['correct'] == False] plt.figure(figsize=(12, 6)) # Plot for true predictions sns.histplot(true_predictions['confidence_level'], bins=10, kde=True, color='blue', label='True Predictions', alpha=0.6, stat='density') # Plot for false predictions sns.histplot(false_predictions['confidence_level'], bins=10, kde=True, color='red', label='False Predictions', alpha=0.6, stat='density') plt.xlabel('Confidence Level') plt.ylabel('Normalized Frequency') plt.title('Comparison of Confidence Levels between True and False Predictions') plt.legend() plt.show() Figure 4. Confidence Distributions.\nFrom this plot, we can see intuitively that true predictions were more heavily distributed towards the high confidence (4.0 and 5.0) values, while false predictions had lower confidence scores, peaking at 4.0 and dropping off near 5.0. Still, are these distributions statistically significant?\nIn the next section, I’ll use a Logistic Regression to fit the true/false predictions to their associated confidence scores, then check if there’s a significant correlation using the calculated p_value and significance_level.\nStatistical Tests This code will load some scikit-learn tools, along with scipy,stats andstatsmodels useful for performing a few tests - the first of which will be the logistic regression and ap_value check.\nfrom sklearn.linear_model import LogisticRegression from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc import scipy.stats as stats import statsmodels.api as sm # Independent variable (X) and dependent variable (y) X = df['Confidence_Level'].values.reshape(-1, 1) y = df['Prediction_Correctness'].values model = LogisticRegression() model.fit(X, y) # Estimated coefficient coef = model.coef_[0][0] intercept = model.intercept_[0] print(f\"Coefficient for Confidence Level: {coef}\") print(f\"Intercept: {intercept}\") # Calculate the odds ratio odds_ratio = np.exp(coef) print(f\"Odds Ratio: {odds_ratio}\") # Interpretation if odds_ratio \u003e 1: print(f\"For each unit increase in confidence level, the odds of a correct prediction increase by {round((odds_ratio - 1) * 100, 2)}%.\") else: print(f\"For each unit increase in confidence level, the odds of a correct prediction decrease by {round((1 - odds_ratio) * 100, 2)}%.\") # Fit the model using statsmodels to get p-value X_with_const = sm.add_constant(X) # Add a constant term for intercept logit_model = sm.Logit(y, X_with_const) result = logit_model.fit() print(result.summary()) # Extract p-value for confidence level coefficient p_value = result.pvalues[1] print(f\"P-value for Confidence Level Coefficient: {p_value}\") # Check significance level significance_level = 0.05 if p_value \u003c significance_level: print(f\"The coefficient for confidence level is statistically significant.\") else: print(f\"The coefficient for confidence level is not statistically significant.\") The results of this test are as follows. It appears we have a statistically significant result.\nCoefficient for Confidence Level: 1.4061994575718302 Intercept: -4.464738502617202 Odds Ratio: 4.080418095576085 For each unit increase in confidence level, the odds of a correct prediction increase by 308.04%. Optimization terminated successfully. Current function value: 0.360052 Iterations 7 Logit Regression Results ============================================================================== Dep. Variable: y No. Observations: 1000 Model: Logit Df Residuals: 998 Method: MLE Df Model: 1 Date: Sat, 02 Nov 2024 Pseudo R-squ.: 0.1339 Time: 14:10:07 Log-Likelihood: -360.05 converged: True LL-Null: -415.71 Covariance Type: nonrobust LLR p-value: 5.056e-26 ============================================================================== coef std err z P\u003e|z| [0.025 0.975] ------------------------------------------------------------------------------ const -4.6039 0.664 -6.936 0.000 -5.905 -3.303 x1 1.4384 0.152 9.449 0.000 1.140 1.737 ============================================================================== P-value for Confidence Level Coefficient: 3.416898004216482e-21 The coefficient for confidence level is statistically significant. To double-check this initial result, I’m going to use a Point-Biserial Correlation test.\nPoint-Biserial Correlation The point-biserial correlation is a special case of the Pearson correlation used when one variable is continuous and the other is binary. Use the formula: $$ r_{pb} = \\frac{\\bar{X}_1 - \\bar{X}_0}{s_X} \\sqrt{\\frac{n_1 n_0}{n^2}} $$ Where:\n$\\bar{X}_1$ = Mean confidence level for correct predictions. $\\bar{X}_0$ = Mean confidence level for incorrect predictions. $s_X$ = Standard deviation of all confidence levels. $n_1,n_0$ = Number of correct and incorrect predictions. $n$ = Total number of predictions. Test for significance:\nt-test: $$ t = r_{pb}\\sqrt{\\frac{n-2}{1-r_{pb}^2}} $$ Degrees of freedom $n-2$ Let’s see how this can be done in code, and what the results are.\n# Separate the data for correct and incorrect predictions correct_predictions = df[df['Prediction_Correctness'] == 1]['Confidence_Level'] incorrect_predictions = df[df['Prediction_Correctness'] == 0]['Confidence_Level'] # Calculate the means mean_correct = correct_predictions.mean() mean_incorrect = incorrect_predictions.mean() # Calculate the standard deviation of all confidence levels std_confidence = df['Confidence_Level'].std() # Calculate the counts n_correct = len(correct_predictions) n_incorrect = len(incorrect_predictions) n_total = len(df) # Calculate point-biserial correlation coefficient r_pb = (mean_correct - mean_incorrect) / std_confidence * np.sqrt((n_correct * n_incorrect) / n_total**2) print(f\"Point-Biserial Correlation Coefficient (r_pb): {r_pb}\") # Test for significance using t-test t_value = r_pb * np.sqrt((n_total - 2) / (1 - r_pb**2)) degrees_of_freedom = n_total - 2 p_value = 2 * (1 - stats.t.cdf(abs(t_value), df=degrees_of_freedom)) print(f\"T-value: {t_value}\") print(f\"P-value: {p_value}\") # Check significance level significance_level = 0.05 if p_value \u003c significance_level: print(\"The point-biserial correlation is statistically significant.\") else: print(\"The point-biserial correlation is not statistically significant.\") And the results are apparently significant:\nPoint-Biserial Correlation Coefficient (r_pb): 0.3647830613321057 T-value: 12.37676335481931 P-value: 0.0 The point-biserial correlation is statistically significant. To add another test of significance, I will use a Mann-Whitney U test.\nMann-Whitney U Test The Mann-Whitney U test assesses whether the distributions of confidence levels differ between true and false predictions.\nThe code and the results follow:\n# Perform Mann-Whitney U Test u_statistic, p_value = stats.mannwhitneyu(correct_predictions, incorrect_predictions, alternative='two-sided') print(f\"Mann-Whitney U Statistic: {u_statistic}\") print(f\"P-value: {p_value}\") # Check significance level significance_level = 0.05 if p_value \u003c significance_level: print(\"The Mann-Whitney U test is statistically significant, indicating a difference in distributions between correct and incorrect predictions.\") else: print(\"The Mann-Whitney U test is not statistically significant, indicating no difference in distributions between correct and incorrect predictions.\") Mann-Whitney U Statistic: 90831.5 P-value: 4.6967698934230915e-26 The Mann-Whitney U test is statistically significant, indicating a difference in distributions between correct and incorrect predictions. ROC Curve The ROC curve will further measure the significance of the confidence level scores provided by the GPT-4 model.\n# ROC Curve fpr, tpr, _ = roc_curve(df['Prediction_Correctness'], df['Confidence_Level']) roc_auc = auc(fpr, tpr) plt.figure(figsize=(10, 5)) plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC Curve (area = {roc_auc:.2f})') plt.plot([0, 1], [0, 1], color='grey', linestyle='--') plt.xlabel('False Positive Rate') plt.ylabel('True Positive Rate') plt.title('Receiver Operating Characteristic (ROC) Curve') plt.legend(loc='lower right') plt.show() Figure 5. ROC Curve\nIf classes were randomly predicted in association with the confidence scores, the area under the ROC curve would be approximately 50%. As it stands, with these results, the ROC curve area is 73%, which is a significant score, although not an outstanding one.\nConclusion In this small experiment, using the LLM model’s estimate of its “confidence level” combined with its explanation for its decision as asked for in the prompt, it appears there is a statistical significance of the confidence expressed by the model in its predictions.\nA larger study might be worthwhile with different types of text and classification classes, as well as different LLM models, to see if this type of prompting improves classification accuracy or at least provides some way to determine confidence when presenting results for human review.\n",
  "wordCount" : "1939",
  "inLanguage": "en",
  "datePublished": "2024-11-02T00:00:00Z",
  "dateModified": "2024-11-02T00:00:00Z",
  "author":[{
    "@type": "Person",
    "name": "Michael OShea"
  }],
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://oshea00.github.io/posts/classify-with-confidence/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "MikesBlog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://oshea00.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://oshea00.github.io/" accesskey="h" title="MikesBlog (Alt + H)">MikesBlog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://oshea00.github.io/archives" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://oshea00.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="https://oshea00.github.io/posts/" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      Classify With Confidence
    </h1>
    <div class="post-meta"><span title='2024-11-02 00:00:00 +0000 UTC'>November 2, 2024</span>&nbsp;·&nbsp;10 min&nbsp;·&nbsp;Michael OShea

</div>
  </header> 
  <div class="post-content"><p><figure class="align-center ">
    <img loading="lazy" src="images/cutepredictioncartoon.png#center"/> 
</figure>

Large foundation models like GPT can classify text according to a well-crafted prompt instruction, and it&rsquo;s remarkable how well they can do this, considering there has been no explicit training with labeled datasets. This has traditionally been done using machine learning models and logistic regression techniques.</p>
<p>However, with generative model classification, we lose the &lsquo;confidence level&rsquo; or the probability score of the prediction available in logistic regression. Traditional models like logistic regression provide a probability score for each class, indicating the model&rsquo;s confidence level in its predictions. This confidence score is not just valuable; it&rsquo;s essential for decision-making, as it helps users gauge how confident the model is about its classifications. While generative model responses may align well with the intended classification, we don&rsquo;t directly get an explicit probability for each class. This can be a limitation, particularly in high-stakes applications where knowing the model&rsquo;s confidence level is crucial.</p>
<p>One way to approach this limitation and gain a better sense of confidence in LLM classification results is to add instructions to the prompt to provide a &ldquo;confidence level&rdquo; over some range, such as 1.0 through 5.0, along with the predicted class of a given text example. But is this confidence value to be trusted?</p>
<h2 id="an-experiment-is-needed">An Experiment Is Needed<a hidden class="anchor" aria-hidden="true" href="#an-experiment-is-needed">#</a></h2>
<p>To help answer that question, I&rsquo;ve done a little experiment to see if the confidence level passed back in the LLM&rsquo;s response has some statistical significance using an open-source dataset from Hugging Face.</p>
<p>AG is a collection of more than 1 million news articles. ComeToMyHead, an academic news search engine, has gathered articles from over 2000 news sources in more than one year of activity. The dataset is provided by the academic community for research purposes in data mining (clustering, classification, etc.), information retrieval (ranking, search, etc.), XML, data compression, data streaming, and any other non-commercial activity.</p>
<p>Xiang Zhang (<a href="mailto:xiang.zhang@nyu.edu">xiang.zhang@nyu.edu</a>) constructed the AG&rsquo;s news topic classification dataset from the dataset above. It is used as a classification benchmark in Zhang, Xiang, Junbo Zhao, and Yann LeCun. &ldquo;Character-level Convolutional Networks for Text Classification.&rdquo; ArXiv:1509.01626.</p>
<p>This code will download the dataset and cache it locally:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">datasets</span> <span class="kn">import</span> <span class="n">load_dataset</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
</span></span><span class="line"><span class="cl"><span class="n">dataset</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s2">&#34;wangrongsheng/ag_news&#34;</span><span class="p">,</span> <span class="n">cache_dir</span><span class="o">=</span><span class="s2">&#34;./data/ag_news&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">labels</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="s2">&#34;train&#34;</span><span class="p">]</span><span class="o">.</span><span class="n">features</span><span class="p">[</span><span class="s2">&#34;label&#34;</span><span class="p">]</span><span class="o">.</span><span class="n">names</span>
</span></span><span class="line"><span class="cl"><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">dataset</span><span class="p">[</span><span class="s1">&#39;train&#39;</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">labels</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</span></span></code></pre></div><figure>
    <img loading="lazy" src="images/dfhead_ag.png"/> <figcaption>
            <p>Figure 1. Sample labeled data of &lsquo;Business&rsquo; topics.</p>
        </figcaption>
</figure>

<h2 id="the-classification-prompt">The Classification Prompt<a hidden class="anchor" aria-hidden="true" href="#the-classification-prompt">#</a></h2>
<p>Here is the example prompt I used with the GPT-4 model to get classification results for 1000 rows of the data, sampling an even distribution of classes:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">classify_prompt</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">Examine the text delimited below by ``` and classify it into one
</span></span></span><span class="line"><span class="cl"><span class="s2">of the following categories: WORLD, SPORTS, BUSINESS, or SCITECH.
</span></span></span><span class="line"><span class="cl"><span class="s2">```</span><span class="si">{</span><span class="n">text</span><span class="si">}</span><span class="s2">```
</span></span></span><span class="line"><span class="cl"><span class="s2">output:
</span></span></span><span class="line"><span class="cl"><span class="s2">On a single line write the category that best fits and 
</span></span></span><span class="line"><span class="cl"><span class="s2">a confidence score in the range 1 to 5 (1 being the least confident)
</span></span></span><span class="line"><span class="cl"><span class="s2">separated by a comma.
</span></span></span><span class="line"><span class="cl"><span class="s2">On the following line write a brief explanation of why you chose that category.
</span></span></span><span class="line"><span class="cl"><span class="s2">Do not include any other information in your response.
</span></span></span><span class="line"><span class="cl"><span class="s2">&#34;&#34;&#34;</span>
</span></span></code></pre></div><p>The raw response looks like this:</p>
<pre tabindex="0"><code>WORLD, 5.0
The text describes a geopolitical event involving an explosion in Baghdad, Iraq, which is a newsworthy incident related to global affairs typically covered in the WORLD news category.
</code></pre><p>Enhancing the sampled dataset to add these values results in the following table:
<figure>
    <img loading="lazy" src="images/sample_df.png"/> <figcaption>
            <p>Figure 2. Prediction results.</p>
        </figcaption>
</figure>
</p>
<h2 id="analyzing-results">Analyzing Results<a hidden class="anchor" aria-hidden="true" href="#analyzing-results">#</a></h2>
<p>I&rsquo;ll extract the labeled and predicted pairs from the data to create a classification report using the following code:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">generate_classification_report</span><span class="p">(</span><span class="n">labeled_predicted_pairs</span><span class="p">,</span> <span class="n">label_names</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># Extract labels and predictions from the pairs</span>
</span></span><span class="line"><span class="cl">    <span class="n">labels</span> <span class="o">=</span> <span class="p">[</span><span class="n">pair</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">pair</span> <span class="ow">in</span> <span class="n">labeled_predicted_pairs</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="n">predictions</span> <span class="o">=</span> <span class="p">[</span><span class="n">pair</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">pair</span> <span class="ow">in</span> <span class="n">labeled_predicted_pairs</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c1"># Map string labels to numerical values</span>
</span></span><span class="line"><span class="cl">    <span class="n">label_to_num</span> <span class="o">=</span> <span class="p">{</span><span class="n">label</span><span class="p">:</span> <span class="n">i</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">label</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">label_names</span><span class="p">)}</span>
</span></span><span class="line"><span class="cl">    <span class="n">num_labels</span> <span class="o">=</span> <span class="p">[</span><span class="n">label_to_num</span><span class="p">[</span><span class="n">label</span><span class="p">]</span> <span class="k">for</span> <span class="n">label</span> <span class="ow">in</span> <span class="n">labels</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="n">num_predictions</span> <span class="o">=</span> <span class="p">[</span><span class="n">label_to_num</span><span class="p">[</span><span class="n">pred</span><span class="p">]</span> <span class="k">for</span> <span class="n">pred</span> <span class="ow">in</span> <span class="n">predictions</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c1"># Generate confusion matrix</span>
</span></span><span class="line"><span class="cl">    <span class="n">conf_matrix</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">num_labels</span><span class="p">,</span> <span class="n">num_predictions</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c1"># Generate classification report</span>
</span></span><span class="line"><span class="cl">    <span class="n">class_report</span> <span class="o">=</span> <span class="n">classification_report</span><span class="p">(</span><span class="n">num_labels</span><span class="p">,</span> <span class="n">num_predictions</span><span class="p">,</span> <span class="n">target_names</span><span class="o">=</span><span class="n">label_names</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">conf_matrix</span><span class="p">,</span> <span class="n">class_report</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Get labeled_predicted_pairs from sampled_df</span>
</span></span><span class="line"><span class="cl"><span class="n">labeled_predicted_pairs</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">sampled_df</span><span class="p">[</span><span class="s1">&#39;labelName&#39;</span><span class="p">],</span> <span class="n">sampled_df</span><span class="p">[</span><span class="s1">&#39;predicted_class&#39;</span><span class="p">]))</span>
</span></span><span class="line"><span class="cl"><span class="n">label_values</span> <span class="o">=</span> <span class="n">sampled_df</span><span class="p">[</span><span class="s1">&#39;labelName&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">unique</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Generate report</span>
</span></span><span class="line"><span class="cl"><span class="n">conf_matrix</span><span class="p">,</span> <span class="n">class_report</span> <span class="o">=</span> <span class="n">generate_classification_report</span><span class="p">(</span><span class="n">labeled_predicted_pairs</span><span class="p">,</span> <span class="n">label_values</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1">#print(&#34;Confusion Matrix:\n&#34;, conf_matrix)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;</span><span class="se">\n</span><span class="s2">Classification Report:</span><span class="se">\n</span><span class="s2">&#34;</span><span class="p">,</span> <span class="n">class_report</span><span class="p">)</span>
</span></span></code></pre></div><p>Which produces this:</p>
<pre tabindex="0"><code>Classification Report:
               precision    recall  f1-score   support

       WORLD       0.84      0.88      0.86       250
      SPORTS       0.96      1.00      0.98       250
    BUSINESS       0.74      0.90      0.82       250
     SCITECH       0.92      0.63      0.75       250

    accuracy                           0.85      1000
   macro avg       0.86      0.85      0.85      1000
weighted avg       0.86      0.85      0.85      1000
</code></pre><p>The overall accuracy is 85%, which is pretty good, and we can dig a little deeper. The &lsquo;support&rsquo; column shows how many samples of each class was in the dataset. I made sure to select a balanced set of samples for each class.</p>
<p>To understand how these were broken out into true and false positive and negatives, we&rsquo;ll graph the &ldquo;confusion matrix&rdquo;, which shows actual versus predicted counts of each class. This data is used to calculate the precision/recall and f1-scores in the above report.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Plot confusion matrix</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">7</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">conf_matrix</span><span class="p">,</span> <span class="n">annot</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">fmt</span><span class="o">=</span><span class="s1">&#39;d&#39;</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;Blues&#39;</span><span class="p">,</span> <span class="n">xticklabels</span><span class="o">=</span><span class="n">label_values</span><span class="p">,</span> <span class="n">yticklabels</span><span class="o">=</span><span class="n">label_values</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Predicted&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Actual&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Confusion Matrix&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span></code></pre></div><figure>
    <img loading="lazy" src="images/confusionmatrix_1.png"/> <figcaption>
            <p>Figure 3. Multi-class Confusion Matrix.</p>
        </figcaption>
</figure>

<h2 id="how-confident-was-the-model">How Confident Was The Model?<a hidden class="anchor" aria-hidden="true" href="#how-confident-was-the-model">#</a></h2>
<p>We can use the confidence level combined with the prediction&rsquo;s &ldquo;correctness&rdquo; score to see how the confidence values were distributed for both true and false predictions as follows:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">true_predictions</span> <span class="o">=</span> <span class="n">sampled_df</span><span class="p">[</span><span class="n">sampled_df</span><span class="p">[</span><span class="s1">&#39;correct&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="kc">True</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">false_predictions</span> <span class="o">=</span> <span class="n">sampled_df</span><span class="p">[</span><span class="n">sampled_df</span><span class="p">[</span><span class="s1">&#39;correct&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="kc">False</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Plot for true predictions</span>
</span></span><span class="line"><span class="cl"><span class="n">sns</span><span class="o">.</span><span class="n">histplot</span><span class="p">(</span><span class="n">true_predictions</span><span class="p">[</span><span class="s1">&#39;confidence_level&#39;</span><span class="p">],</span> <span class="n">bins</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">kde</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;True Predictions&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span> <span class="n">stat</span><span class="o">=</span><span class="s1">&#39;density&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Plot for false predictions</span>
</span></span><span class="line"><span class="cl"><span class="n">sns</span><span class="o">.</span><span class="n">histplot</span><span class="p">(</span><span class="n">false_predictions</span><span class="p">[</span><span class="s1">&#39;confidence_level&#39;</span><span class="p">],</span> <span class="n">bins</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">kde</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;False Predictions&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span> <span class="n">stat</span><span class="o">=</span><span class="s1">&#39;density&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Confidence Level&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Normalized Frequency&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Comparison of Confidence Levels between True and False Predictions&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span></code></pre></div><p><figure>
    <img loading="lazy" src="images/confidence.png"/> <figcaption>
            <p>Figure 4. Confidence Distributions.</p>
        </figcaption>
</figure>

From this plot, we can see intuitively that true predictions were more heavily distributed towards the high confidence (4.0 and 5.0) values, while false predictions had lower confidence scores, peaking at 4.0 and dropping off near 5.0. Still, are these distributions statistically significant?</p>
<p>In the next section, I&rsquo;ll use a Logistic Regression to fit the true/false predictions to their associated confidence scores, then check if there&rsquo;s a significant correlation using the calculated <code>p_value</code> and <code>significance_level</code>.</p>
<h2 id="statistical-tests">Statistical Tests<a hidden class="anchor" aria-hidden="true" href="#statistical-tests">#</a></h2>
<p>This code will load some <code>scikit-learn</code> tools, along with <code>scipy</code>,<code>stats</code> and<code>statsmodels</code> useful for performing a few tests - the first of which will be the logistic regression and a<code>p_value</code> check.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">confusion_matrix</span><span class="p">,</span> <span class="n">classification_report</span><span class="p">,</span> <span class="n">roc_curve</span><span class="p">,</span> <span class="n">auc</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">scipy.stats</span> <span class="k">as</span> <span class="nn">stats</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">statsmodels.api</span> <span class="k">as</span> <span class="nn">sm</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Independent variable (X) and dependent variable (y)</span>
</span></span><span class="line"><span class="cl"><span class="n">X</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;Confidence_Level&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">y</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;Prediction_Correctness&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">model</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Estimated coefficient</span>
</span></span><span class="line"><span class="cl"><span class="n">coef</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">intercept</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">intercept_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;Coefficient for Confidence Level: </span><span class="si">{</span><span class="n">coef</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;Intercept: </span><span class="si">{</span><span class="n">intercept</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Calculate the odds ratio</span>
</span></span><span class="line"><span class="cl"><span class="n">odds_ratio</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">coef</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;Odds Ratio: </span><span class="si">{</span><span class="n">odds_ratio</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Interpretation</span>
</span></span><span class="line"><span class="cl"><span class="k">if</span> <span class="n">odds_ratio</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;For each unit increase in confidence level, the odds of a correct prediction increase by </span><span class="si">{</span><span class="nb">round</span><span class="p">((</span><span class="n">odds_ratio</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="si">}</span><span class="s2">%.&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;For each unit increase in confidence level, the odds of a correct prediction decrease by </span><span class="si">{</span><span class="nb">round</span><span class="p">((</span><span class="mi">1</span> <span class="o">-</span> <span class="n">odds_ratio</span><span class="p">)</span> <span class="o">*</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="si">}</span><span class="s2">%.&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Fit the model using statsmodels to get p-value</span>
</span></span><span class="line"><span class="cl"><span class="n">X_with_const</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">add_constant</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>  <span class="c1"># Add a constant term for intercept</span>
</span></span><span class="line"><span class="cl"><span class="n">logit_model</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">Logit</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">X_with_const</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">result</span> <span class="o">=</span> <span class="n">logit_model</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Extract p-value for confidence level coefficient</span>
</span></span><span class="line"><span class="cl"><span class="n">p_value</span> <span class="o">=</span> <span class="n">result</span><span class="o">.</span><span class="n">pvalues</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;P-value for Confidence Level Coefficient: </span><span class="si">{</span><span class="n">p_value</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Check significance level</span>
</span></span><span class="line"><span class="cl"><span class="n">significance_level</span> <span class="o">=</span> <span class="mf">0.05</span>
</span></span><span class="line"><span class="cl"><span class="k">if</span> <span class="n">p_value</span> <span class="o">&lt;</span> <span class="n">significance_level</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;The coefficient for confidence level is statistically significant.&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;The coefficient for confidence level is not statistically significant.&#34;</span><span class="p">)</span>
</span></span></code></pre></div><p>The results of this test are as follows. It appears we have a statistically significant result.</p>
<pre tabindex="0"><code>Coefficient for Confidence Level: 1.4061994575718302
Intercept: -4.464738502617202
Odds Ratio: 4.080418095576085
For each unit increase in confidence level, the odds of a correct prediction
increase by 308.04%.
Optimization terminated successfully.
         Current function value: 0.360052
         Iterations 7
                           Logit Regression Results                           
==============================================================================
Dep. Variable:                      y   No. Observations:                 1000
Model:                          Logit   Df Residuals:                      998
Method:                           MLE   Df Model:                            1
Date:                Sat, 02 Nov 2024   Pseudo R-squ.:                  0.1339
Time:                        14:10:07   Log-Likelihood:                -360.05
converged:                       True   LL-Null:                       -415.71
Covariance Type:            nonrobust   LLR p-value:                 5.056e-26
==============================================================================
                 coef    std err          z      P&gt;|z|      [0.025      0.975]
------------------------------------------------------------------------------
const         -4.6039      0.664     -6.936      0.000      -5.905      -3.303
x1             1.4384      0.152      9.449      0.000       1.140       1.737
==============================================================================
P-value for Confidence Level Coefficient: 3.416898004216482e-21
The coefficient for confidence level is statistically significant.
</code></pre><p>To double-check this initial result, I&rsquo;m going to use a Point-Biserial Correlation test.</p>
<h3 id="point-biserial-correlation">Point-Biserial Correlation<a hidden class="anchor" aria-hidden="true" href="#point-biserial-correlation">#</a></h3>
<p>The point-biserial correlation is a special case of the Pearson correlation used when one variable is continuous and the other is binary.
Use the formula:
$$
r_{pb} = \frac{\bar{X}_1 - \bar{X}_0}{s_X} \sqrt{\frac{n_1 n_0}{n^2}}
$$
Where:</p>
<ul>
<li>$\bar{X}_1$ = Mean confidence level for correct predictions.</li>
<li>$\bar{X}_0$ = Mean confidence level for incorrect predictions.</li>
<li>$s_X$ = Standard deviation of all confidence levels.</li>
<li>$n_1,n_0$ = Number of correct and incorrect predictions.</li>
<li>$n$ = Total number of predictions.</li>
</ul>
<p>Test for significance:</p>
<ul>
<li>t-test:
$$
t = r_{pb}\sqrt{\frac{n-2}{1-r_{pb}^2}}
$$</li>
<li>Degrees of freedom $n-2$</li>
</ul>
<p>Let&rsquo;s see how this can be done in code, and what the results are.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Separate the data for correct and incorrect predictions</span>
</span></span><span class="line"><span class="cl"><span class="n">correct_predictions</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;Prediction_Correctness&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">][</span><span class="s1">&#39;Confidence_Level&#39;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">incorrect_predictions</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;Prediction_Correctness&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">][</span><span class="s1">&#39;Confidence_Level&#39;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Calculate the means</span>
</span></span><span class="line"><span class="cl"><span class="n">mean_correct</span> <span class="o">=</span> <span class="n">correct_predictions</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="n">mean_incorrect</span> <span class="o">=</span> <span class="n">incorrect_predictions</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Calculate the standard deviation of all confidence levels</span>
</span></span><span class="line"><span class="cl"><span class="n">std_confidence</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;Confidence_Level&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">std</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Calculate the counts</span>
</span></span><span class="line"><span class="cl"><span class="n">n_correct</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">correct_predictions</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">n_incorrect</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">incorrect_predictions</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">n_total</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Calculate point-biserial correlation coefficient</span>
</span></span><span class="line"><span class="cl"><span class="n">r_pb</span> <span class="o">=</span> <span class="p">(</span><span class="n">mean_correct</span> <span class="o">-</span> <span class="n">mean_incorrect</span><span class="p">)</span> <span class="o">/</span> <span class="n">std_confidence</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">((</span><span class="n">n_correct</span> <span class="o">*</span> <span class="n">n_incorrect</span><span class="p">)</span> <span class="o">/</span> <span class="n">n_total</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;Point-Biserial Correlation Coefficient (r_pb): </span><span class="si">{</span><span class="n">r_pb</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Test for significance using t-test</span>
</span></span><span class="line"><span class="cl"><span class="n">t_value</span> <span class="o">=</span> <span class="n">r_pb</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">((</span><span class="n">n_total</span> <span class="o">-</span> <span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">r_pb</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="n">degrees_of_freedom</span> <span class="o">=</span> <span class="n">n_total</span> <span class="o">-</span> <span class="mi">2</span>
</span></span><span class="line"><span class="cl"><span class="n">p_value</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">stats</span><span class="o">.</span><span class="n">t</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="nb">abs</span><span class="p">(</span><span class="n">t_value</span><span class="p">),</span> <span class="n">df</span><span class="o">=</span><span class="n">degrees_of_freedom</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;T-value: </span><span class="si">{</span><span class="n">t_value</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;P-value: </span><span class="si">{</span><span class="n">p_value</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Check significance level</span>
</span></span><span class="line"><span class="cl"><span class="n">significance_level</span> <span class="o">=</span> <span class="mf">0.05</span>
</span></span><span class="line"><span class="cl"><span class="k">if</span> <span class="n">p_value</span> <span class="o">&lt;</span> <span class="n">significance_level</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="s2">&#34;The point-biserial correlation is statistically significant.&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="s2">&#34;The point-biserial correlation is not statistically significant.&#34;</span><span class="p">)</span>
</span></span></code></pre></div><p>And the results are apparently significant:</p>
<pre tabindex="0"><code>Point-Biserial Correlation Coefficient (r_pb): 0.3647830613321057
T-value: 12.37676335481931
P-value: 0.0
The point-biserial correlation is statistically significant.
</code></pre><p>To add another test of significance, I will use a Mann-Whitney U test.</p>
<h3 id="mann-whitney-u-test">Mann-Whitney U Test<a hidden class="anchor" aria-hidden="true" href="#mann-whitney-u-test">#</a></h3>
<p>The Mann-Whitney U test assesses whether the distributions of confidence levels differ between true and false predictions.</p>
<p>The code and the results follow:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Perform Mann-Whitney U Test</span>
</span></span><span class="line"><span class="cl"><span class="n">u_statistic</span><span class="p">,</span> <span class="n">p_value</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">mannwhitneyu</span><span class="p">(</span><span class="n">correct_predictions</span><span class="p">,</span> <span class="n">incorrect_predictions</span><span class="p">,</span> <span class="n">alternative</span><span class="o">=</span><span class="s1">&#39;two-sided&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;Mann-Whitney U Statistic: </span><span class="si">{</span><span class="n">u_statistic</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;P-value: </span><span class="si">{</span><span class="n">p_value</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Check significance level</span>
</span></span><span class="line"><span class="cl"><span class="n">significance_level</span> <span class="o">=</span> <span class="mf">0.05</span>
</span></span><span class="line"><span class="cl"><span class="k">if</span> <span class="n">p_value</span> <span class="o">&lt;</span> <span class="n">significance_level</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="s2">&#34;The Mann-Whitney U test is statistically significant, indicating a difference in distributions between correct and incorrect predictions.&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="s2">&#34;The Mann-Whitney U test is not statistically significant, indicating no difference in distributions between correct and incorrect predictions.&#34;</span><span class="p">)</span>
</span></span></code></pre></div><pre tabindex="0"><code>Mann-Whitney U Statistic: 90831.5
P-value: 4.6967698934230915e-26
The Mann-Whitney U test is statistically significant, indicating a difference in
distributions between correct and incorrect predictions.
</code></pre><h2 id="roc-curve">ROC Curve<a hidden class="anchor" aria-hidden="true" href="#roc-curve">#</a></h2>
<p>The ROC curve will further measure the significance of the confidence level scores provided by the GPT-4 model.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># ROC Curve</span>
</span></span><span class="line"><span class="cl"><span class="n">fpr</span><span class="p">,</span> <span class="n">tpr</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">roc_curve</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;Prediction_Correctness&#39;</span><span class="p">],</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;Confidence_Level&#39;</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="n">roc_auc</span> <span class="o">=</span> <span class="n">auc</span><span class="p">(</span><span class="n">fpr</span><span class="p">,</span> <span class="n">tpr</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">fpr</span><span class="p">,</span> <span class="n">tpr</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;ROC Curve (area = </span><span class="si">{</span><span class="n">roc_auc</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">)&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;grey&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;False Positive Rate&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;True Positive Rate&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Receiver Operating Characteristic (ROC) Curve&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;lower right&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span></code></pre></div><figure>
    <img loading="lazy" src="images/roc.png"/> <figcaption>
            <p>Figure 5. ROC Curve</p>
        </figcaption>
</figure>

<p>If classes were randomly predicted in association with the confidence scores, the area under the ROC curve would be approximately 50%. As it stands, with these results, the ROC curve area is 73%, which is a significant score, although not an outstanding one.</p>
<h2 id="conclusion">Conclusion<a hidden class="anchor" aria-hidden="true" href="#conclusion">#</a></h2>
<p>In this small experiment, using the LLM model&rsquo;s estimate of its &ldquo;confidence level&rdquo; combined with its explanation for its decision as asked for in the prompt, it appears there is a statistical significance of the confidence expressed by the model in its predictions.</p>
<p>A larger study might be worthwhile with different types of text and classification classes, as well as different LLM models, to see if this type of prompting improves classification accuracy or at least provides some way to determine confidence when presenting results for human review.</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://oshea00.github.io/tags/llm/">Llm</a></li>
      <li><a href="https://oshea00.github.io/tags/openai/">Openai</a></li>
      <li><a href="https://oshea00.github.io/tags/prompt-engineering/">Prompt Engineering</a></li>
      <li><a href="https://oshea00.github.io/tags/classification/">Classification</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="https://oshea00.github.io/posts/agentic-ai-fragmentation/">
    <span class="title">« Prev</span>
    <br>
    <span>Navigating the Fragmented Landscape of Agentic AI Tools</span>
  </a>
  <a class="next" href="https://oshea00.github.io/posts/a-rose-by-any-other-name/">
    <span class="title">Next »</span>
    <br>
    <span>Comparing Prompt Results - A Rose By Any Other Name</span>
  </a>
</nav>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Classify With Confidence on x"
            href="https://x.com/intent/tweet/?text=Classify%20With%20Confidence&amp;url=https%3a%2f%2foshea00.github.io%2fposts%2fclassify-with-confidence%2f&amp;hashtags=llm%2copenai%2cpromptengineering%2cclassification">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Classify With Confidence on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2foshea00.github.io%2fposts%2fclassify-with-confidence%2f&amp;title=Classify%20With%20Confidence&amp;summary=Classify%20With%20Confidence&amp;source=https%3a%2f%2foshea00.github.io%2fposts%2fclassify-with-confidence%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Classify With Confidence on reddit"
            href="https://reddit.com/submit?url=https%3a%2f%2foshea00.github.io%2fposts%2fclassify-with-confidence%2f&title=Classify%20With%20Confidence">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Classify With Confidence on facebook"
            href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2foshea00.github.io%2fposts%2fclassify-with-confidence%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Classify With Confidence on whatsapp"
            href="https://api.whatsapp.com/send?text=Classify%20With%20Confidence%20-%20https%3a%2f%2foshea00.github.io%2fposts%2fclassify-with-confidence%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Classify With Confidence on telegram"
            href="https://telegram.me/share/url?text=Classify%20With%20Confidence&amp;url=https%3a%2f%2foshea00.github.io%2fposts%2fclassify-with-confidence%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Classify With Confidence on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=Classify%20With%20Confidence&u=https%3a%2f%2foshea00.github.io%2fposts%2fclassify-with-confidence%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="https://oshea00.github.io/">MikesBlog</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
