<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Transformers -  Positional Encoding | MyAIStuff</title>
<meta name="keywords" content="llm, transformer, torch">
<meta name="description" content="Since transformer input is processed in parallel rather than serially, it is necessary to encode the relative positions of the input sequence of tokens in some way. The positional encoding in the transformer model uses sinusoidal functions to create a unique encoding for each position.
In working through the article on Transformers, as described in the original paper &ldquo;Attention is All You Need&rdquo; by Vaswani et al., the following formulas are used to encode the PE tensor values:">
<meta name="author" content="Michael OShea">
<link rel="canonical" href="https://oshea00.github.io/posts/transformers-positional-encoding/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.5ff2630c4d1b3e25bc21f0ecd96681dbcf58219e741fa627857820b5485cb770.css" integrity="sha256-X/JjDE0bPiW8IfDs2WaB289YIZ50H6YnhXggtUhct3A=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://oshea00.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://oshea00.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://oshea00.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://oshea00.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://oshea00.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://oshea00.github.io/posts/transformers-positional-encoding/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css" integrity="sha384-wcIxkf4k558AjM3Yz3BBFQUbk/zgIYC2R0QpeeYb+TwlBVMrlgLqwRjRtGZiK7ww" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.js" integrity="sha384-hIoBPJpTUs74ddyc4bFZSM1TVlQDA60VBbJS0oA934VSz82sBx1X7kSx2ATBDIyd" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/contrib/auto-render.min.js" integrity="sha384-43gviWU0YVjaDtb/GhzOouOXtZMP/7XUzwPTstBeZFe/+rCMvRwr4yROQP43s0Xk" crossorigin="anonymous"></script>
<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
          
          
          delimiters: [
              {left: '$$', right: '$$', display: true},
              {left: '$', right: '$', display: false},
              {left: '\\(', right: '\\)', display: false},
              {left: '\\[', right: '\\]', display: true}
          ],
          
          throwOnError : false
        });
    });
</script>

  

<meta property="og:title" content="Transformers -  Positional Encoding" />
<meta property="og:description" content="Since transformer input is processed in parallel rather than serially, it is necessary to encode the relative positions of the input sequence of tokens in some way. The positional encoding in the transformer model uses sinusoidal functions to create a unique encoding for each position.
In working through the article on Transformers, as described in the original paper &ldquo;Attention is All You Need&rdquo; by Vaswani et al., the following formulas are used to encode the PE tensor values:" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://oshea00.github.io/posts/transformers-positional-encoding/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-05-27T00:00:00+00:00" />
<meta property="article:modified_time" content="2024-05-27T00:00:00+00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Transformers -  Positional Encoding"/>
<meta name="twitter:description" content="Since transformer input is processed in parallel rather than serially, it is necessary to encode the relative positions of the input sequence of tokens in some way. The positional encoding in the transformer model uses sinusoidal functions to create a unique encoding for each position.
In working through the article on Transformers, as described in the original paper &ldquo;Attention is All You Need&rdquo; by Vaswani et al., the following formulas are used to encode the PE tensor values:"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://oshea00.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Transformers -  Positional Encoding",
      "item": "https://oshea00.github.io/posts/transformers-positional-encoding/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Transformers -  Positional Encoding",
  "name": "Transformers -  Positional Encoding",
  "description": "Since transformer input is processed in parallel rather than serially, it is necessary to encode the relative positions of the input sequence of tokens in some way. The positional encoding in the transformer model uses sinusoidal functions to create a unique encoding for each position.\nIn working through the article on Transformers, as described in the original paper \u0026ldquo;Attention is All You Need\u0026rdquo; by Vaswani et al., the following formulas are used to encode the PE tensor values:",
  "keywords": [
    "llm", "transformer", "torch"
  ],
  "articleBody": "Since transformer input is processed in parallel rather than serially, it is necessary to encode the relative positions of the input sequence of tokens in some way. The positional encoding in the transformer model uses sinusoidal functions to create a unique encoding for each position.\nIn working through the article on Transformers, as described in the original paper “Attention is All You Need” by Vaswani et al., the following formulas are used to encode the PE tensor values:\n$$ PE_{(pos,2i)}=sin(\\frac{pos}{10000^{2i/d_{model}}}) $$ $$ PE_{(pos,2i+1)}=cos(\\frac{pos}{10000^{2i/d_{model}}}) $$ Where $pos$ is the position in the sequence, $i$ is the position in the positional encoding tensor, and $d_{model}$ is the dimension of the model, which is also used as the length of the embedding tensors which will be assigned to each position in the sequence.\nNote below where these Positional Encoding tensors are applied to the Input and Output Embedding tensors: Figure 1. Attention is All You Need\nAt first glance, it’s not apparent to me how the encoding values in the PE tensors can provide positional “intelligence” to the transformer model. What is going on here?\nThe intuition for the PE vector values created with these formulas has several elements:\nUsing different frequencies ranging from short to long wavelengths in these sinusoidal functions allows the model to capture positional information at various scales from near to far. These smooth repeating functions can scale easily to different sequence lengths. Each position has a unique encoding aided by the alternating use of $sin$ and $cos$ in the even and odd positions. It is always easier for me to use a visual to understand things better. Using a scaled-down transformer might help to get a better intuition of what this looks like. Let’s use the above formulas and PyTorch to create the PE tensor. We will use a maximum sequence length of twenty and a $d_{model}$ dimension of six:\n# model config seq_len = 20 # maximum tokenized sequence length d_model = 6 # dimension of the model and the embeddings (meanings) # Positional encoding. # Creating the division term for the positional encoding formula pe = torch.zeros(seq_len, d_model) position = torch.arange(0, seq_len, dtype = torch.float).unsqueeze(1) div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)) # Apply sine to even indices in PE pe[:, 0::2] = torch.sin(position * div_term) # Apply cosine to odd indices in PE pe[:, 1::2] = torch.cos(position * div_term) The shape of the PE tensor will be $[20, 6]$, or $[seqlen, d_{model}]$. The embedding dimensions are always the same as the model dimension. This PE tensor will be added to the input and output sequence embedding tensors during model processing (see Figure 1.).\nFigure 2. PE Tensor\nEmbeddings are created based on the dictionary of tokens. For each token in the training input data, an embedding tensor of length $d_{model}$ is assigned from training. These embedding tensors encode the “meaning”, as understood by the model, via the text it has been trained on.\nTo see what these positional encoding values look like, for each column in the above PE tensor, I’ve plotted the values:\nFigure 3. Graphing Positional Encoding\nAs you can see, the first two positions are the $sin$ and $cos$ functions. In later positions, the same functions are modified to longer wavelengths, so we can see how adding these functions to the embeddings imbues them with some sense of relative position, from near to far, between the tokens being evaluated.\nWith some imagination, it’s possible to see how other encodings could be used to represent positional offsets. This would be a fun experiment to try. I’ve put the code from the article here in case you want to play with it.\n",
  "wordCount" : "610",
  "inLanguage": "en",
  "datePublished": "2024-05-27T00:00:00Z",
  "dateModified": "2024-05-27T00:00:00Z",
  "author":[{
    "@type": "Person",
    "name": "Michael OShea"
  }],
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://oshea00.github.io/posts/transformers-positional-encoding/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "MyAIStuff",
    "logo": {
      "@type": "ImageObject",
      "url": "https://oshea00.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://oshea00.github.io/" accesskey="h" title="MyAIStuff (Alt + H)">MyAIStuff</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://oshea00.github.io/archives" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://oshea00.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="https://oshea00.github.io/posts/" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      Transformers -  Positional Encoding
    </h1>
    <div class="post-meta"><span title='2024-05-27 00:00:00 +0000 UTC'>May 27, 2024</span>&nbsp;·&nbsp;3 min&nbsp;·&nbsp;Michael OShea

</div>
  </header> 
  <div class="post-content"><p>Since transformer input is processed in parallel rather than serially, it is necessary to encode the relative positions of the input sequence of tokens in some way. The positional encoding in the transformer model uses sinusoidal functions to create a unique encoding for each position.</p>
<p>In working through the article on Transformers, as described in the original paper <a href="https://arxiv.org/pdf/1706.03762">&ldquo;Attention is All You Need&rdquo; by Vaswani et al.</a>, the following formulas are used to encode the PE tensor values:</p>
$$
PE_{(pos,2i)}=sin(\frac{pos}{10000^{2i/d_{model}}})
$$
$$
PE_{(pos,2i+1)}=cos(\frac{pos}{10000^{2i/d_{model}}})
$$
<p>Where $pos$ is the position in the sequence, $i$ is the position in the positional encoding tensor, and $d_{model}$ is the dimension of the model, which is also used as the length of the embedding tensors which will be assigned to each position in the sequence.</p>
<p>Note below where these Positional Encoding tensors are applied to the Input and Output Embedding tensors:
<figure class="align-center ">
    <img loading="lazy" src="images/transformer.png#center"/> <figcaption>
            <p>Figure 1. Attention is All You Need</p>
        </figcaption>
</figure>
</p>
<p>At first glance, it&rsquo;s not apparent to me how the encoding values in the PE tensors can provide positional &ldquo;intelligence&rdquo; to the transformer model. What is going on here?</p>
<p>The intuition for the PE vector values created with these formulas has several elements:</p>
<ol>
<li>Using different frequencies ranging from short to long wavelengths in these sinusoidal functions allows the model to capture positional information at various scales from near to far.</li>
<li>These smooth repeating functions can scale easily to different sequence lengths.</li>
<li>Each position has a unique encoding aided by the alternating use of $sin$ and $cos$ in the even and odd positions.</li>
</ol>
<p>It is always easier for me to use a visual to understand things better. Using a scaled-down transformer might help to get a better intuition of what this looks like. Let&rsquo;s use the above formulas and PyTorch to create the PE tensor. We will use a maximum sequence length of twenty and a $d_{model}$ dimension of six:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># model config</span>
</span></span><span class="line"><span class="cl"><span class="n">seq_len</span> <span class="o">=</span> <span class="mi">20</span> <span class="c1"># maximum tokenized sequence length</span>
</span></span><span class="line"><span class="cl"><span class="n">d_model</span> <span class="o">=</span> <span class="mi">6</span> <span class="c1"># dimension of the model and the embeddings (meanings)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Positional encoding.</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Creating the division term for the positional encoding formula</span>
</span></span><span class="line"><span class="cl"><span class="n">pe</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">seq_len</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">position</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">div_term</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span> <span class="o">*</span> <span class="p">(</span><span class="o">-</span><span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mf">10000.0</span><span class="p">)</span> <span class="o">/</span> <span class="n">d_model</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Apply sine to even indices in PE</span>
</span></span><span class="line"><span class="cl"><span class="n">pe</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">position</span> <span class="o">*</span> <span class="n">div_term</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Apply cosine to odd indices in PE</span>
</span></span><span class="line"><span class="cl"><span class="n">pe</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">position</span> <span class="o">*</span> <span class="n">div_term</span><span class="p">)</span>
</span></span></code></pre></div><p>The shape of the PE tensor will be $[20, 6]$, or $[seqlen, d_{model}]$.
The embedding dimensions are always the same as the model dimension. This PE tensor will be added to the input and output sequence embedding tensors during model processing (see <em>Figure 1.</em>).</p>
<figure class="align-center ">
    <img loading="lazy" src="images/peTensor.png#center"/> <figcaption>
            <p>Figure 2. PE Tensor</p>
        </figcaption>
</figure>

<p>Embeddings are created based on the dictionary of tokens. For each token in the training input data, an embedding tensor of length $d_{model}$ is assigned from training. These embedding tensors encode the &ldquo;meaning&rdquo;, as understood by the model, via the text it has been trained on.</p>
<p>To see what these positional encoding values look like, for each column in the above PE tensor, I&rsquo;ve plotted the values:</p>
<figure class="align-center ">
    <img loading="lazy" src="images/peEncoding.png#center"/> <figcaption>
            <p>Figure 3. Graphing Positional Encoding</p>
        </figcaption>
</figure>

<p>As you can see, the first two positions are the $sin$ and $cos$ functions. In later positions, the same functions are modified to longer wavelengths, so we can see how adding these functions to the embeddings imbues them with some sense of relative position, from near to far, between the tokens being evaluated.</p>
<p>With some imagination, it&rsquo;s possible to see how other encodings could be used to represent positional offsets. This would be a fun experiment to try. I&rsquo;ve put the <a href="https://github.com/oshea00/pytorchcuda/tree/main/transformer">code from the article here</a> in case you want to play with it.</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://oshea00.github.io/tags/llm/">Llm</a></li>
      <li><a href="https://oshea00.github.io/tags/transformer/">Transformer</a></li>
      <li><a href="https://oshea00.github.io/tags/torch/">Torch</a></li>
    </ul>
<nav class="paginav">
  <a class="next" href="https://oshea00.github.io/posts/finetuning-llama3-8b/">
    <span class="title">Next »</span>
    <br>
    <span>Fine-tuning Llama3</span>
  </a>
</nav>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Transformers -  Positional Encoding on x"
            href="https://x.com/intent/tweet/?text=Transformers%20-%20%20Positional%20Encoding&amp;url=https%3a%2f%2foshea00.github.io%2fposts%2ftransformers-positional-encoding%2f&amp;hashtags=llm%2ctransformer%2ctorch">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Transformers -  Positional Encoding on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2foshea00.github.io%2fposts%2ftransformers-positional-encoding%2f&amp;title=Transformers%20-%20%20Positional%20Encoding&amp;summary=Transformers%20-%20%20Positional%20Encoding&amp;source=https%3a%2f%2foshea00.github.io%2fposts%2ftransformers-positional-encoding%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Transformers -  Positional Encoding on reddit"
            href="https://reddit.com/submit?url=https%3a%2f%2foshea00.github.io%2fposts%2ftransformers-positional-encoding%2f&title=Transformers%20-%20%20Positional%20Encoding">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Transformers -  Positional Encoding on facebook"
            href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2foshea00.github.io%2fposts%2ftransformers-positional-encoding%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Transformers -  Positional Encoding on whatsapp"
            href="https://api.whatsapp.com/send?text=Transformers%20-%20%20Positional%20Encoding%20-%20https%3a%2f%2foshea00.github.io%2fposts%2ftransformers-positional-encoding%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Transformers -  Positional Encoding on telegram"
            href="https://telegram.me/share/url?text=Transformers%20-%20%20Positional%20Encoding&amp;url=https%3a%2f%2foshea00.github.io%2fposts%2ftransformers-positional-encoding%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Transformers -  Positional Encoding on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=Transformers%20-%20%20Positional%20Encoding&u=https%3a%2f%2foshea00.github.io%2fposts%2ftransformers-positional-encoding%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2024 <a href="https://oshea00.github.io/">MyAIStuff</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
