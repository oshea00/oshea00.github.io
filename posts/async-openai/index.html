<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Scaling OpenAI With AsyncOpenAI | MikesBlog</title>
<meta name="keywords" content="llm, openai, asyncopenai">
<meta name="description" content="As I stood outside and looked at the neighborhood wasteland that post-July 4th left behind, the whiff of gunpowder still hanging in the air, I felt a burst of good neighbor energy flow through me, so I grabbed a broom. Sweeping up the street gave me time to think about the other chores I had for the day, including the writing of a new blog post, and I began to wonder how I could use ChatGPT to help me speed some things up.">
<meta name="author" content="Michael OShea">
<link rel="canonical" href="https://oshea00.github.io/posts/async-openai/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.5ff2630c4d1b3e25bc21f0ecd96681dbcf58219e741fa627857820b5485cb770.css" integrity="sha256-X/JjDE0bPiW8IfDs2WaB289YIZ50H6YnhXggtUhct3A=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://oshea00.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://oshea00.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://oshea00.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://oshea00.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://oshea00.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://oshea00.github.io/posts/async-openai/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
      <script async src="https://www.googletagmanager.com/gtag/js?id=G-37B7H2GBCX"></script>
      <script>
        var doNotTrack = false;
        if ( false ) {
          var dnt = (navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack);
          var doNotTrack = (dnt == "1" || dnt == "yes");
        }
        if (!doNotTrack) {
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());
          gtag('config', 'G-37B7H2GBCX');
        }
      </script><meta property="og:title" content="Scaling OpenAI With AsyncOpenAI" />
<meta property="og:description" content="As I stood outside and looked at the neighborhood wasteland that post-July 4th left behind, the whiff of gunpowder still hanging in the air, I felt a burst of good neighbor energy flow through me, so I grabbed a broom. Sweeping up the street gave me time to think about the other chores I had for the day, including the writing of a new blog post, and I began to wonder how I could use ChatGPT to help me speed some things up." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://oshea00.github.io/posts/async-openai/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-07-07T00:00:00+00:00" />
<meta property="article:modified_time" content="2024-07-07T00:00:00+00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Scaling OpenAI With AsyncOpenAI"/>
<meta name="twitter:description" content="As I stood outside and looked at the neighborhood wasteland that post-July 4th left behind, the whiff of gunpowder still hanging in the air, I felt a burst of good neighbor energy flow through me, so I grabbed a broom. Sweeping up the street gave me time to think about the other chores I had for the day, including the writing of a new blog post, and I began to wonder how I could use ChatGPT to help me speed some things up."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://oshea00.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Scaling OpenAI With AsyncOpenAI",
      "item": "https://oshea00.github.io/posts/async-openai/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Scaling OpenAI With AsyncOpenAI",
  "name": "Scaling OpenAI With AsyncOpenAI",
  "description": "As I stood outside and looked at the neighborhood wasteland that post-July 4th left behind, the whiff of gunpowder still hanging in the air, I felt a burst of good neighbor energy flow through me, so I grabbed a broom. Sweeping up the street gave me time to think about the other chores I had for the day, including the writing of a new blog post, and I began to wonder how I could use ChatGPT to help me speed some things up.\n",
  "keywords": [
    "llm", "openai", "asyncopenai"
  ],
  "articleBody": "As I stood outside and looked at the neighborhood wasteland that post-July 4th left behind, the whiff of gunpowder still hanging in the air, I felt a burst of good neighbor energy flow through me, so I grabbed a broom. Sweeping up the street gave me time to think about the other chores I had for the day, including the writing of a new blog post, and I began to wonder how I could use ChatGPT to help me speed some things up.\nFigure 1. Work, work, work.\nMy thoughts turned to the bills. Every now and then, I download the transactions from my credit card’s website and attempt to figure out what I’m spending and where it is going. The transaction file looks like this.\nFigure 2. Raw transactions.\nNote the field Name and its contents. Normally, I spend 15-30 minutes figuring out how to describe the transactions with tags in a column that I can use to group payments. Looking at row two, I have spent something on Amazon Digital and there’s a unique reference in the Name field. My tag might be ‘AMAZON DIGITAL’. But I will need to mentally parse each of these and assign the tag manually. This looks like a job for GPT.\nIt would be nice if GPT could examine these fields and tag them uniformly based on their contents—thus, turning a half-hour of Excel “clicky” into 30 seconds or so of work. Let’s get started.\nFor this example, I have decided to use OpenAI’s GPT-4o model, and my first task is to develop a prompt capable of doing what I want. After many attempts (another good blog topic on prompt engineering awaits) I came up with this.\ndef get_prompt(name): return \"\"\"Examine the transaction name text delimited by ```. The format starts with a company or domain name. Call that portion COMPANY. COMPANY will consist of a phrase of one or more words. The first word may be a number. DESC follows COMPANY. DESC starts with a non-letter character, a non-english word, or a number. All words in COMPANY should be upper case and separated with single spaces. Output the COMPANY on a single line. Replace any non-letter characters in COMPANY with spaces. Do not repeat these instructions as part of your response. \"\"\" + f\"```{name}```\" Using some Pandas magic and the python OpenAI client, I was able to use this prompt to produce a tag based on the Name and insert a column called Desc that I could use for grouping.\nFigure 3. Tagged transactions in the ‘Desc’ column.\nNote in line [377] that I am post-processing the data from GPT-4o to remove some artifacts I don’t like, such as removing names and other ‘PII’ that might appear in the tags. Also, in some Name values, company names were duplicated. So, rather than ‘MICROSOFT MICROSOFT,’ I wanted just ‘MICROSOFT.’ This is a good example of adding a step to ensure your data is good, clean, safe, etc., and should be a part of any LLM-generated processing.\nProcessing at scale Now that I had a reasonably good process for tagging my data, I decided to run all 323 transactions through calls to OpenAI. It took a few minutes. A few factors were playing into the total time:\nMy tier level on this OpenAI account was only Tier-1. I was using the synchronous OpenAI() client. Increasing the tier level to Tier-3 required processing a few more tokens for this account. A goal that was achieved fairly quickly as I ran more of these tests. Increasing the tier level allows for processing more tokens per minute (TPM) which allows for sending larger batches.\nFigure 4. Tier-3 limits.\nThe next part, and the main subject here, was to use batches of parallel queries to improve throughput, and this required the use of asyncio combined with the async client in the openai package: AsyncOpenAI.\nAsyncOpenAI has flow control You will find examples of using asyncio and aiohttps, combined with backoff and/or tenacity to call the OpenAI API asychronously using response headers to dynamically adjust calls when token limits are being reached. I don’t recommend this approach unless you are trying to write your own library.\nAs it turns out, the openai library already includes a client that does much of this for you called AsyncOpenAI, and it is mostly a drop-in replacement for the synchronous OpenAI client. It will handle flow control and backoff for you, so you don’t have to drop down into the details under the covers.\nThat is the example I’m showing here. Note that other than the ‘async/await’ keywords, the asynchronous version looks exactly like the synchronous one.\nclient = AsyncOpenAI() async def async_completion(messages,prompt): messages.append({\"role\":\"user\",\"content\":f\"{prompt}\"}) completion = await client.chat.completions.create( model=\"gpt-4o\", messages=messages, temperature=0.2, top_p=0.9, max_tokens=4096, stream=False ) messages.append({\"role\":\"assistant\",\"content\":f\"{completion.choices[0].message.content}\"}) return messages The next part is batching calls. We need a few helper functions. The first function is simply my own code that handles setting up the prompt and calling the async_completion() function. The next function handles the semaphore.\nasync def reformat_transaction_name(name): messages = [ {'role':'system','content':'you are a helpful named entity recognizer.'}, ] response = await async_completion(messages,get_prompt(name)) return response[-1]['content'] async def reformat_transaction_with_semaphore(transaction_name, semaphore): async with semaphore: return await reformat_transaction_name(transaction_name) Now for the batching with Semaphore part.\nfrom asyncio import Semaphore async def run_batch(items): semaphore = Semaphore(len(items)) tasks = [reformat_transaction_with_semaphore(name,semaphore) for name in items] results = await asyncio.gather(*tasks) return results In the run_batch(items) function, a semaphore is created to match the size of the batch being processed. The len(items) allows me to adjust this when calling it.\nNow for the fun part In this code, I setup and submit the batches based on BATCHSIZE to run run_batch(items) by windowing through the list of items in the Dataframe - doing a bit of accounting along the way.\n# Batched async import time start_time = time.time() BATCHSIZE = 65 desc_items = [] for i in range(0,len(cctrans['Name']),BATCHSIZE): items = cctrans['Name'][i:i+BATCHSIZE] results = asyncio.run(run_batch(items)) desc_items += results end_time = time.time() print(f\"Batch completion time: {end_time-start_time} seconds\") print(f\"Item count: {len(desc_items)}\") print(f\"Items per second: {len(desc_items) / (end_time-start_time)}\") Here are the results:\nBatch completion time: 6.294457197189331 seconds Item count: 323 Items per second: 51.31498870851476 Significantly better than a few minutes at 6.29 seconds.\nOn the home stretch So, at this point, you have enough code examples to go try this on your own, but I will spend a little time talking about testing and costs, etc. If you’re able to stick around.\nPicking good BATCHSIZE It is good to run tests with different batch sizes to determine what gives you the best throughput. Using the timing information from various batch sizes, I found BATCHSIZE=65 to be somewhat optimal; however, as I move up in tier level, I will need to re-check this, as higher limits will allow more TPM to be processed.\nAs you can see, there is some variablility in throughput most likely based on how busy OpenAI is. Your mileage will vary.\nFigure 5. Batch Size Runs.\nHow much will this cost? Another good practice is to estimate how many tokens you will be using. Using tiktoken and some pricing information can help with that.\nimport tiktoken token_1M_price=5 prompt_text = get_prompt('Amazon web services aws.amazon.co WA') tokenizer = tiktoken.get_encoding(\"cl100k_base\") tokens = tokenizer.encode(prompt_text) prompt_token_count = len(tokens) tokens = tokenizer.encode('AMAZON WEB SERVICES') response_token_count = len(tokens) print(f\"Prompt length: {prompt_token_count}\") print(f\"Response length: {response_token_count}\") print(f\"Item count: {len(cctrans['Transaction'])}\") jobsize=len(cctrans['Transaction'])*(prompt_token_count+response_token_count) print(f\"Total estimated job size in tokens: {jobsize}\") print(f\"Estimated price per job at ${token_1M_price:.2f}/1M_tokens is ${(jobsize/1_000_000)*token_1M_price:.2f}\") Prompt length: 128 Response length: 5 Item count: 323 Total estimated job size in tokens: 42959 Estimated price per job at $5.00/1M_tokens is $0.21 So, there you have it. I’m spending about 21 cents per job.\nFinal thoughts Well, the street is now clean, and I’ve finished another blog post.\nYou should be well on your way to improving throughput with OpenAI.\nDo come back!\n",
  "wordCount" : "1299",
  "inLanguage": "en",
  "datePublished": "2024-07-07T00:00:00Z",
  "dateModified": "2024-07-07T00:00:00Z",
  "author":[{
    "@type": "Person",
    "name": "Michael OShea"
  }],
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://oshea00.github.io/posts/async-openai/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "MikesBlog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://oshea00.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://oshea00.github.io/" accesskey="h" title="MikesBlog (Alt + H)">MikesBlog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://oshea00.github.io/archives" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://oshea00.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="https://oshea00.github.io/posts/" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      Scaling OpenAI With AsyncOpenAI
    </h1>
    <div class="post-meta"><span title='2024-07-07 00:00:00 +0000 UTC'>July 7, 2024</span>&nbsp;·&nbsp;7 min&nbsp;·&nbsp;Michael OShea

</div>
  </header> 
  <div class="post-content"><p>As I stood outside and looked at the neighborhood wasteland that post-July 4th left behind, the whiff of gunpowder still hanging in the air, I felt a burst of good neighbor energy flow through me, so I grabbed a broom. Sweeping up the street gave me time to think about the other chores I had for the day, including the writing of a new blog post, and I began to wonder how I could use ChatGPT to help me speed some things up.</p>
<figure class="align-center ">
    <img loading="lazy" src="images/postits_w480.png#center"/> <figcaption>
            <p>Figure 1. Work, work, work.</p>
        </figcaption>
</figure>

<p>My thoughts turned to the bills. Every now and then, I download the transactions from my credit card’s website and attempt to figure out what I&rsquo;m spending and where it is going. The transaction file looks like this.</p>
<figure class="align-center ">
    <img loading="lazy" src="images/cctrans.png#center"/> <figcaption>
            <p>Figure 2. Raw transactions.</p>
        </figcaption>
</figure>

<p>Note the field <strong>Name</strong> and its contents. Normally, I spend 15-30 minutes figuring out how to describe the transactions with tags in a column that I can use to group payments. Looking at row two, I have spent something on Amazon Digital and there’s a unique reference in the <strong>Name</strong> field. My tag might be &lsquo;<em>AMAZON DIGITAL</em>&rsquo;. But I will need to mentally parse each of these and assign the tag manually. This looks like a job for GPT.</p>
<p>It would be nice if GPT could examine these fields and tag them uniformly based on their contents—thus, turning a half-hour of Excel “clicky” into 30 seconds or so of work. Let&rsquo;s get started.</p>
<p>For this example, I have decided to use OpenAI&rsquo;s GPT-4o model, and my first task is to develop a prompt capable of doing what I want. After many attempts (another good blog topic on prompt engineering awaits) I came up with this.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">get_prompt</span><span class="p">(</span><span class="n">name</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="s2">&#34;&#34;&#34;Examine the transaction name text delimited by ```.
</span></span></span><span class="line"><span class="cl"><span class="s2">    The format starts with a company or domain name. Call that portion COMPANY.
</span></span></span><span class="line"><span class="cl"><span class="s2">    COMPANY will consist of a phrase of one or more words. The first word may be a number.
</span></span></span><span class="line"><span class="cl"><span class="s2">    DESC follows COMPANY. DESC starts with a non-letter character, a non-english word, or a number.
</span></span></span><span class="line"><span class="cl"><span class="s2">    All words in COMPANY should be upper case and separated with single spaces.
</span></span></span><span class="line"><span class="cl"><span class="s2">    Output the COMPANY on a single line.
</span></span></span><span class="line"><span class="cl"><span class="s2">    Replace any non-letter characters in COMPANY with spaces.
</span></span></span><span class="line"><span class="cl"><span class="s2">    Do not repeat these instructions as part of your response.
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span> <span class="o">+</span> <span class="sa">f</span><span class="s2">&#34;```</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2">```&#34;</span>
</span></span></code></pre></div><p>Using some Pandas magic and the python OpenAI client, I was able to use this prompt to produce a tag based on the <strong>Name</strong> and insert a column called <strong>Desc</strong> that I could use for grouping.</p>
<figure class="align-center ">
    <img loading="lazy" src="images/cctrans_desc.png#center"/> <figcaption>
            <p>Figure 3. Tagged transactions in the &lsquo;Desc&rsquo; column.</p>
        </figcaption>
</figure>

<p>Note in line [377] that I am post-processing the data from GPT-4o to remove some artifacts I don&rsquo;t like, such as removing names and other &lsquo;PII&rsquo; that might appear in the tags. Also, in some <strong>Name</strong> values, company names were duplicated. So, rather than &lsquo;MICROSOFT MICROSOFT,&rsquo; I wanted just &lsquo;MICROSOFT.&rsquo; This is a good example of adding a step to ensure your data is good, clean, safe, etc., and should be a part of any LLM-generated processing.</p>
<h2 id="processing-at-scale">Processing at scale<a hidden class="anchor" aria-hidden="true" href="#processing-at-scale">#</a></h2>
<p>Now that I had a reasonably good process for tagging my data, I decided to run all 323 transactions through calls to OpenAI. It took a few minutes.
A few factors were playing into the total time:</p>
<ul>
<li>My tier level on this OpenAI account was only Tier-1.</li>
<li>I was using the synchronous OpenAI() client.</li>
</ul>
<p>Increasing the tier level to Tier-3 required processing a few more tokens for this account. A goal that was achieved fairly quickly as I ran more of these tests. Increasing the tier level allows for processing more tokens per minute (TPM) which allows for sending larger batches.</p>
<figure class="align-center ">
    <img loading="lazy" src="images/tier3limits.png#center"/> <figcaption>
            <p>Figure 4. Tier-3 limits.</p>
        </figcaption>
</figure>

<p>The next part, and the main subject here, was to use batches of parallel queries to improve throughput, and this required the use of <code>asyncio</code> combined with the async client in the openai package: <code>AsyncOpenAI</code>.</p>
<h3 id="asyncopenai-has-flow-control">AsyncOpenAI has flow control<a hidden class="anchor" aria-hidden="true" href="#asyncopenai-has-flow-control">#</a></h3>
<p>You will find examples of using <code>asyncio</code> and <code>aiohttps</code>, combined with <code>backoff</code> and/or <code>tenacity</code> to call the OpenAI API asychronously using response headers to dynamically adjust calls when token limits are being reached. I don&rsquo;t recommend this approach unless you are trying to write your own library.</p>
<p>As it turns out, the <code>openai</code>  library already includes a client that does much of this for you called <code>AsyncOpenAI</code>, and it is mostly a drop-in replacement for the synchronous <code>OpenAI</code> client. It will handle flow control and backoff for you, so you don&rsquo;t have to drop down into the details under the covers.</p>
<p>That is the example I&rsquo;m showing here. Note that other than the &lsquo;async/await&rsquo; keywords, the asynchronous version looks exactly like the synchronous one.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">client</span> <span class="o">=</span> <span class="n">AsyncOpenAI</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="k">async</span> <span class="k">def</span> <span class="nf">async_completion</span><span class="p">(</span><span class="n">messages</span><span class="p">,</span><span class="n">prompt</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">messages</span><span class="o">.</span><span class="n">append</span><span class="p">({</span><span class="s2">&#34;role&#34;</span><span class="p">:</span><span class="s2">&#34;user&#34;</span><span class="p">,</span><span class="s2">&#34;content&#34;</span><span class="p">:</span><span class="sa">f</span><span class="s2">&#34;</span><span class="si">{</span><span class="n">prompt</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">})</span>
</span></span><span class="line"><span class="cl">    <span class="n">completion</span> <span class="o">=</span> <span class="k">await</span> <span class="n">client</span><span class="o">.</span><span class="n">chat</span><span class="o">.</span><span class="n">completions</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="n">model</span><span class="o">=</span><span class="s2">&#34;gpt-4o&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">messages</span><span class="o">=</span><span class="n">messages</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">temperature</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">top_p</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">max_tokens</span><span class="o">=</span><span class="mi">4096</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">stream</span><span class="o">=</span><span class="kc">False</span>
</span></span><span class="line"><span class="cl">    <span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">messages</span><span class="o">.</span><span class="n">append</span><span class="p">({</span><span class="s2">&#34;role&#34;</span><span class="p">:</span><span class="s2">&#34;assistant&#34;</span><span class="p">,</span><span class="s2">&#34;content&#34;</span><span class="p">:</span><span class="sa">f</span><span class="s2">&#34;</span><span class="si">{</span><span class="n">completion</span><span class="o">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">message</span><span class="o">.</span><span class="n">content</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">})</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">messages</span>
</span></span></code></pre></div><p>The next part is batching calls. We need a few helper functions. The first function is simply my own code that handles setting up the prompt and calling the <code>async_completion()</code> function. The next function handles the semaphore.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">async</span> <span class="k">def</span> <span class="nf">reformat_transaction_name</span><span class="p">(</span><span class="n">name</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">messages</span> <span class="o">=</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl">        <span class="p">{</span><span class="s1">&#39;role&#39;</span><span class="p">:</span><span class="s1">&#39;system&#39;</span><span class="p">,</span><span class="s1">&#39;content&#39;</span><span class="p">:</span><span class="s1">&#39;you are a helpful named entity recognizer.&#39;</span><span class="p">},</span>
</span></span><span class="line"><span class="cl">    <span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="n">response</span> <span class="o">=</span> <span class="k">await</span> <span class="n">async_completion</span><span class="p">(</span><span class="n">messages</span><span class="p">,</span><span class="n">get_prompt</span><span class="p">(</span><span class="n">name</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">response</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="s1">&#39;content&#39;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">async</span> <span class="k">def</span> <span class="nf">reformat_transaction_with_semaphore</span><span class="p">(</span><span class="n">transaction_name</span><span class="p">,</span> <span class="n">semaphore</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">async</span> <span class="k">with</span> <span class="n">semaphore</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="k">await</span> <span class="n">reformat_transaction_name</span><span class="p">(</span><span class="n">transaction_name</span><span class="p">)</span>
</span></span></code></pre></div><p>Now for the batching with <code>Semaphore</code> part.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">asyncio</span> <span class="kn">import</span> <span class="n">Semaphore</span>
</span></span><span class="line"><span class="cl"><span class="k">async</span> <span class="k">def</span> <span class="nf">run_batch</span><span class="p">(</span><span class="n">items</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">semaphore</span> <span class="o">=</span> <span class="n">Semaphore</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">items</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    <span class="n">tasks</span> <span class="o">=</span> <span class="p">[</span><span class="n">reformat_transaction_with_semaphore</span><span class="p">(</span><span class="n">name</span><span class="p">,</span><span class="n">semaphore</span><span class="p">)</span> <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">items</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="n">results</span> <span class="o">=</span> <span class="k">await</span> <span class="n">asyncio</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="o">*</span><span class="n">tasks</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">results</span>
</span></span></code></pre></div><p>In the <code>run_batch(items)</code> function, a semaphore is created to match the size of the batch being processed. The <code>len(items)</code> allows me to adjust this when calling it.</p>
<h3 id="now-for-the-fun-part">Now for the fun part<a hidden class="anchor" aria-hidden="true" href="#now-for-the-fun-part">#</a></h3>
<p>In this code, I setup and submit the batches based on BATCHSIZE to run <code>run_batch(items)</code> by windowing through the list of items in the Dataframe - doing a bit of accounting along the way.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Batched async</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">time</span>
</span></span><span class="line"><span class="cl"><span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="n">BATCHSIZE</span> <span class="o">=</span> <span class="mi">65</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">desc_items</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">cctrans</span><span class="p">[</span><span class="s1">&#39;Name&#39;</span><span class="p">]),</span><span class="n">BATCHSIZE</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">items</span> <span class="o">=</span> <span class="n">cctrans</span><span class="p">[</span><span class="s1">&#39;Name&#39;</span><span class="p">][</span><span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="n">BATCHSIZE</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="n">results</span> <span class="o">=</span> <span class="n">asyncio</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">run_batch</span><span class="p">(</span><span class="n">items</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    <span class="n">desc_items</span> <span class="o">+=</span> <span class="n">results</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">end_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;Batch completion time: </span><span class="si">{</span><span class="n">end_time</span><span class="o">-</span><span class="n">start_time</span><span class="si">}</span><span class="s2"> seconds&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;Item count: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">desc_items</span><span class="p">)</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;Items per second: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">desc_items</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">end_time</span><span class="o">-</span><span class="n">start_time</span><span class="p">)</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span>
</span></span></code></pre></div><p>Here are the results:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">Batch completion time: 6.294457197189331 seconds
</span></span><span class="line"><span class="cl">Item count: <span class="m">323</span>
</span></span><span class="line"><span class="cl">Items per second: 51.31498870851476
</span></span></code></pre></div><p>Significantly better than a few minutes at 6.29 seconds.</p>
<h3 id="on-the-home-stretch">On the home stretch<a hidden class="anchor" aria-hidden="true" href="#on-the-home-stretch">#</a></h3>
<p>So, at this point, you have enough code examples to go try this on your own, but I will spend a little time talking about testing and costs, etc. If you&rsquo;re able to stick around.</p>
<h4 id="picking-good-batchsize">Picking good BATCHSIZE<a hidden class="anchor" aria-hidden="true" href="#picking-good-batchsize">#</a></h4>
<p>It is good to run tests with different batch sizes to determine what gives you the best throughput. Using the timing information from various batch sizes, I found BATCHSIZE=65 to be somewhat optimal; however, as I move up in tier level, I will need to re-check this, as higher limits will allow more TPM to be processed.</p>
<p>As you can see, there is some variablility in throughput most likely based on how busy OpenAI is. Your mileage will vary.</p>
<figure class="align-center ">
    <img loading="lazy" src="images/batchchart.png#center"/> <figcaption>
            <p>Figure 5. Batch Size Runs.</p>
        </figcaption>
</figure>

<h4 id="how-much-will-this-cost">How much will this cost?<a hidden class="anchor" aria-hidden="true" href="#how-much-will-this-cost">#</a></h4>
<p>Another good practice is to estimate how many tokens you will be using. Using <code>tiktoken</code> and some pricing information can help with that.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">tiktoken</span>
</span></span><span class="line"><span class="cl"><span class="n">token_1M_price</span><span class="o">=</span><span class="mi">5</span>
</span></span><span class="line"><span class="cl"><span class="n">prompt_text</span> <span class="o">=</span> <span class="n">get_prompt</span><span class="p">(</span><span class="s1">&#39;Amazon web services    aws.amazon.co WA&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">tiktoken</span><span class="o">.</span><span class="n">get_encoding</span><span class="p">(</span><span class="s2">&#34;cl100k_base&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">prompt_text</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">prompt_token_count</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="s1">&#39;AMAZON WEB SERVICES&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">response_token_count</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;Prompt length: </span><span class="si">{</span><span class="n">prompt_token_count</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;Response length: </span><span class="si">{</span><span class="n">response_token_count</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;Item count: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">cctrans</span><span class="p">[</span><span class="s1">&#39;Transaction&#39;</span><span class="p">])</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">jobsize</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">cctrans</span><span class="p">[</span><span class="s1">&#39;Transaction&#39;</span><span class="p">])</span><span class="o">*</span><span class="p">(</span><span class="n">prompt_token_count</span><span class="o">+</span><span class="n">response_token_count</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;Total estimated job size in tokens: </span><span class="si">{</span><span class="n">jobsize</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;Estimated price per job at $</span><span class="si">{</span><span class="n">token_1M_price</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">/1M_tokens is $</span><span class="si">{</span><span class="p">(</span><span class="n">jobsize</span><span class="o">/</span><span class="mi">1_000_000</span><span class="p">)</span><span class="o">*</span><span class="n">token_1M_price</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span>
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">Prompt length: <span class="m">128</span>
</span></span><span class="line"><span class="cl">Response length: <span class="m">5</span>
</span></span><span class="line"><span class="cl">Item count: <span class="m">323</span>
</span></span><span class="line"><span class="cl">Total estimated job size in tokens: <span class="m">42959</span>
</span></span><span class="line"><span class="cl">Estimated price per job at <span class="nv">$5</span>.00/1M_tokens is <span class="nv">$0</span>.21
</span></span></code></pre></div><p>So, there you have it. I&rsquo;m spending about 21 cents per job.</p>
<h2 id="final-thoughts">Final thoughts<a hidden class="anchor" aria-hidden="true" href="#final-thoughts">#</a></h2>
<p>Well, the street is now clean, and I&rsquo;ve finished another blog post.</p>
<p>You should be well on your way to improving throughput with OpenAI.</p>
<p>Do come back!</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://oshea00.github.io/tags/llm/">Llm</a></li>
      <li><a href="https://oshea00.github.io/tags/openai/">Openai</a></li>
      <li><a href="https://oshea00.github.io/tags/asyncopenai/">Asyncopenai</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="https://oshea00.github.io/posts/a-rose-by-any-other-name/">
    <span class="title">« Prev</span>
    <br>
    <span>Comparing Prompt Results - A Rose By Any Other Name</span>
  </a>
  <a class="next" href="https://oshea00.github.io/posts/transformers-positional-encoding/">
    <span class="title">Next »</span>
    <br>
    <span>Transformers -  Positional Encoding</span>
  </a>
</nav>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Scaling OpenAI With AsyncOpenAI on x"
            href="https://x.com/intent/tweet/?text=Scaling%20OpenAI%20With%20AsyncOpenAI&amp;url=https%3a%2f%2foshea00.github.io%2fposts%2fasync-openai%2f&amp;hashtags=llm%2copenai%2casyncopenai">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Scaling OpenAI With AsyncOpenAI on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2foshea00.github.io%2fposts%2fasync-openai%2f&amp;title=Scaling%20OpenAI%20With%20AsyncOpenAI&amp;summary=Scaling%20OpenAI%20With%20AsyncOpenAI&amp;source=https%3a%2f%2foshea00.github.io%2fposts%2fasync-openai%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Scaling OpenAI With AsyncOpenAI on reddit"
            href="https://reddit.com/submit?url=https%3a%2f%2foshea00.github.io%2fposts%2fasync-openai%2f&title=Scaling%20OpenAI%20With%20AsyncOpenAI">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Scaling OpenAI With AsyncOpenAI on facebook"
            href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2foshea00.github.io%2fposts%2fasync-openai%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Scaling OpenAI With AsyncOpenAI on whatsapp"
            href="https://api.whatsapp.com/send?text=Scaling%20OpenAI%20With%20AsyncOpenAI%20-%20https%3a%2f%2foshea00.github.io%2fposts%2fasync-openai%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Scaling OpenAI With AsyncOpenAI on telegram"
            href="https://telegram.me/share/url?text=Scaling%20OpenAI%20With%20AsyncOpenAI&amp;url=https%3a%2f%2foshea00.github.io%2fposts%2fasync-openai%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Scaling OpenAI With AsyncOpenAI on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=Scaling%20OpenAI%20With%20AsyncOpenAI&u=https%3a%2f%2foshea00.github.io%2fposts%2fasync-openai%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2024 <a href="https://oshea00.github.io/">MikesBlog</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
