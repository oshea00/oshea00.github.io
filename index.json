[{"content":" You might want to test an expected response from a prompt sent to a large language model, but string comparisons will not help you. The inherent variability in large language model (LLM) responses will require you to find new ways to compare generated prompt results.\nThere are a few reasons why a generated prompt result will not exactly match a prior result: the prompt itself may have changed, the model parameters may have changed, or the model\u0026rsquo;s inherent variability may inject a small amount of change in the results.\nOne technique for dealing with \u0026ldquo;fuzzy\u0026rdquo; comparisons is to use embeddings to compare texts. Embeddings can be thought of as a multidimensional space of meaning, within which any fragment of text can be assigned a location to a point in that space using a coordinate. The coordinate is the \u0026ldquo;embedding\u0026rdquo;. The ability to express the distance between any two points in such a space can be used as a proxy for \u0026ldquo;sameness in meaning.\u0026rdquo; Birds of a feather flock together.\nA coordinate list (or tuple) of two numbers, such as (x, y), is needed to assign a unique point location in a two-dimensional space. To extend this to an n-dimensional space, a list of length n is required. Embedding models typically assign coordinates in spaces of over 100 dimensions. This depends on the particular embedding model chosen. For OpenAI embedding models such as'text-embedding-ada-002'and'text-embedding-3-small', the embedding will be list of 1,536 float values - a coordinate in a 1,536 dimensional space.\nI\u0026rsquo;ve always found it fascinating that no matter how many dimenstions there are, the distance between any two points in n-dimensions is always a single number.\nThere are a couple of choices you can use to determine distance between n-dimensional coordinates, the most straightforward being the \u0026ldquo;Euclidean distance\u0026rdquo; (or norm) which most high school algebra students will remember as: $$ d = \\sqrt{(x_1-x_2)^2 + (y_1-y_2)^2} $$ or, more generally, for any n-dimensional Euclidean space:\n$$ d(p,q) = \\sqrt{\\sum_{i=1}^{n}(q_i - p_i)^2} $$Another choice is the Cosine similarity, which considers the angle between two coordinate vectors. Embeddings can be interpreted as a vector from the origin to the coordinate the embedding represents. The length of the embedding vector is always normalized to 1.0. If the vectors point in the same direction, they have similar meaning. Birds of a feather flock together and fly in the same direction too:\n$$ cos(\\theta) = \\frac{A \\cdot B}{\\left\\|A\\right\\| \\left\\|B\\right\\|} $$If the two embedding vectors are identical, the angle between them will be 0, so the angle\u0026rsquo;s cosine will be 1. The possible values range from -1 to 1, where -1 means opposite in meaning, 0 means no overlap in meaning, and 1 is identical. In practical terms, the values we are concerned with range from 0 to 1.\nFor the following code examples, I will use Cosine similarity. Rather than roll my own from the above formula, I will use a helper function found in thetorch.nnpackage, nn.CosineSimilarity(). This function will take advantage of the GPU, if present, and will also handle a few edge cases and singularities that may arise from comparing embedding vectors.\nSetting up a few functions to help generate prompt results and compare results:\nfrom openai import OpenAI import torch import torch.nn as nn def get_prompt_result(prompt:str, temperature:float=0, top_p:float = 1) -\u0026gt; str: result = client.chat.completions.create( model=\u0026#34;gpt-4o\u0026#34;, messages=[ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: prompt, } ], temperature=temperature, top_p=top_p ) return result.choices[0].message.content def get_embedding(text:str, model:str = \u0026#39;text-embedding-ada-002\u0026#39;) -\u0026gt; list[float]: return client.embeddings.create( input=[text], model=model # text-embedding-3-small ).data[0].embedding def get_similarity(embedding1:list[float], embedding2:list[float]) -\u0026gt; float: cosine_similarity = nn.CosineSimilarity(dim=0) similarity = cosine_similarity(torch.tensor(embedding1), torch.tensor(embedding2)) return similarity.item() def describe_similarity(value:float) -\u0026gt; str: if value \u0026gt; 0.99: return \u0026#34;identical\u0026#34; elif value \u0026gt; 0.95: return \u0026#34;very similar\u0026#34; elif value \u0026gt; 0.9: return \u0026#34;similar\u0026#34; elif value \u0026gt; 0.8: return \u0026#34;somewhat similar\u0026#34; else: return \u0026#34;not similar\u0026#34; Let\u0026rsquo;s show how different prompts can produce different results even if the prompts themselves seem to be asking nearly the same question:\nbaseline_result_text = get_prompt_result(\u0026#34;What are some nice places to visit in Paris?\u0026#34;) current_result_text = get_prompt_result(\u0026#34;Describe a few good places to visit in Paris?\u0026#34;) returns these results:\nParis is a city rich in history, culture, and beauty, offering a wide array of attractions for visitors. Here are some must-see places in Paris: 1. **Eiffel Tower**: The iconic symbol of Paris, offering stunning views of the city from its observation decks. 2. **Louvre Museum**: Home to thousands of works of art, including the Mona Lisa and the Venus de Milo. 3. **Notre-Dame Cathedral**: A masterpiece of Gothic architecture, though currently under restoration following the 2019 fire. 4. **Champs-Élysées and Arc de Triomphe**: A famous avenue leading to the monumental arch, which offers panoramic views of Paris. 5. **Montmartre and Sacré-Cœur Basilica**: A historic district known for its artistic history, charming streets, and the beautiful basilica with its stunning views. 6. **Musée d\u0026#39;Orsay**: An art museum housed in a former railway station, featuring an extensive collection of Impressionist and Post-Impressionist masterpieces. 7. **Palace of Versailles**: Located just outside Paris, this opulent palace and its gardens are a testament to the grandeur of French royalty. 8. **Seine River Cruise**: A boat tour along the Seine River offers a unique perspective of Paris\u0026#39;s landmarks, especially beautiful at night. 9. **Luxembourg Gardens**: A beautiful park perfect for a leisurely stroll, with statues, fountains, and the Luxembourg Palace. 10. **Le Marais**: A historic district known for its narrow medieval streets, trendy boutiques, and vibrant nightlife. 11. **Panthéon**: A neoclassical mausoleum containing the remains of distinguished French citizens, with impressive architecture and a crypt. 12. **Centre Pompidou**: A modern art museum with a distinctive architectural design, housing a vast collection of contemporary art. 13. **Sainte-Chapelle**: A stunning Gothic chapel known for its magnificent stained glass windows. 14. **Opéra Garnier**: A grand opera house with opulent interiors, worth visiting for its architecture and performances. 15. **Père Lachaise Cemetery**: The final resting place of many famous individuals, including Jim Morrison, Oscar Wilde, and Edith Piaf. These are just a few highlights, and Paris has much more to offer depending on your interests, from charming neighborhoods and cafes to world-class shopping and dining experiences.t and\u0026hellip;\nParis, often referred to as \u0026#34;The City of Light,\u0026#34; is renowned for its rich history, stunning architecture, and vibrant culture. Here are a few must-visit places in Paris: 1. **Eiffel Tower**: No trip to Paris is complete without visiting its most iconic landmark. You can take an elevator ride to the top for a breathtaking view of the city or enjoy a picnic on the Champ de Mars with the tower as your backdrop. 2. **Louvre Museum**: Home to thousands of works of art, including the Mona Lisa and the Venus de Milo, the Louvre is the world\u0026#39;s largest art museum and a historic monument in Paris. The glass pyramid entrance is a modern architectural marvel. 3. **Notre-Dame Cathedral**: This masterpiece of French Gothic architecture is famous for its stunning facade, intricate sculptures, and beautiful stained glass windows. Although it suffered damage from a fire in 2019, it remains a symbol of Parisian heritage. 4. **Montmartre and the Basilica of the Sacré-Cœur**: Perched atop a hill, Montmartre is known for its bohemian atmosphere, artists, and charming streets. The Basilica of the Sacré-Cœur offers panoramic views of Paris and is a beautiful example of Romano-Byzantine architecture. 5. **Champs-Élysées and Arc de Triomphe**: The Champs-Élysées is one of the most famous avenues in the world, lined with shops, theaters, and cafes. At its western end stands the Arc de Triomphe, a monument honoring those who fought and died for France. 6. **Musée d\u0026#39;Orsay**: Housed in a former railway station, this museum is renowned for its extensive collection of Impressionist and Post-Impressionist masterpieces by artists such as Monet, Van Gogh, and Degas. 7. **Seine River Cruise**: A boat cruise on the Seine River offers a unique perspective of Paris\u0026#39;s landmarks, including the Eiffel Tower, Notre-Dame Cathedral, and the Louvre. It\u0026#39;s a relaxing way to see the city, especially at night when the buildings are illuminated. 8. **Palace of Versailles**: Just a short trip from Paris, the Palace of Versailles is a UNESCO World Heritage site known for its opulent architecture, stunning gardens, and the Hall of Mirrors. It was the royal residence of Louis XIV. 9. **Le Marais**: This historic district is known for its narrow medieval streets, trendy boutiques, art galleries, and vibrant nightlife. It\u0026#39;s also home to the Jewish Quarter and the beautiful Place des Vosges. 10. **Luxembourg Gardens**: These beautifully manicured gardens are perfect for a leisurely stroll or a relaxing afternoon. The gardens feature statues, fountains, and the Luxembourg Palace, which houses the French Senate. Each of these locations offers a unique glimpse into the rich tapestry of Parisian culture and history. Whether you\u0026#39;re an art lover, history buff, or simply looking to soak in the city\u0026#39;s ambiance, Paris has something to offer everyone. They are slightly different in that the first prompt returns fifteen locations and the second prompt only ten. Let\u0026rsquo;s calculate the embeddings and compare them:\nsimilarity = get_similarity(baseline_embedding, current_embedding) print(similarity,describe_similarity(similarity)) \u0026gt; 0.9737002849578857 very similar So, not identical, but essentially the same. You might have some testing added to your code that uses this metric to determine if anticipated results are \u0026ldquo;drifting\u0026rdquo; beyond some pre-determined limit. In this case, the ranges I chose might alert for the need to look at possible model changes that have crept into the process, or perhaps a prompt was changed in a recent release.\nWe can compare results using the same exact prompt twice. Notice that the defaulttemperaturefor the prompt generation functionget_prompt_result()is set to \u0026ldquo;0\u0026rdquo; - which should provide the most \u0026ldquo;repeatable\u0026rdquo; output fromgpt-4o. In practice there is always a tiny bit of variablilty even with temperature set to the most stable value. This varies between different LLMs:\nbaseline_result_text = get_prompt_result(\u0026#34;What are some nice places to visit in Paris?\u0026#34;) current_result_text = get_prompt_result(\u0026#34;What are some nice places to visit in Paris?\u0026#34;) similarity = get_similarity(get_embedding(baseline_result_text), get_embedding(current_result_text)) print(similarity, describe_similarity(similarity)) print(f\u0026#34;Is baseline_result_text == current_result_text? {baseline_result_text == current_result_text}\u0026#34;) However, by comparing the embeddings, we can determine them to be identical for practical purposes, even if they are not exactly identical as strings.\n\u0026gt; 0.997055172920227 identical \u0026gt; Is baseline_result_text == current_result_text? False The code above allows for changing the model parameters used in the prompt generation. You can play with this code yourself and get a feel for how these values can change repeatability of the results. You might find this useful to add to your prompt testing and processing in live systems.\n","permalink":"https://oshea00.github.io/posts/a-rose-by-any-other-name/","summary":"You might want to test an expected response from a prompt sent to a large language model, but string comparisons will not help you. The inherent variability in large language model (LLM) responses will require you to find new ways to compare generated prompt results.\nThere are a few reasons why a generated prompt result will not exactly match a prior result: the prompt itself may have changed, the model parameters may have changed, or the model\u0026rsquo;s inherent variability may inject a small amount of change in the results.","title":"Comparing Prompt Results - A Rose By Any Other Name"},{"content":"As I stood outside and looked at the neighborhood wasteland that post-July 4th left behind, the whiff of gunpowder still hanging in the air, I felt a burst of good neighbor energy flow through me, so I grabbed a broom. Sweeping up the street gave me time to think about the other chores I had for the day, including the writing of a new blog post, and I began to wonder how I could use ChatGPT to help me speed some things up.\nFigure 1. Work, work, work.\nMy thoughts turned to the bills. Every now and then, I download the transactions from my credit card’s website and attempt to figure out what I\u0026rsquo;m spending and where it is going. The transaction file looks like this.\nFigure 2. Raw transactions.\nNote the field Name and its contents. Normally, I spend 15-30 minutes figuring out how to describe the transactions with tags in a column that I can use to group payments. Looking at row two, I have spent something on Amazon Digital and there’s a unique reference in the Name field. My tag might be \u0026lsquo;AMAZON DIGITAL\u0026rsquo;. But I will need to mentally parse each of these and assign the tag manually. This looks like a job for GPT.\nIt would be nice if GPT could examine these fields and tag them uniformly based on their contents—thus, turning a half-hour of Excel “clicky” into 30 seconds or so of work. Let\u0026rsquo;s get started.\nFor this example, I have decided to use OpenAI\u0026rsquo;s GPT-4o model, and my first task is to develop a prompt capable of doing what I want. After many attempts (another good blog topic on prompt engineering awaits) I came up with this.\ndef get_prompt(name): return \u0026#34;\u0026#34;\u0026#34;Examine the transaction name text delimited by ```. The format starts with a company or domain name. Call that portion COMPANY. COMPANY will consist of a phrase of one or more words. The first word may be a number. DESC follows COMPANY. DESC starts with a non-letter character, a non-english word, or a number. All words in COMPANY should be upper case and separated with single spaces. Output the COMPANY on a single line. Replace any non-letter characters in COMPANY with spaces. Do not repeat these instructions as part of your response. \u0026#34;\u0026#34;\u0026#34; + f\u0026#34;```{name}```\u0026#34; Using some Pandas magic and the python OpenAI client, I was able to use this prompt to produce a tag based on the Name and insert a column called Desc that I could use for grouping.\nFigure 3. Tagged transactions in the \u0026lsquo;Desc\u0026rsquo; column.\nNote in line [377] that I am post-processing the data from GPT-4o to remove some artifacts I don\u0026rsquo;t like, such as removing names and other \u0026lsquo;PII\u0026rsquo; that might appear in the tags. Also, in some Name values, company names were duplicated. So, rather than \u0026lsquo;MICROSOFT MICROSOFT,\u0026rsquo; I wanted just \u0026lsquo;MICROSOFT.\u0026rsquo; This is a good example of adding a step to ensure your data is good, clean, safe, etc., and should be a part of any LLM-generated processing.\nProcessing at scale Now that I had a reasonably good process for tagging my data, I decided to run all 323 transactions through calls to OpenAI. It took a few minutes. A few factors were playing into the total time:\nMy tier level on this OpenAI account was only Tier-1. I was using the synchronous OpenAI() client. Increasing the tier level to Tier-3 required processing a few more tokens for this account. A goal that was achieved fairly quickly as I ran more of these tests. Increasing the tier level allows for processing more tokens per minute (TPM) which allows for sending larger batches.\nFigure 4. Tier-3 limits.\nThe next part, and the main subject here, was to use batches of parallel queries to improve throughput, and this required the use of asyncio combined with the async client in the openai package: AsyncOpenAI.\nAsyncOpenAI has flow control You will find examples of using asyncio and aiohttps, combined with backoff and/or tenacity to call the OpenAI API asychronously using response headers to dynamically adjust calls when token limits are being reached. I don\u0026rsquo;t recommend this approach unless you are trying to write your own library.\nAs it turns out, the openai library already includes a client that does much of this for you called AsyncOpenAI, and it is mostly a drop-in replacement for the synchronous OpenAI client. It will handle flow control and backoff for you, so you don\u0026rsquo;t have to drop down into the details under the covers.\nThat is the example I\u0026rsquo;m showing here. Note that other than the \u0026lsquo;async/await\u0026rsquo; keywords, the asynchronous version looks exactly like the synchronous one.\nclient = AsyncOpenAI() async def async_completion(messages,prompt): messages.append({\u0026#34;role\u0026#34;:\u0026#34;user\u0026#34;,\u0026#34;content\u0026#34;:f\u0026#34;{prompt}\u0026#34;}) completion = await client.chat.completions.create( model=\u0026#34;gpt-4o\u0026#34;, messages=messages, temperature=0.2, top_p=0.9, max_tokens=4096, stream=False ) messages.append({\u0026#34;role\u0026#34;:\u0026#34;assistant\u0026#34;,\u0026#34;content\u0026#34;:f\u0026#34;{completion.choices[0].message.content}\u0026#34;}) return messages The next part is batching calls. We need a few helper functions. The first function is simply my own code that handles setting up the prompt and calling the async_completion() function. The next function handles the semaphore.\nasync def reformat_transaction_name(name): messages = [ {\u0026#39;role\u0026#39;:\u0026#39;system\u0026#39;,\u0026#39;content\u0026#39;:\u0026#39;you are a helpful named entity recognizer.\u0026#39;}, ] response = await async_completion(messages,get_prompt(name)) return response[-1][\u0026#39;content\u0026#39;] async def reformat_transaction_with_semaphore(transaction_name, semaphore): async with semaphore: return await reformat_transaction_name(transaction_name) Now for the batching with Semaphore part.\nfrom asyncio import Semaphore async def run_batch(items): semaphore = Semaphore(len(items)) tasks = [reformat_transaction_with_semaphore(name,semaphore) for name in items] results = await asyncio.gather(*tasks) return results In the run_batch(items) function, a semaphore is created to match the size of the batch being processed. The len(items) allows me to adjust this when calling it.\nNow for the fun part In this code, I setup and submit the batches based on BATCHSIZE to run run_batch(items) by windowing through the list of items in the Dataframe - doing a bit of accounting along the way.\n# Batched async import time start_time = time.time() BATCHSIZE = 65 desc_items = [] for i in range(0,len(cctrans[\u0026#39;Name\u0026#39;]),BATCHSIZE): items = cctrans[\u0026#39;Name\u0026#39;][i:i+BATCHSIZE] results = asyncio.run(run_batch(items)) desc_items += results end_time = time.time() print(f\u0026#34;Batch completion time: {end_time-start_time} seconds\u0026#34;) print(f\u0026#34;Item count: {len(desc_items)}\u0026#34;) print(f\u0026#34;Items per second: {len(desc_items) / (end_time-start_time)}\u0026#34;) Here are the results:\nBatch completion time: 6.294457197189331 seconds Item count: 323 Items per second: 51.31498870851476 Significantly better than a few minutes at 6.29 seconds.\nOn the home stretch So, at this point, you have enough code examples to go try this on your own, but I will spend a little time talking about testing and costs, etc. If you\u0026rsquo;re able to stick around.\nPicking good BATCHSIZE It is good to run tests with different batch sizes to determine what gives you the best throughput. Using the timing information from various batch sizes, I found BATCHSIZE=65 to be somewhat optimal; however, as I move up in tier level, I will need to re-check this, as higher limits will allow more TPM to be processed.\nAs you can see, there is some variablility in throughput most likely based on how busy OpenAI is. Your mileage will vary.\nFigure 5. Batch Size Runs.\nHow much will this cost? Another good practice is to estimate how many tokens you will be using. Using tiktoken and some pricing information can help with that.\nimport tiktoken token_1M_price=5 prompt_text = get_prompt(\u0026#39;Amazon web services aws.amazon.co WA\u0026#39;) tokenizer = tiktoken.get_encoding(\u0026#34;cl100k_base\u0026#34;) tokens = tokenizer.encode(prompt_text) prompt_token_count = len(tokens) tokens = tokenizer.encode(\u0026#39;AMAZON WEB SERVICES\u0026#39;) response_token_count = len(tokens) print(f\u0026#34;Prompt length: {prompt_token_count}\u0026#34;) print(f\u0026#34;Response length: {response_token_count}\u0026#34;) print(f\u0026#34;Item count: {len(cctrans[\u0026#39;Transaction\u0026#39;])}\u0026#34;) jobsize=len(cctrans[\u0026#39;Transaction\u0026#39;])*(prompt_token_count+response_token_count) print(f\u0026#34;Total estimated job size in tokens: {jobsize}\u0026#34;) print(f\u0026#34;Estimated price per job at ${token_1M_price:.2f}/1M_tokens is ${(jobsize/1_000_000)*token_1M_price:.2f}\u0026#34;) Prompt length: 128 Response length: 5 Item count: 323 Total estimated job size in tokens: 42959 Estimated price per job at $5.00/1M_tokens is $0.21 So, there you have it. I\u0026rsquo;m spending about 21 cents per job.\nFinal thoughts Well, the street is now clean, and I\u0026rsquo;ve finished another blog post.\nYou should be well on your way to improving throughput with OpenAI.\nDo come back!\n","permalink":"https://oshea00.github.io/posts/async-openai/","summary":"As I stood outside and looked at the neighborhood wasteland that post-July 4th left behind, the whiff of gunpowder still hanging in the air, I felt a burst of good neighbor energy flow through me, so I grabbed a broom. Sweeping up the street gave me time to think about the other chores I had for the day, including the writing of a new blog post, and I began to wonder how I could use ChatGPT to help me speed some things up.","title":"Scaling OpenAI With AsyncOpenAI"},{"content":"Since transformer input is processed in parallel rather than serially, it is necessary to encode the relative positions of the input sequence of tokens in some way. The positional encoding in the transformer model uses sinusoidal functions to create a unique encoding for each position.\nIn working through the article on Transformers, as described in the original paper \u0026ldquo;Attention is All You Need\u0026rdquo; by Vaswani et al., the following formulas are used to encode the PE tensor values:\n$$ PE_{(pos,2i)}=sin(\\frac{pos}{10000^{2i/d_{model}}}) $$$$ PE_{(pos,2i+1)}=cos(\\frac{pos}{10000^{2i/d_{model}}}) $$Where $pos$ is the position in the sequence, $i$ is the position in the positional encoding tensor, and $d_{model}$ is the dimension of the model, which is also used as the length of the embedding tensors which will be assigned to each position in the sequence.\nNote below where these Positional Encoding tensors are applied to the Input and Output Embedding tensors: Figure 1. Attention is All You Need\nAt first glance, it\u0026rsquo;s not apparent to me how the encoding values in the PE tensors can provide positional \u0026ldquo;intelligence\u0026rdquo; to the transformer model. What is going on here?\nThe intuition for the PE tensor values created with these formulas has several elements:\nUsing different frequencies ranging from short to long wavelengths in these sinusoidal functions allows the model to capture positional information at various scales from near to far. These smooth repeating functions can scale easily to different sequence lengths. Each position has a unique encoding aided by the alternating use of $sin$ and $cos$ in the even and odd positions. It is always easier for me to use a visual to understand things better. Using a scaled-down transformer might help to get a better intuition of what this looks like. Let\u0026rsquo;s use the above formulas and PyTorch to create the PE tensor. We will use a maximum sequence length of twenty and a $d_{model}$ dimension of six:\n# model config seq_len = 20 # maximum tokenized sequence length d_model = 6 # dimension of the model and the embeddings (meanings) # Positional encoding. # Creating the division term for the positional encoding formula pe = torch.zeros(seq_len, d_model) position = torch.arange(0, seq_len, dtype = torch.float).unsqueeze(1) div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)) # Apply sine to even indices in PE pe[:, 0::2] = torch.sin(position * div_term) # Apply cosine to odd indices in PE pe[:, 1::2] = torch.cos(position * div_term) The shape of the PE tensor will be $[20, 6]$, or $[seqlen, d_{model}]$. The embedding dimensions are always the same as the model dimension. This PE tensor will be added to the input and output sequence embedding tensors during model processing (see Figure 1.).\nFigure 2. PE Tensor\nEmbeddings are created based on the dictionary of tokens. For each token in the training input data, an embedding tensor of length $d_{model}$ is assigned from training. These embedding tensors encode the \u0026ldquo;meaning\u0026rdquo;, as understood by the model, via the text it has been trained on.\nTo see what these positional encoding values look like, for each column in the above PE tensor, I\u0026rsquo;ve plotted the values:\nFigure 3. Graphing Positional Encoding\nAs you can see, the first two positions are the $sin$ and $cos$ functions. In later positions, the same functions are modified to longer wavelengths, so we can see how adding these functions to the embeddings imbues them with some sense of relative position, from near to far, between the tokens being evaluated.\nWith some imagination, it\u0026rsquo;s possible to see how other encodings could be used to represent positional offsets. This would be a fun experiment to try. I\u0026rsquo;ve put the code from the article here in case you want to play with it.\n","permalink":"https://oshea00.github.io/posts/transformers-positional-encoding/","summary":"Since transformer input is processed in parallel rather than serially, it is necessary to encode the relative positions of the input sequence of tokens in some way. The positional encoding in the transformer model uses sinusoidal functions to create a unique encoding for each position.\nIn working through the article on Transformers, as described in the original paper \u0026ldquo;Attention is All You Need\u0026rdquo; by Vaswani et al., the following formulas are used to encode the PE tensor values:","title":"Transformers -  Positional Encoding"},{"content":"Since Llama3 was released, the PyTorch llama3 documentation has a few glitches pointing at configurations in torchtune that are still referencing Llama2. The meta website is a little more up-to-date, but the documentation is a little light on details. So, I wrote this article to bring everything together.\nPrerequisites You\u0026rsquo;ll want to use Python 3.11 until Torch compile supports Python 3.12 , and I recommend setting up a virtual environment for this using venv or pipenv. Install torchtune pip install torchtune Install EleutherAI\u0026rsquo;s Evaluation Harness pip install lm_eval==0.4.* Download Llama3-8B model You will need to get access to Llama3 via instructions on the official Meta Llama3 page. You will also need your Hugging Face token setup from here.\nNote: some examples here reference \u0026ldquo;checkpoint-directory\u0026rdquo;. This will be the directory where your downloaded model weights are stored. In the following examples we\u0026rsquo;ll use /tmp/Meta-Llama-3-8B for the checkpoint directory.\n1 2 3 tune download meta-llama/Meta-Llama-3-8B \\ --output-dir /tmp/Meta-Llama-3-8B \\ --hf-token $HF_TOKEN Fine-tune the model The out-of-the-box recipe for torchtune single-GPU script tuning uses the Stanford Alpaca dataset, which has 52K instruction-following prompt pairs. It\u0026rsquo;s worth looking this over if you want to provide your own data, but for now, we\u0026rsquo;ll use the default recipe.\nGet some coffee. This process will take, depending on your GPU, at least a couple of hours on a single-GPU. I\u0026rsquo;m running a 24GB NVIDIA RTX 4090, and this process took three hours.\nWith less VRAM and a lighter-weight GPU, this could take up to 16 hours or more. There are instructions on the Meta Llama3, and the Llama3 PyTorch torchtune site that discuss running on multiple-GPU systems and tuning for smaller GPUs.\n1 2 3 4 tune run lora_finetune_single_device --config llama3/8B_lora_single_device \\ checkpointer.checkpoint_dir=/tmp/Meta-Llama-3-8B/original \\ tokenizer.path=/tmp/Meta-Llama-3-8B/original/tokenizer.model \\ checkpointer.output_dir=/tmp/Meta-Llama-3-8B Tuning run results When completed, the above command will place meta_model_0.pt and adapter_0.pt files in the checkpoint directory.\nEvaluating the tuned model To run evaluations, you can use torchtune to make copies of the various elleuther_evaluation config files, then edit them to reflect where to look for models and which merics to run.\nFor example\ntune cp eleuther_evaluation ./custom_eval_config.yaml However, the instructions I found on the PyTorch end-to-end workflow needed fixing, so I have provided already edited copies of these files for our use here.\nBaseline Evaluation of Un-tuned Llama3-8B custom_eval_config_orig.yaml\n# Config for EleutherEvalRecipe in eleuther_eval.py # # To launch, run the following command from root torchtune directory: # tune run eleuther_eval --config eleuther_evaluation tasks=[\u0026#34;truthfulqa_mc2\u0026#34;,\u0026#34;hellaswag\u0026#34;] # Model Arguments model: _component_: torchtune.models.llama3.llama3_8b checkpointer: _component_: torchtune.utils.FullModelMetaCheckpointer checkpoint_dir: /tmp/Meta-Llama-3-8B/original checkpoint_files: [ consolidated.00.pth ] recipe_checkpoint: null output_dir: /tmp/Meta-Llama-3-8B/original model_type: LLAMA3 # Tokenizer tokenizer: _component_: torchtune.models.llama3.llama3_tokenizer path: /tmp/Meta-Llama-3-8B/original/tokenizer.model # Environment device: cuda dtype: bf16 seed: 217 # EleutherAI specific eval args tasks: [\u0026#34;truthfulqa_mc2\u0026#34;] limit: null max_seq_length: 4096 # Quantization specific args quantizer: null Run\ntune run eleuther_eval --config ./custom_eval_config_orig.yaml On the 24GB RTX 4090 this takes about four minutes, and the output looks like this. We get about 43.9% accuracy. Fine-Tuned Llama 8B Evaluation custom_eval_config.yaml\n# Config for EleutherEvalRecipe in eleuther_eval.py # # To launch, run the following command from root torchtune directory: # tune run eleuther_eval --config eleuther_evaluation tasks=[\u0026#34;truthfulqa_mc2\u0026#34;,\u0026#34;hellaswag\u0026#34;] # Model Arguments model: _component_: torchtune.models.llama3.llama3_8b checkpointer: _component_: torchtune.utils.FullModelMetaCheckpointer checkpoint_dir: /tmp/Meta-Llama-3-8B checkpoint_files: [ meta_model_0.pt ] recipe_checkpoint: null output_dir: /tmp/Meta-Llama-3-8B model_type: LLAMA3 # Tokenizer tokenizer: _component_: torchtune.models.llama3.llama3_tokenizer path: /tmp/Meta-Llama-3-8B/original/tokenizer.model # Environment device: cuda dtype: bf16 seed: 217 # EleutherAI specific eval args tasks: [\u0026#34;truthfulqa_mc2\u0026#34;] limit: null max_seq_length: 4096 # Quantization specific args quantizer: null Note in the above config we are now pointing at the location of the fine-tuned weights meta_model_0.pt\nRun\ntune run eleuther_eval --config ./custom_eval_config.yaml The output looks like this. We get 55.3% accuracy. An increase of about 11.4%! Model Generation Now for the fun part. Seeing how the fine-tuned model handles prompts. We will use a top_k=300 and a temperature=0.6 for these tests. I noticed that temperature=0.8 definitely produces hallucinatory output.\nFine-tuned Llama-8B Generation custom_generation_config.yaml\n# Config for EleutherEvalRecipe in eleuther_eval.py # # To launch, run the following command from root torchtune directory: # tune run eleuther_eval --config eleuther_evaluation tasks=[\u0026#34;truthfulqa_mc2\u0026#34;,\u0026#34;hellaswag\u0026#34;] # Model Arguments model: _component_: torchtune.models.llama3.llama3_8b checkpointer: _component_: torchtune.utils.FullModelMetaCheckpointer checkpoint_dir: /tmp/Meta-Llama-3-8B checkpoint_files: [ meta_model_0.pt ] recipe_checkpoint: null output_dir: /tmp/Meta-Llama-3-8B model_type: LLAMA3 # Tokenizer tokenizer: _component_: torchtune.models.llama3.llama3_tokenizer path: /tmp/Meta-Llama-3-8B/original/tokenizer.model # Environment device: cuda dtype: bf16 seed: 217 # Quantization specific args quantizer: null # Generation arguments; defaults taken from gpt-fast # prompt: \u0026#34;Hello, my name is\u0026#34; max_new_tokens: 600 temperature: 0.6 # 0.8 and 0.6 are popular values to try top_k: 300 Run\ntune run generate --config ./custom_generation_config.yaml \\ prompt=\u0026#34;What are some interesting sites to visit in the Bay Area?\u0026#34; So, this works, but we still have to load up 16GB worth of weights and run inference, which takes about 60 seconds on my rig. Your mileage may vary. Let\u0026rsquo;s see if we can \u0026ldquo;quantize\u0026rdquo; our weights to speed up load time, and the inference time.\nQuantization Quantizing the model weights involves reducing the weights to smaller integer types. There are many algorithms, but we will use one of the standard torchtune recipes using TORCHAO to produce an \u0026lsquo;INT4\u0026rsquo; quantization.\nINT4 config custom_quantization_config.yaml\n# Config for EleutherEvalRecipe in eleuther_eval.py # # To launch, run the following command from root torchtune directory: # tune run eleuther_eval --config eleuther_evaluation tasks=[\u0026#34;truthfulqa_mc2\u0026#34;,\u0026#34;hellaswag\u0026#34;] # Model Arguments model: _component_: torchtune.models.llama3.llama3_8b checkpointer: _component_: torchtune.utils.FullModelMetaCheckpointer checkpoint_dir: /tmp/Meta-Llama-3-8B checkpoint_files: [ meta_model_0.pt ] recipe_checkpoint: null output_dir: /tmp/Meta-Llama-3-8B model_type: LLAMA3 # Tokenizer tokenizer: _component_: torchtune.models.llama3.llama3_tokenizer path: /tmp/Meta-Llama-3-8B/original/tokenizer.model # Environment device: cuda dtype: bf16 seed: 217 quantizer: _component_: torchtune.utils.quantization.Int4WeightOnlyQuantizer groupsize: 256 Run\ntune run quantize --config ./custom_quantization_config.yaml This runs fairly quickly, producing a meta_model_0-4w.pt weights file of only 4.92GB in the checkpoint directory. Generation using the quantized model OK! Let\u0026rsquo;s see how much faster we can run things, but keep in mind that the INT4 version of the weights will reduce the model\u0026rsquo;s language performance somewhat.\nQuantized generation configuration custom_generation_4w_config.yaml\n# Config for EleutherEvalRecipe in eleuther_eval.py # # To launch, run the following command from root torchtune directory: # tune run eleuther_eval --config eleuther_evaluation tasks=[\u0026#34;truthfulqa_mc2\u0026#34;,\u0026#34;hellaswag\u0026#34;] # Model Arguments model: _component_: torchtune.models.llama3.llama3_8b checkpointer: _component_: torchtune.utils.FullModelTorchTuneCheckpointer checkpoint_dir: /tmp/Meta-Llama-3-8B checkpoint_files: [ meta_model_0-4w.pt ] recipe_checkpoint: null output_dir: /tmp/Meta-Llama-3-8B model_type: LLAMA3 # Tokenizer tokenizer: _component_: torchtune.models.llama3.llama3_tokenizer path: /tmp/Meta-Llama-3-8B/original/tokenizer.model # Environment device: cuda dtype: bf16 seed: 217 # Quantization specific args quantizer: _component_: torchtune.utils.quantization.Int4WeightOnlyQuantizer groupsize: 256 # Generation arguments; defaults taken from gpt-fast # prompt: \u0026#34;Hello, my name is\u0026#34; max_new_tokens: 600 temperature: 0.6 # 0.8 and 0.6 are popular values to try top_k: 300 Note there are some key differences in this generation configuration file with the non-quantized generation file we used earlier on the fine-tuned model. Namely, we are pointing at the meta_model_0-4w.pt weights now. Also, we must provide the quantizer details matching the ones we used in the quantization step.\nRun\ntune run generate --config ./custom_generation_4w_config.yaml \\ prompt=\u0026#34;What are some interesting sites to visit in the Bay Area?\u0026#34; Here is what I got It takes about 22 seconds to load up the model, and a mere 7 seconds to run the inference. This is about a 300% improvement over the un-quantized generation run, and the output also looks pretty good.\nHowever, let\u0026rsquo;s see if we can evaluate this quantized model\u0026rsquo;s performance on instruction following to see if the quantization has affected the accuracy of the model.\nQuantized evaluation configuration custom_eval_4w_config.yaml\n# Config for EleutherEvalRecipe in eleuther_eval.py # # To launch, run the following command from root torchtune directory: # tune run eleuther_eval --config eleuther_evaluation tasks=[\u0026#34;truthfulqa_mc2\u0026#34;,\u0026#34;hellaswag\u0026#34;] # Model Arguments model: _component_: torchtune.models.llama3.llama3_8b checkpointer: _component_: torchtune.utils.FullModelTorchTuneCheckpointer checkpoint_dir: /tmp/Meta-Llama-3-8B checkpoint_files: [ meta_model_0-4w.pt ] recipe_checkpoint: null output_dir: /tmp/Meta-Llama-3-8B model_type: LLAMA3 # Tokenizer tokenizer: _component_: torchtune.models.llama3.llama3_tokenizer path: /tmp/Meta-Llama-3-8B/original/tokenizer.model # Environment device: cuda dtype: bf16 seed: 217 # EleutherAI specific eval args tasks: [\u0026#34;truthfulqa_mc2\u0026#34;] limit: null max_seq_length: 4096 # Quantization specific args quantizer: _component_: torchtune.utils.quantization.Int4WeightOnlyQuantizer groupsize: 256 Note in the above config we have changed the checkpointer to FullModelTorchTuneCheckpointer, as this checkpointer can support the weights_only=true checkpoint file we created with the Int4WeightOnlyQuantizer when we quantized the model. We have also added the quantizer details.\nRun\ntune run eleuther_eval --config ./custom_eval_4w_config.yaml Here are the results showing we got an accuracy of 49% with the quantized, fine-tuned, model. This is a net increase of 5% over the baseline non-finetuned Llama3 model. By quantizing, we \u0026ldquo;took back\u0026rdquo; about 6% of the accuracy we gained in the fine-tuning of the model.\nSo, we traded some accuracy for performance, but we still ended up with an overall improvement. This shows the importance of adding evaluation benchmarking when fine-tuning LLM models.\nWe Made It! If you followed along this far, congratulations 🎉!\nI hope you had as much fun as I did. Next, I\u0026rsquo;ll cover how to encode this smaller model into GGUF format and post it to a Huggingface repository to share with others.\n","permalink":"https://oshea00.github.io/posts/finetuning-llama3-8b/","summary":"Since Llama3 was released, the PyTorch llama3 documentation has a few glitches pointing at configurations in torchtune that are still referencing Llama2. The meta website is a little more up-to-date, but the documentation is a little light on details. So, I wrote this article to bring everything together.\nPrerequisites You\u0026rsquo;ll want to use Python 3.11 until Torch compile supports Python 3.12 , and I recommend setting up a virtual environment for this using venv or pipenv.","title":"Fine-tuning Llama3"},{"content":"An increasing number of open-sourced generative AI large language models (LLM) are being hosted behind an OpenAI API-compatible endpoint or have tools that offer an OpenAI API. The Python library for accessing OpenAI is just a REST client, and the library provides a way to specify the URL and an API key, as well as the model being offered by the provider.\nHere are a few examples of how the OpenAI library is used with other open-source models.\nOllama Although there are macOS and Windows versions, Ollama is primarily a Linux-based tool that lets you download various LLMs easily and host them on your own hardware. An impressive number of models are available. Ollama runs locally as an API by default that accepts REST commands compatible with the OpenAI API.\nSee the Ollama GitHub repository.\nHere’s a quick start to run a small LLM model locally. If you have an Nvidia or AMD GPU, it will configure itself to use them automatically. If not, it defaults to CPU.\nInstall Ollama\ncurl -fsSL https://ollama.com/install.sh | sh Manual GGUF model creation procedure The following steps show how to install a GGUF model file into Ollama. This is useful in cases where the model may not already be in the Ollama Hub list of models. It also tends to work well in cases where you are behind a proxy that is causing problems using the ollama run \u0026lt;model\u0026gt; command.\nYou will need the HuggingFace CLI to download the Phi-2 LLM model manually.\npython3 -m pip install huggingface-cli Download the LLM model in GGUF format. Note: make sure to have your HF_TOKEN from here setup as an enviroment variable, or run huggingface-cli login prior to this.\n# If run from your home directory, this creates a ~/downloads directory if not already existing: huggingface-cli download \\ TheBloke/phi-2-GGUF \\ phi-2.Q4_K_M.gguf \\ --local-dir downloads \\ --local-dir-use-symlinks False Create a file named Modelfile (actually, any name is OK) with the following line in it.\nFROM ./downloads/phi-2.Q4_K_M.gguf Build the model using this file as input to Ollama create:\nollama create phi2 -f Modelfile Now you can verify the model is present, then run it:\nollama list ollama run phi2 \u0026gt;\u0026gt;\u0026gt; in a sentence, why is the sky blue? The sky is blue because of a phenomenon known as Rayleigh scattering.... The response on my Surface with i7-1185G7 @ 3.00GHZ CPU (no GPU) takes about 30 seconds.\nBy comparison, my RTX4090-equipped Corsair Vengeance i7400 liquid-cooled i9 12900X 64GB RAM box runs the Llama7 8B Model for the same query about the sky in less than a second.\nHere is an example of running a query using the OpenAI library against the local Ollama API referencing the PHI-2 model we loaded above. In the case of Ollama, there is no api_key, but it is a required element for the library.\nNvidia Nvidia provides access to open-source foundation models on its build platform (build.nvidia.com/explore/discover), which makes it easy to try these. Example code for these models uses an OpenAI API-compatible method. This is not an OpenAI-hosted model. It uses the same OpenAI Python library as the Ollama example above except, in this case, NVidia does have a process for requesting and using an API_KEY that works with their playground host. In this example, the phi-3 model is being requested.\nOpen-source models will increasingly be represented in cases where large models are overkill or the desire for complete control over the data is paramount. Containerizing these and running them on cloud infrastructure or running them locally with your own data on your own hardware is within reach of modest budgets and is a good way to learn more about LLMs and the tools used to develop solutions with them.\n","permalink":"https://oshea00.github.io/posts/openai-compatible-apis/","summary":"An increasing number of open-sourced generative AI large language models (LLM) are being hosted behind an OpenAI API-compatible endpoint or have tools that offer an OpenAI API. The Python library for accessing OpenAI is just a REST client, and the library provides a way to specify the URL and an API key, as well as the model being offered by the provider.\nHere are a few examples of how the OpenAI library is used with other open-source models.","title":"OpenAI Python API Compatibility"}]