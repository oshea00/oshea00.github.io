[{"content":"Since transformer input is processed in parallel rather than serially, it is necessary to encode the relative positions of the input sequence of tokens in some way. The positional encoding in the transformer model uses sinusoidal functions to create a unique encoding for each position.\nIn working through the article on Transformers, as described in the original paper \u0026ldquo;Attention is All You Need\u0026rdquo; by Vaswani et al., the following formulas are used to encode the PE tensor values:\n$$ PE_{(pos,2i)}=sin(\\frac{pos}{10000^{2i/d_{model}}}) $$ $$ PE_{(pos,2i+1)}=cos(\\frac{pos}{10000^{2i/d_{model}}}) $$ Where $pos$ is the position in the sequence, $i$ is the position in the positional encoding tensor, and $d_{model}$ is the dimension of the model, which is also used as the length of the embedding tensors which will be assigned to each position in the sequence.\nNote below where these Positional Encoding tensors are applied to the Input and Output Embedding tensors: Figure 1. Attention is All You Need\nAt first glance, it\u0026rsquo;s not apparent to me how the encoding values in the PE tensors can provide positional \u0026ldquo;intelligence\u0026rdquo; to the transformer model. What is going on here?\nThe intuition for the PE vector values created with these formulas has several elements:\nUsing different frequencies ranging from short to long wavelengths in these sinusoidal functions allows the model to capture positional information at various scales from near to far. These smooth repeating functions can scale easily to different sequence lengths. Each position has a unique encoding aided by the alternating use of $sin$ and $cos$ in the even and odd positions. It is always easier for me to use a visual to understand things better. Using a scaled-down transformer might help to get a better intuition of what this looks like. Let\u0026rsquo;s use the above formulas and PyTorch to create the PE tensor. We will use a maximum sequence length of twenty and a $d_{model}$ dimension of six:\n# model config seq_len = 20 # maximum tokenized sequence length d_model = 6 # dimension of the model and the embeddings (meanings) # Positional encoding. # Creating the division term for the positional encoding formula pe = torch.zeros(seq_len, d_model) position = torch.arange(0, seq_len, dtype = torch.float).unsqueeze(1) div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)) # Apply sine to even indices in PE pe[:, 0::2] = torch.sin(position * div_term) # Apply cosine to odd indices in PE pe[:, 1::2] = torch.cos(position * div_term) The shape of the PE tensor will be $[20, 6]$, or $[seqlen, d_{model}]$. The embedding dimensions are always the same as the model dimension. This PE tensor will be added to the input and output sequence embedding tensors during model processing (see Figure 1.).\nFigure 2. PE Tensor\nEmbeddings are created based on the dictionary of tokens. For each token in the training input data, an embedding tensor of length $d_{model}$ is assigned from training. These embedding tensors encode the \u0026ldquo;meaning\u0026rdquo;, as understood by the model, via the text it has been trained on.\nTo see what these positional encoding values look like, for each column in the above PE tensor, I\u0026rsquo;ve plotted the values:\nFigure 3. Graphing Positional Encoding\nAs you can see, the first two positions are the $sin$ and $cos$ functions. In later positions, the same functions are modified to longer wavelengths, so we can see how adding these functions to the embeddings imbues them with some sense of relative position, from near to far, between the tokens being evaluated.\nWith some imagination, it\u0026rsquo;s possible to see how other encodings could be used to represent positional offsets. This would be a fun experiment to try. I\u0026rsquo;ve put the code from the article here in case you want to play with it.\n","permalink":"https://oshea00.github.io/posts/transformers-positional-encoding/","summary":"Since transformer input is processed in parallel rather than serially, it is necessary to encode the relative positions of the input sequence of tokens in some way. The positional encoding in the transformer model uses sinusoidal functions to create a unique encoding for each position.\nIn working through the article on Transformers, as described in the original paper \u0026ldquo;Attention is All You Need\u0026rdquo; by Vaswani et al., the following formulas are used to encode the PE tensor values:","title":"Transformers -  Positional Encoding"},{"content":"Since Llama3 was released, the PyTorch llama3 documentation has a few glitches pointing at configurations in torchtune that are still referencing Llama2. The meta website is a little more up-to-date, but the documentation is a little light on details. So, I wrote this article to bring everything together.\nPrerequisites You\u0026rsquo;ll want to use Python 3.11 until Torch compile supports Python 3.12 , and I recommend setting up a virtual environment for this using venv or pipenv. Install torchtune pip install torchtune Install EleutherAI\u0026rsquo;s Evaluation Harness pip install lm_eval==0.4.* Download Llama3-8B model You will need to get access to Llama3 via instructions on the official Meta Llama3 page. You will also need your Hugging Face token setup from here.\nNote: some examples here reference \u0026ldquo;checkpoint-directory\u0026rdquo;. This will be the directory where your downloaded model weights are stored. In the following examples we\u0026rsquo;ll use /tmp/Meta-Llama-3-8B for the checkpoint directory.\n1 2 3 tune download meta-llama/Meta-Llama-3-8B \\ --output-dir /tmp/Meta-Llama-3-8B \\ --hf-token $HF_TOKEN Fine-tune the model The out-of-the-box recipe for torchtune single-GPU script tuning uses the Stanford Alpaca dataset, which has 52K instruction-following prompt pairs. It\u0026rsquo;s worth looking this over if you want to provide your own data, but for now, we\u0026rsquo;ll use the default recipe.\nGet some coffee. This process will take, depending on your GPU, at least a couple of hours on a single-GPU. I\u0026rsquo;m running a 24GB NVIDIA RTX 4090, and this process took three hours.\nWith less VRAM and a lighter-weight GPU, this could take up to 16 hours or more. There are instructions on the Meta Llama3, and the Llama3 PyTorch torchtune site that discuss running on multiple-GPU systems and tuning for smaller GPUs.\n1 2 3 4 tune run lora_finetune_single_device --config llama3/8B_lora_single_device \\ checkpointer.checkpoint_dir=/tmp/Meta-Llama-3-8B/original \\ tokenizer.path=/tmp/Meta-Llama-3-8B/original/tokenizer.model \\ checkpointer.output_dir=/tmp/Meta-Llama-3-8B Tuning run results When completed, the above command will place meta_model_0.pt and adapter_0.pt files in the checkpoint directory.\nEvaluating the tuned model To run evaluations, you can use torchtune to make copies of the various elleuther_evaluation config files, then edit them to reflect where to look for models and which merics to run.\nFor example\ntune cp eleuther_evaluation ./custom_eval_config.yaml However, the instructions I found on the PyTorch end-to-end workflow needed fixing, so I have provided already edited copies of these files for our use here.\nBaseline Evaluation of Un-tuned Llama3-8B custom_eval_config_orig.yaml\n# Config for EleutherEvalRecipe in eleuther_eval.py # # To launch, run the following command from root torchtune directory: # tune run eleuther_eval --config eleuther_evaluation tasks=[\u0026#34;truthfulqa_mc2\u0026#34;,\u0026#34;hellaswag\u0026#34;] # Model Arguments model: _component_: torchtune.models.llama3.llama3_8b checkpointer: _component_: torchtune.utils.FullModelMetaCheckpointer checkpoint_dir: /tmp/Meta-Llama-3-8B/original checkpoint_files: [ consolidated.00.pth ] recipe_checkpoint: null output_dir: /tmp/Meta-Llama-3-8B/original model_type: LLAMA3 # Tokenizer tokenizer: _component_: torchtune.models.llama3.llama3_tokenizer path: /tmp/Meta-Llama-3-8B/original/tokenizer.model # Environment device: cuda dtype: bf16 seed: 217 # EleutherAI specific eval args tasks: [\u0026#34;truthfulqa_mc2\u0026#34;] limit: null max_seq_length: 4096 # Quantization specific args quantizer: null Run\ntune run eleuther_eval --config ./custom_eval_config_orig.yaml On the 24GB RTX 4090 this takes about four minutes, and the output looks like this. We get about 43.9% accuracy. Fine-Tuned Llama 8B Evaluation custom_eval_config.yaml\n# Config for EleutherEvalRecipe in eleuther_eval.py # # To launch, run the following command from root torchtune directory: # tune run eleuther_eval --config eleuther_evaluation tasks=[\u0026#34;truthfulqa_mc2\u0026#34;,\u0026#34;hellaswag\u0026#34;] # Model Arguments model: _component_: torchtune.models.llama3.llama3_8b checkpointer: _component_: torchtune.utils.FullModelMetaCheckpointer checkpoint_dir: /tmp/Meta-Llama-3-8B checkpoint_files: [ meta_model_0.pt ] recipe_checkpoint: null output_dir: /tmp/Meta-Llama-3-8B model_type: LLAMA3 # Tokenizer tokenizer: _component_: torchtune.models.llama3.llama3_tokenizer path: /tmp/Meta-Llama-3-8B/original/tokenizer.model # Environment device: cuda dtype: bf16 seed: 217 # EleutherAI specific eval args tasks: [\u0026#34;truthfulqa_mc2\u0026#34;] limit: null max_seq_length: 4096 # Quantization specific args quantizer: null Note in the above config we are now pointing at the location of the fine-tuned weights meta_model_0.pt\nRun\ntune run eleuther_eval --config ./custom_eval_config.yaml The output looks like this. We get 55.3% accuracy. An increase of about 11.4%! Model Generation Now for the fun part. Seeing how the fine-tuned model handles prompts. We will use a top_k=300 and a temperature=0.6 for these tests. I noticed that temperature=0.8 definitely produces hallucinatory output.\nFine-tuned Llama-8B Generation custom_generation_config.yaml\n# Config for EleutherEvalRecipe in eleuther_eval.py # # To launch, run the following command from root torchtune directory: # tune run eleuther_eval --config eleuther_evaluation tasks=[\u0026#34;truthfulqa_mc2\u0026#34;,\u0026#34;hellaswag\u0026#34;] # Model Arguments model: _component_: torchtune.models.llama3.llama3_8b checkpointer: _component_: torchtune.utils.FullModelMetaCheckpointer checkpoint_dir: /tmp/Meta-Llama-3-8B checkpoint_files: [ meta_model_0.pt ] recipe_checkpoint: null output_dir: /tmp/Meta-Llama-3-8B model_type: LLAMA3 # Tokenizer tokenizer: _component_: torchtune.models.llama3.llama3_tokenizer path: /tmp/Meta-Llama-3-8B/original/tokenizer.model # Environment device: cuda dtype: bf16 seed: 217 # Quantization specific args quantizer: null # Generation arguments; defaults taken from gpt-fast # prompt: \u0026#34;Hello, my name is\u0026#34; max_new_tokens: 600 temperature: 0.6 # 0.8 and 0.6 are popular values to try top_k: 300 Run\ntune run generate --config ./custom_generation_config.yaml \\ prompt=\u0026#34;What are some interesting sites to visit in the Bay Area?\u0026#34; So, this works, but we still have to load up 16GB worth of weights and run inference, which takes about 60 seconds on my rig. Your mileage may vary. Let\u0026rsquo;s see if we can \u0026ldquo;quantize\u0026rdquo; our weights to speed up load time, and the inference time.\nQuantization Quantizing the model weights involves reducing the weights to smaller integer types. There are many algorithms, but we will use one of the standard torchtune recipes using TORCHAO to produce an \u0026lsquo;INT4\u0026rsquo; quantization.\nINT4 config custom_quantization_config.yaml\n# Config for EleutherEvalRecipe in eleuther_eval.py # # To launch, run the following command from root torchtune directory: # tune run eleuther_eval --config eleuther_evaluation tasks=[\u0026#34;truthfulqa_mc2\u0026#34;,\u0026#34;hellaswag\u0026#34;] # Model Arguments model: _component_: torchtune.models.llama3.llama3_8b checkpointer: _component_: torchtune.utils.FullModelMetaCheckpointer checkpoint_dir: /tmp/Meta-Llama-3-8B checkpoint_files: [ meta_model_0.pt ] recipe_checkpoint: null output_dir: /tmp/Meta-Llama-3-8B model_type: LLAMA3 # Tokenizer tokenizer: _component_: torchtune.models.llama3.llama3_tokenizer path: /tmp/Meta-Llama-3-8B/original/tokenizer.model # Environment device: cuda dtype: bf16 seed: 217 quantizer: _component_: torchtune.utils.quantization.Int4WeightOnlyQuantizer groupsize: 256 Run\ntune run quantize --config ./custom_quantization_config.yaml This runs fairly quickly, producing a meta_model_0-4w.pt weights file of only 4.92GB in the checkpoint directory. Generation using the quantized model OK! Let\u0026rsquo;s see how much faster we can run things, but keep in mind that the INT4 version of the weights will reduce the model\u0026rsquo;s language performance somewhat.\nQuantized generation configuration custom_generation_4w_config.yaml\n# Config for EleutherEvalRecipe in eleuther_eval.py # # To launch, run the following command from root torchtune directory: # tune run eleuther_eval --config eleuther_evaluation tasks=[\u0026#34;truthfulqa_mc2\u0026#34;,\u0026#34;hellaswag\u0026#34;] # Model Arguments model: _component_: torchtune.models.llama3.llama3_8b checkpointer: _component_: torchtune.utils.FullModelTorchTuneCheckpointer checkpoint_dir: /tmp/Meta-Llama-3-8B checkpoint_files: [ meta_model_0-4w.pt ] recipe_checkpoint: null output_dir: /tmp/Meta-Llama-3-8B model_type: LLAMA3 # Tokenizer tokenizer: _component_: torchtune.models.llama3.llama3_tokenizer path: /tmp/Meta-Llama-3-8B/original/tokenizer.model # Environment device: cuda dtype: bf16 seed: 217 # Quantization specific args quantizer: _component_: torchtune.utils.quantization.Int4WeightOnlyQuantizer groupsize: 256 # Generation arguments; defaults taken from gpt-fast # prompt: \u0026#34;Hello, my name is\u0026#34; max_new_tokens: 600 temperature: 0.6 # 0.8 and 0.6 are popular values to try top_k: 300 Note there are some key differences in this generation configuration file with the non-quantized generation file we used earlier on the fine-tuned model. Namely, we are pointing at the meta_model_0-4w.pt weights now. Also, we must provide the quantizer details matching the ones we used in the quantization step.\nRun\ntune run generate --config ./custom_generation_4w_config.yaml \\ prompt=\u0026#34;What are some interesting sites to visit in the Bay Area?\u0026#34; Here is what I got It takes about 22 seconds to load up the model, and a mere 7 seconds to run the inference. This is about a 300% improvement over the un-quantized generation run, and the output also looks pretty good.\nHowever, let\u0026rsquo;s see if we can evaluate this quantized model\u0026rsquo;s performance on instruction following to see if the quantization has affected the accuracy of the model.\nQuantized evaluation configuration custom_eval_4w_config.yaml\n# Config for EleutherEvalRecipe in eleuther_eval.py # # To launch, run the following command from root torchtune directory: # tune run eleuther_eval --config eleuther_evaluation tasks=[\u0026#34;truthfulqa_mc2\u0026#34;,\u0026#34;hellaswag\u0026#34;] # Model Arguments model: _component_: torchtune.models.llama3.llama3_8b checkpointer: _component_: torchtune.utils.FullModelTorchTuneCheckpointer checkpoint_dir: /tmp/Meta-Llama-3-8B checkpoint_files: [ meta_model_0-4w.pt ] recipe_checkpoint: null output_dir: /tmp/Meta-Llama-3-8B model_type: LLAMA3 # Tokenizer tokenizer: _component_: torchtune.models.llama3.llama3_tokenizer path: /tmp/Meta-Llama-3-8B/original/tokenizer.model # Environment device: cuda dtype: bf16 seed: 217 # EleutherAI specific eval args tasks: [\u0026#34;truthfulqa_mc2\u0026#34;] limit: null max_seq_length: 4096 # Quantization specific args quantizer: _component_: torchtune.utils.quantization.Int4WeightOnlyQuantizer groupsize: 256 Note in the above config we have changed the checkpointer to FullModelTorchTuneCheckpointer, as this checkpointer can support the weights_only=true checkpoint file we created with the Int4WeightOnlyQuantizer when we quantized the model. We have also added the quantizer details.\nRun\ntune run eleuther_eval --config ./custom_eval_4w_config.yaml Here are the results showing we got an accuracy of 49% with the quantized, fine-tuned, model. This is a net increase of 5% over the baseline non-finetuned Llama3 model. By quantizing, we \u0026ldquo;took back\u0026rdquo; about 6% of the accuracy we gained in the fine-tuning of the model.\nSo, we traded some accuracy for performance, but we still ended up with an overall improvement. This shows the importance of adding evaluation benchmarking when fine-tuning LLM models.\nWe Made It! If you followed along this far, congratulations ðŸŽ‰!\nI hope you had as much fun as I did. Next, I\u0026rsquo;ll cover how to encode this smaller model into GGUF format and post it to a Huggingface repository to share with others.\n","permalink":"https://oshea00.github.io/posts/finetuning-llama3-8b/","summary":"Since Llama3 was released, the PyTorch llama3 documentation has a few glitches pointing at configurations in torchtune that are still referencing Llama2. The meta website is a little more up-to-date, but the documentation is a little light on details. So, I wrote this article to bring everything together.\nPrerequisites You\u0026rsquo;ll want to use Python 3.11 until Torch compile supports Python 3.12 , and I recommend setting up a virtual environment for this using venv or pipenv.","title":"Fine-tuning Llama3"},{"content":"An increasing number of open-sourced generative AI large language models (LLM) are being hosted behind an OpenAI API-compatible endpoint or have tools that offer an OpenAI API. The Python library for accessing OpenAI is just a REST client, and the library provides a way to specify the URL and an API key, as well as the model being offered by the provider.\nHere are a few examples of how the OpenAI library is used with other open-source models.\nOllama Although there are macOS and Windows versions, Ollama is primarily a Linux-based tool that lets you download various LLMs easily and host them on your own hardware. An impressive number of models are available. Ollama runs locally as an API by default that accepts REST commands compatible with the OpenAI API.\nSee the Ollama GitHub repository.\nHereâ€™s a quick start to run a small LLM model locally. If you have an Nvidia or AMD GPU, it will configure itself to use them automatically. If not, it defaults to CPU.\nInstall Ollama\ncurl -fsSL https://ollama.com/install.sh | sh Manual GGUF model creation procedure The following steps show how to install a GGUF model file into Ollama. This is useful in cases where the model may not already be in the Ollama Hub list of models. It also tends to work well in cases where you are behind a proxy that is causing problems using the ollama run \u0026lt;model\u0026gt; command.\nYou will need the HuggingFace CLI to download the Phi-2 LLM model manually.\npython3 -m pip install huggingface-cli Download the LLM model in GGUF format. Note: make sure to have your HF_TOKEN from here setup as an enviroment variable, or run huggingface-cli login prior to this.\n# If run from your home directory, this creates a ~/downloads directory if not already existing: huggingface-cli download \\ TheBloke/phi-2-GGUF \\ phi-2.Q4_K_M.gguf \\ --local-dir downloads \\ --local-dir-use-symlinks False Create a file named Modelfile (actually, any name is OK) with the following line in it.\nFROM ./downloads/phi-2.Q4_K_M.gguf Build the model using this file as input to Ollama create:\nollama create phi2 -f Modelfile Now you can verify the model is present, then run it:\nollama list ollama run phi2 \u0026gt;\u0026gt;\u0026gt; in a sentence, why is the sky blue? The sky is blue because of a phenomenon known as Rayleigh scattering.... The response on my Surface with i7-1185G7 @ 3.00GHZ CPU (no GPU) takes about 30 seconds.\nBy comparison, my RTX4090-equipped Corsair Vengeance i7400 liquid-cooled i9 12900X 64GB RAM box runs the Llama7 8B Model for the same query about the sky in less than a second.\nHere is an example of running a query using the OpenAI library against the local Ollama API referencing the PHI-2 model we loaded above. In the case of Ollama, there is no api_key, but it is a required element for the library.\nNvidia Nvidia provides access to open-source foundation models on its build platform (build.nvidia.com/explore/discover), which makes it easy to try these. Example code for these models uses an OpenAI API-compatible method. This is not an OpenAI-hosted model. It uses the same OpenAI Python library as the Ollama example above except, in this case, NVidia does have a process for requesting and using an API_KEY that works with their playground host. In this example, the phi-3 model is being requested.\nOpen-source models will increasingly be represented in cases where large models are overkill or the desire for complete control over the data is paramount. Containerizing these and running them on cloud infrastructure or running them locally with your own data on your own hardware is within reach of modest budgets and is a good way to learn more about LLMs and the tools used to develop solutions with them.\n","permalink":"https://oshea00.github.io/posts/openai-compatible-apis/","summary":"An increasing number of open-sourced generative AI large language models (LLM) are being hosted behind an OpenAI API-compatible endpoint or have tools that offer an OpenAI API. The Python library for accessing OpenAI is just a REST client, and the library provides a way to specify the URL and an API key, as well as the model being offered by the provider.\nHere are a few examples of how the OpenAI library is used with other open-source models.","title":"OpenAI Python API Compatibility"}]