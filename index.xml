<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>MikesBlog</title>
    <link>https://oshea00.github.io/</link>
    <description>Recent content on MikesBlog</description>
    <generator>Hugo -- 0.136.5</generator>
    <language>en</language>
    <lastBuildDate>Tue, 20 Aug 2024 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://oshea00.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Comparing Prompt Results - A Rose By Any Other Name</title>
      <link>https://oshea00.github.io/posts/a-rose-by-any-other-name/</link>
      <pubDate>Tue, 20 Aug 2024 00:00:00 +0000</pubDate>
      <guid>https://oshea00.github.io/posts/a-rose-by-any-other-name/</guid>
      <description>&lt;p&gt;&lt;figure class=&#34;align-center &#34;&gt;
    &lt;img loading=&#34;lazy&#34; src=&#34;images/androidrose.png#center&#34;/&gt; 
&lt;/figure&gt;

You might want to test an expected response from a prompt sent to a large language model, but string comparisons will not help you. The inherent variability in large language model (LLM) responses will require you to find new ways to compare generated prompt results.&lt;/p&gt;
&lt;p&gt;There are a few reasons why a generated prompt result will not exactly match a prior result: the prompt itself may have changed, the model parameters may have changed, or the model&amp;rsquo;s inherent variability may inject a small amount of change in the results.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Scaling OpenAI With AsyncOpenAI</title>
      <link>https://oshea00.github.io/posts/async-openai/</link>
      <pubDate>Sun, 07 Jul 2024 00:00:00 +0000</pubDate>
      <guid>https://oshea00.github.io/posts/async-openai/</guid>
      <description>&lt;p&gt;As I stood outside and looked at the neighborhood wasteland that post-July 4th left behind, the whiff of gunpowder still hanging in the air, I felt a burst of good neighbor energy flow through me, so I grabbed a broom. Sweeping up the street gave me time to think about the other chores I had for the day, including the writing of a new blog post, and I began to wonder how I could use ChatGPT to help me speed some things up.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Transformers -  Positional Encoding</title>
      <link>https://oshea00.github.io/posts/transformers-positional-encoding/</link>
      <pubDate>Mon, 27 May 2024 00:00:00 +0000</pubDate>
      <guid>https://oshea00.github.io/posts/transformers-positional-encoding/</guid>
      <description>&lt;p&gt;Since transformer input is processed in parallel rather than serially, it is necessary to encode the relative positions of the input sequence of tokens in some way. The positional encoding in the transformer model uses sinusoidal functions to create a unique encoding for each position.&lt;/p&gt;
&lt;p&gt;In working through the article on Transformers, as described in the original paper &lt;a href=&#34;https://arxiv.org/pdf/1706.03762&#34;&gt;&amp;ldquo;Attention is All You Need&amp;rdquo; by Vaswani et al.&lt;/a&gt;, the following formulas are used to encode the PE tensor values:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Fine-tuning Llama3</title>
      <link>https://oshea00.github.io/posts/finetuning-llama3-8b/</link>
      <pubDate>Sat, 11 May 2024 00:00:00 +0000</pubDate>
      <guid>https://oshea00.github.io/posts/finetuning-llama3-8b/</guid>
      <description>&lt;p&gt;Since Llama3 was released, the &lt;a href=&#34;https://pytorch.org/torchtune/stable/tutorials/llama3.html&#34;&gt;PyTorch llama3&lt;/a&gt; documentation has a few glitches pointing at configurations in torchtune that are still referencing Llama2. &lt;a href=&#34;https://llama.meta.com/docs/how-to-guides/fine-tuning/&#34;&gt;The meta website&lt;/a&gt; is a little more up-to-date, but the documentation is a little light on details. So, I
wrote this article to bring everything together.&lt;/p&gt;
&lt;h2 id=&#34;prerequisites&#34;&gt;Prerequisites&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;You&amp;rsquo;ll want to use Python 3.11 &lt;a href=&#34;https://github.com/pytorch/pytorch/issues/120233&#34;&gt;until Torch compile supports Python 3.12&lt;/a&gt; , and I recommend setting up a virtual environment for this using &lt;code&gt;venv&lt;/code&gt; or &lt;code&gt;pipenv&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Install &lt;a href=&#34;https://pytorch.org/torchtune/stable/install.html#install-label&#34;&gt;torchtune&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;pip install torchtune
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;Install &lt;a href=&#34;https://github.com/EleutherAI/lm-evaluation-harness&#34;&gt;EleutherAI&amp;rsquo;s Evaluation Harness&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;pip install &lt;span class=&#34;nv&#34;&gt;lm_eval&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;==&lt;/span&gt;0.4.*
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;download-llama3-8b-model&#34;&gt;Download Llama3-8B model&lt;/h2&gt;
&lt;p&gt;You will need to get access to Llama3 via instructions on the &lt;a href=&#34;https://github.com/meta-llama/llama3/blob/main/README.md&#34;&gt;official Meta Llama3&lt;/a&gt; page. You will also need your Hugging Face token setup from &lt;a href=&#34;https://huggingface.co/settings/tokens&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>OpenAI Python API Compatibility</title>
      <link>https://oshea00.github.io/posts/openai-compatible-apis/</link>
      <pubDate>Thu, 09 May 2024 00:00:00 +0000</pubDate>
      <guid>https://oshea00.github.io/posts/openai-compatible-apis/</guid>
      <description>&lt;p&gt;An increasing number of open-sourced generative AI large language models (LLM) are being hosted behind an OpenAI API-compatible endpoint or have tools that offer an OpenAI API. The Python library for accessing OpenAI is just a REST client, and the library provides a way to specify the URL and an API key, as well as the model being offered by the provider.&lt;/p&gt;
&lt;p&gt;Here are a few examples of how the OpenAI library is used with other open-source models.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
